{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 in the pandas df to SQL Server data pipeline\n",
    "\n",
    "### In Part 2, we assume that we've already run the Part 1 notebook (or equivalent scripts)\n",
    "\n",
    "##### Ie: the rental table has already been created within the craigslist database, and we have successfully inserted at least some data into the rental table!\n",
    "\n",
    "##### If so, then we can check for the last date of inserted date--ie, MAX() of date_possted--and then filter the scraped data > the MAX()date in the SQL table, clean the data, and then insert the new data into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports-- file processing & json libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# SQL ODBC for API connection between Python & SQL Server\n",
    "import pyodbc\n",
    "import sqlalchemy as sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18645 entries, 0 to 18644\n",
      "Data columns (total 48 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   listing_urls             18645 non-null  object \n",
      " 1   ids                      17236 non-null  float64\n",
      " 2   sqft                     13209 non-null  float64\n",
      " 3   cities                   17219 non-null  object \n",
      " 4   prices                   17227 non-null  object \n",
      " 5   bedrooms                 17184 non-null  float64\n",
      " 6   bathrooms                17184 non-null  object \n",
      " 7   attr_vars                17220 non-null  object \n",
      " 8   listing_descrip          17220 non-null  object \n",
      " 9   date_of_webcrawler       17236 non-null  object \n",
      " 10  kitchen                  17220 non-null  float64\n",
      " 11  date_posted              17220 non-null  object \n",
      " 12  region                   18645 non-null  object \n",
      " 13  sub_region               18645 non-null  object \n",
      " 14  cats_OK                  18645 non-null  int64  \n",
      " 15  dogs_OK                  18645 non-null  int64  \n",
      " 16  wheelchair_accessible    18645 non-null  int64  \n",
      " 17  laundry_in_bldg          18645 non-null  int64  \n",
      " 18  no_laundry               18645 non-null  int64  \n",
      " 19  washer_and_dryer         18645 non-null  int64  \n",
      " 20  washer_and_dryer_hookup  18645 non-null  int64  \n",
      " 21  laundry_on_site          18645 non-null  int64  \n",
      " 22  full_kitchen             18645 non-null  int64  \n",
      " 23  dishwasher               18645 non-null  int64  \n",
      " 24  refrigerator             18645 non-null  int64  \n",
      " 25  oven                     18645 non-null  int64  \n",
      " 26  flooring_carpet          18645 non-null  int64  \n",
      " 27  flooring_wood            18645 non-null  int64  \n",
      " 28  flooring_tile            18645 non-null  int64  \n",
      " 29  flooring_hardwood        18645 non-null  int64  \n",
      " 30  flooring_other           18645 non-null  int64  \n",
      " 31  apt_type                 18645 non-null  int64  \n",
      " 32  in_law_apt_type          18645 non-null  int64  \n",
      " 33  condo_type               18645 non-null  int64  \n",
      " 34  townhouse_type           18645 non-null  int64  \n",
      " 35  cottage_or_cabin_type    18645 non-null  int64  \n",
      " 36  single_fam_type          18645 non-null  int64  \n",
      " 37  duplex_type              18645 non-null  int64  \n",
      " 38  is_furnished             18645 non-null  int64  \n",
      " 39  attached_garage          18645 non-null  int64  \n",
      " 40  detached_garage          18645 non-null  int64  \n",
      " 41  carport                  18645 non-null  int64  \n",
      " 42  off_street_parking       18645 non-null  int64  \n",
      " 43  no_parking               18645 non-null  int64  \n",
      " 44  EV_charging              18645 non-null  int64  \n",
      " 45  air_condition            18645 non-null  int64  \n",
      " 46  no_smoking               18645 non-null  int64  \n",
      " 47  Unnamed: 0               535 non-null    float64\n",
      "dtypes: float64(5), int64(33), object(10)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files to a single Pandas' df using pd.concat()\"\"\"\n",
    "    path =  parent_direc # specify parent path of directories containing the scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes??\n",
    "    df_concat = pd.concat((pd.read_csv(file) for file in glob.iglob(\n",
    "        os.path.join(path, '**', fn_regex), \n",
    "        recursive=True)), ignore_index=True)  # os.path.join helps ensure this concatenation is OS independent\n",
    "    return df_concat\n",
    "\n",
    "## Import Dataset\n",
    "# import all scraped SF bay area rental listings data\n",
    "scraped_data_path = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\scraped_data\\\\sfbay\"\n",
    "\n",
    "clist_rental = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_path)\n",
    "clist_rental.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine last date (ie, MAX()) of the data stored in the rental table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n",
      "Latest date of scraped data inserted into the SQL table:\n",
      "          latest_date\n",
      "0 2021-12-14 03:57:00\n"
     ]
    }
   ],
   "source": [
    "# Perform SQL query on the date_posted col to determine the most recent date of data stored in the table  \n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def determine_latest_date(self, sql_query):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rentals' table\"\"\"\n",
    "\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        # specify SQL query\n",
    "        sql_query = sql_query \n",
    "\n",
    "        # perform query, and convert query results to Pandas' df\n",
    "        max_date = pd.read_sql(sql_query, conn)\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        ## sanity check:\n",
    "        \n",
    "        print(f\"Latest date of scraped data inserted into the SQL table:\\n{max_date}\")\n",
    "\n",
    "        return max_date\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "\n",
    "# specify query to select the latest date based on date_posted:\n",
    "query = \"SELECT MAX(date_posted) AS latest_date FROM rental;\"\n",
    "\n",
    "latest_date = SQL_db.determine_latest_date(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest date among the scraped data stored in the SQL table is:\n",
      "2021-12-14 03:57:00\n"
     ]
    }
   ],
   "source": [
    "## next, convert this latest_date to a string value, so we can use this to filter the scraped dataframe dataset\n",
    "def datetime_col_to_str_of_datetime(df, datetime_col):\n",
    "    \"\"\"Given datetime col from pandas' DataFrame,\n",
    "    transform to a string of the datetime value.\"\"\"\n",
    "    return df[datetime_col].head(1).astype(str).reset_index().loc[0, datetime_col] \n",
    "\n",
    "# specify name of df and datetime col:\n",
    "df, dt_col = latest_date, 'latest_date' \n",
    "#apply function using the 2 arguments shown above\n",
    "latest_date_str = datetime_col_to_str_of_datetime(df, dt_col)\n",
    "# sanity check\n",
    "print(f\"The latest date among the scraped data stored in the SQL table is:\\n{latest_date_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newest scraped data not stored in the SQL table is:\n",
      "5076     2021-12-16 23:56\n",
      "5078     2021-12-16 23:02\n",
      "5079     2021-12-16 22:39\n",
      "5080     2021-12-16 22:23\n",
      "5081     2021-12-16 21:59\n",
      "               ...       \n",
      "18460    2021-12-14 09:42\n",
      "18464    2021-12-14 09:07\n",
      "18465    2021-12-14 08:41\n",
      "18467    2021-12-14 07:28\n",
      "18469    2021-12-14 06:51\n",
      "Name: date_posted, Length: 1120, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# filter the dataframe > MAX() of latest_date stored in SQL rental table\n",
    "# %%\n",
    "# filter dataset since specific date\n",
    "def filter_df_since_specified_date(df, target_date):\n",
    "    \"\"\"Filter the imported scraped dataset\n",
    "    to all data since user-specified date.\n",
    "    NB: specify date in YYYY-MM-DD format\"\"\"\n",
    "    df = df.loc[df.date_posted > target_date]\n",
    "    return df\n",
    "\n",
    "# get all data since the latest stored data from SQL table (via the query on MAX(posted_date)) \n",
    "clist_rental_subset = filter_df_since_specified_date(clist_rental, latest_date_str)\n",
    "\n",
    "# sanity check\n",
    "print(f\"The newest scraped data not stored in the SQL table is:\\n{clist_rental_subset.date_posted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform all data cleaning and wrangling features, as in the Part 1 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining price, city name, kitchen, & listing id nulls: \n",
      "prices     0\n",
      "cities     0\n",
      "kitchen    0\n",
      "ids        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def remove_price_city_kitch_and_id_nulls(df):\n",
    "    \"\"\"Remove rows that do not have price, city name, kitchen, or listing ID data, as these are essential variables in this rental listings dataset.\"\"\"\n",
    "    return df.dropna(subset=['prices', 'ids', 'kitchen', 'cities'])\n",
    "\n",
    "clist_rental = remove_price_city_kitch_and_id_nulls(clist_rental)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining price, city name, kitchen, & listing id nulls: \\n{clist_rental[['prices', 'cities', 'kitchen', 'ids']].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pleasanton          1\n",
       "Antioch             1\n",
       "Mariner's Island    1\n",
       "Benicia             1\n",
       "Rockridge           1\n",
       "Portola Valley      1\n",
       "Livermore           1\n",
       "Discovery Bay       1\n",
       "Bloomsdale          1\n",
       "Green Valley        1\n",
       "Name: cities, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_split_city_names(df, address_critera: list, neighborhood_criteria:list, split_city_delimiters: list):\n",
    "    \"\"\"Clean city names data in several ways:\n",
    "    a.) Remove extraneous address & neighborhood data placed in the city names HTML object, such as 'Rd', 'Blvd', or 'Downtown'.\n",
    "    b.) Unsplit city names data that are split via ',' & '/' delimiters.\n",
    "    c.) Replace abbreviated or mispelled city names.\n",
    "    d.) Remove any digits/integers within the city names data--ie, by using a '\\d+' regex as the argument of str.replace() and replace it with empty strings.\n",
    "    e.) Remove any city names records thast are left with merely empty strings (ie, the other steps removed all data for that given cities record).\n",
    "    f.) Remove any whitespace to avoid the same city names from being treated as different entities by Pandas, Python, or SQL. \n",
    "    g.) Also, use str.capwords() to capitalize words (ie, excluding apostrophes).\"\"\"\n",
    "    addr_criteria = address_critera\n",
    "    # Join pipe ('|') symbols to address list so we can str.split() on any one of these criteria:\n",
    "    addr_criteria = '|'.join(addr_criteria)\n",
    "    # specify extraneous neighborhood criteria we should also remove from col\n",
    "    nbhood_criteria = neighborhood_criteria # remove nieghborhood names as well as state abbreviation (shown on website as 'Ca') that is shown without the usual comma delimiter!\n",
    "    nbhood_criteria = '|'.join(nbhood_criteria) # join pipe symbols so we can perform 'or' condition splitting on each element separated by pipes \n",
    "    # b.) specify delimiters we need to refer to un-split city names:\n",
    "    split_city_delimiters = split_city_delimiters\n",
    "    split_city_delim_criteria = '|'.join(split_city_delimiters) # join pipes to delimiters so we can use str.split() based on multiple 'or' criteria simultaneously\n",
    "    # clean city names data by removing address delimiters, and unsplitting city names based on ',' & '\\' delimiters\n",
    "    df['cities'] =  df['cities'].str.split(addr_criteria).str[-1].str.replace(nbhood_criteria, '', case=True).str.lstrip()\n",
    "    df['cities'] = df['cities'].str.split(split_city_delim_criteria).str[0] #unsplit city names\n",
    "    # c.) replace specific abbreviated or mispelled city names:\n",
    "    df = df.replace({'cities':{'Rohnert Pk':'Rohnert Park', 'Hillsborough Ca': 'Hillsborough', 'South Sf': \n",
    "    'South San Francisco', 'East San Jose':'San Jose', 'Vallejo Ca':'Vallejo', 'Westgate On Saratoga .':'San Jose',\n",
    "    'Bodega':'Bodega Bay', 'Briarwood At Central Park':'Fremont', 'Campbell Ca': 'Campbell', \n",
    "    'Almaden':'San Jose', 'Ca':np.nan}}) \n",
    "    # d.) Remove digits/integers from cities column records:\n",
    "    df['cities'] = df['cities'].str.replace('\\d+', '')  # remove any digits by using '/d+' regex to look up digits, and then replace with empty string\n",
    "    # e.) Remove any rows that have empty strings or null values for cities col (having performed the various data filtering and cleaning above)\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    # f.) Remove whitespace\n",
    "    df['cities'] = df['cities'].str.strip() \n",
    "    # g.) capitalize the city names using str.capwords() \n",
    "    df['cities'] = df['cities'].str.split().apply(lambda x: [val.capitalize() for val in x]).str.join(' ')\n",
    "    return df\n",
    "\n",
    "# specify various address and street name that we need to remove from the city names \n",
    "address_criteria = ['Boulevard', 'Blvd', 'Road', 'Rd', 'Avenue', 'Ave', 'Street', 'St', ' Dr' 'Drive', 'Real', 'E Hillsdale Blvd'] \n",
    "# specify various extraneous neighborhood names such as 'Downtown' \n",
    "neighborhood_criteria = ['Downtown', 'Central/Downtown', 'North', 'California', 'Bay Area', 'St. Helena', 'St', 'nyon', 'Jack London Square', 'Walking Distance To', 'El Camino', 'Mendocino County', 'San Mateo County', 'Alameda County', 'Rio Nido Nr', 'Mission Elementary', 'Napa County', 'Golden Gate', 'Jennings', 'South Lake Tahoe', 'Tahoe Paradise', 'Kingswood Estates', 'South Bay', 'Skyline', 'San Antonio Tx', 'East Bay', 'Morton Dr']\n",
    "# specify what delimiters we want to search for to unsplit the split city names data:\n",
    "split_city_delimiters =  [',', '/']\n",
    "\n",
    "# clean city names data:\n",
    "clist_rental = clean_split_city_names(clist_rental, neighborhood_criteria, address_criteria, split_city_delimiters)\n",
    "# sanity check\n",
    "clist_rental.cities.value_counts().tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cols_to_indicators(df, list_of_cols):\n",
    "    \"\"\" Transform relevant attribute columns to numeric, and specify NaNs for any missing or non-numeric data.\"\"\"\n",
    "    df[list_of_cols] = df[list_of_cols].astype('uint8', errors='ignore') # convert any missing data to NaN \n",
    "    print(f\"Sanity check: The data types of {list_of_cols} are now: \\n{df[list_of_cols].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "# specify a list of cols to convert to numeric -- # since there are many cols we want to transform to indicator variables, it's easier to simply drop the few cols that comprise str (aka, object) data \n",
    "cols_to_indicators = clist_rental.drop(columns =['ids', 'listing_urls', 'region', 'sub_region', 'cities', 'attr_vars', 'listing_descrip', 'sqft', 'prices', 'bedrooms', 'bathrooms', 'date_posted', 'date_of_webcrawler']) \n",
    "cols_to_indicators_lis = list(cols_to_indicators.columns)\n",
    "cols_to_indicators = [] # free space\n",
    "\n",
    "clist_rental = transform_cols_to_indicators(clist_rental, cols_to_indicators_lis)\n",
    "\n",
    "cols_to_indicators_lis = [] # free space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, transform kitchen var separately, since this tends to otherwise convert to float:\n",
    "clist_rental = transform_cols_to_indicators(clist_rental, 'kitchen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sqft to object, so that we can more easily load it into the SQL Server table\n",
    "def transform_cols_to_object(df, col_to_transform):\n",
    "    return df[col_to_transform].astype('object')\n",
    "\n",
    "clist_rental['sqft'] = transform_cols_to_object(clist_rental, 'sqft')\n",
    "#sanity check\n",
    "clist_rental['sqft'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining bedroom & bathroom nulls: \n",
      "bedrooms     0\n",
      "bathrooms    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# next, remove any bathroom or bedroom nulls:\n",
    "def remove_bedroom_and_br_nulls(df):\n",
    "    return df.dropna(subset=['bedrooms', 'bathrooms'])\n",
    "\n",
    "clist_rental = remove_bedroom_and_br_nulls(clist_rental)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining bedroom & bathroom nulls: \\n{clist_rental[['bedrooms', 'bathrooms']].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          None\n",
       "1         898.0\n",
       "2         615.0\n",
       "3          None\n",
       "4         800.0\n",
       "          ...  \n",
       "18176     450.0\n",
       "18177     980.0\n",
       "18178    1150.0\n",
       "18179    2500.0\n",
       "18180    1945.0\n",
       "Name: sqft, Length: 16472, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, remove all remaining null values with Python 'None' value \n",
    "def replace_nulls_with_None_val(df):\n",
    "    df = df.fillna(np.nan).replace([np.nan], [None]) # \n",
    "    return df\n",
    "\n",
    "clist_rental = replace_nulls_with_None_val(clist_rental)\n",
    "\n",
    "# sanity check\n",
    "clist_rental.sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: \n",
      "1      9880\n",
      "2      3677\n",
      "1.5     638\n",
      "2.5     402\n",
      "3       221\n",
      "3.5      53\n",
      "4        20\n",
      "5        11\n",
      "9+        7\n",
      "7         4\n",
      "5.5       3\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# re: # of bathrooms data, transform any records containing 'shared' or 'split' to 1\n",
    "# Why?: Because we can assume that any rental units comprising a 'shared' bathroom is essentially 1 bathroom\n",
    "def transform_shared_and_split_to_ones(df, col_to_transform):\n",
    "    \"\"\"Transform any records (from given col) containing the string values of 'shared' or 'split' to a value of 1.\"\"\"\n",
    "    # transform col to object, so we can use Python str methods to transform the data\n",
    "    df[col_to_transform] = df[col_to_transform].astype('object') \n",
    "    bedroom_replace_criteria = ['shared', 'split']\n",
    "    bedroom_replace_criteria = '|'.join(bedroom_replace_criteria) # join pipe symbols so we can use str.replace() on multiple 'or' conditions simultaneously \n",
    "    return df[col_to_transform].str.replace(bedroom_replace_criteria,'1')\n",
    "\n",
    "# clean bathrooms data by replacing the 'split' and 'shared' string values:\n",
    "clist_rental['bathrooms'] = transform_shared_and_split_to_ones(clist_rental, 'bathrooms')\n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check: \\n{clist_rental['bathrooms'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value counts for bathrooms data--having cleaned ambiguous records: \n",
      "1      9880\n",
      "2      3677\n",
      "1.5     638\n",
      "2.5     402\n",
      "3       221\n",
      "3.5      53\n",
      "4        20\n",
      "5        11\n",
      "9         7\n",
      "7         4\n",
      "5.5       3\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# replace any ambiguous # of bathrooms data--such as '9+' with empty strings (ie, essentially nulls) \n",
    "def replace_ambiguous_data_plus_signs_with_empty_str(df, col_to_transform):\n",
    "    \"\"\"Replace ambiguous rows of data (ie, any containing a plus sign) for bathrooms col with empty strings\"\"\"\n",
    "    return df[col_to_transform].str.replace(r'\\+', '')  # use str.replace() to use a regex to search for plus signs, and in effect remove these by replacing them with empty strings \n",
    "\n",
    "clist_rental['bathrooms']  = replace_ambiguous_data_plus_signs_with_empty_str(clist_rental, 'bathrooms')\n",
    "# sanity check\n",
    "print(f\"New value counts for bathrooms data--having cleaned ambiguous records: \\n{clist_rental['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14916 entries, 0 to 18180\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   bathrooms  14916 non-null  float64\n",
      " 1   ids        14916 non-null  float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 349.6 KB\n",
      "Sanity check on data type of ids & bathrooms data: None\n"
     ]
    }
   ],
   "source": [
    "# transform bathrooms data to float\n",
    "# Why float?: Because some listings specify half bathrooms--e.g., 1.5 denotes one-and-half bathrooms. Re: ids, integer data type not store the entire id value due to maximum (byte) storage constraints. \n",
    "def transform_cols_to_float(df, col_to_transform):\n",
    "    return df[col_to_transform].astype('float')\n",
    "\n",
    "# convert bathrooms to float:\n",
    "clist_rental['bathrooms'] = transform_cols_to_float(clist_rental, 'bathrooms')    \n",
    "\n",
    "# convert ids to float:\n",
    "clist_rental['ids'] = transform_cols_to_float(clist_rental, 'ids')    \n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check on data type of ids & bathrooms data: {clist_rental[['bathrooms', 'ids']].info()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cols_to_int(df, list_of_cols_to_num):\n",
    "    \"\"\" Transform relevant attribute columns to numeric.\n",
    "    NB: Since the scraped 'prices' data can contain commas, we need to use str.replace(',','') to remove them before converting to numeric.\"\"\"\n",
    "    df['prices'] = pd.to_numeric(df['prices'].str.replace(\",\",\"\"), errors='coerce') # remove commas from prices data\n",
    "    df[list_of_cols_to_num] = df[list_of_cols_to_num].astype('int')\n",
    "    print(f\"Sanity check: The data types of {list_of_cols_to_num} are now: \\n{df[list_of_cols_to_num].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "# specify a list of cols to convert to integer\n",
    "cols_to_int = clist_rental[['prices', 'bedrooms']]\n",
    "cols_to_int_lis = list(cols_to_int.columns)  # convert relevant cols to list of col names\n",
    "\n",
    "cols_to_int = [] # free space\n",
    "\n",
    "clist_rental = transform_cols_to_int(clist_rental, cols_to_int_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cols_to_datetime(df, col_to_convert):\n",
    "    \"\"\"Transform relevant column(s) to datetime using pd.to_datetime() method, and use infer_datetime_format=True to enable allow for datetime conversion using differing formats (ie, date_posted has a somewhat more precise format). \"\"\"\n",
    "    return pd.to_datetime(df[col_to_convert], infer_datetime_format=True)\n",
    "\n",
    "# apply transformations to datetime for the 2 relevant cols:\n",
    "clist_rental['date_of_webcrawler'] =  transform_cols_to_datetime(clist_rental,'date_of_webcrawler')\n",
    "clist_rental['date_posted'] = transform_cols_to_datetime(clist_rental,'date_posted')\n",
    "\n",
    "#sanity check\n",
    "clist_rental[['date_posted', 'date_of_webcrawler']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_df(df):\n",
    "    \"\"\"Remove duplicate rows based on listing ids\"\"\"\n",
    "    return df.drop_duplicates(keep='first', subset = ['ids'])\n",
    "\n",
    "clist_rental = deduplicate_df(clist_rental)\n",
    "\n",
    "# sanity check -- \n",
    "clist_duplicate_ids_check = clist_rental[clist_rental.duplicated(\"ids\", keep= False)]\n",
    "print(f\"There should be no remaining duplicate listing ids (ie, 0 rows): \\n{clist_duplicate_ids_check.shape[0]}\")  # check that number of duplicate rows is false (ie, wrt duplicate listing ids)\n",
    "\n",
    "# free memory\n",
    "clist_duplicate_ids_check = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check--the remaining columns in the dataset are:\n",
      " Index(['ids', 'sqft', 'cities', 'prices', 'bedrooms', 'bathrooms', 'attr_vars',\n",
      "       'date_of_webcrawler', 'kitchen', 'date_posted', 'region', 'sub_region',\n",
      "       'cats_OK', 'dogs_OK', 'wheelchair_accessible', 'laundry_in_bldg',\n",
      "       'no_laundry', 'washer_and_dryer', 'washer_and_dryer_hookup',\n",
      "       'laundry_on_site', 'full_kitchen', 'dishwasher', 'refrigerator', 'oven',\n",
      "       'flooring_carpet', 'flooring_wood', 'flooring_tile',\n",
      "       'flooring_hardwood', 'flooring_other', 'apt_type', 'in_law_apt_type',\n",
      "       'condo_type', 'townhouse_type', 'cottage_or_cabin_type',\n",
      "       'single_fam_type', 'duplex_type', 'is_furnished', 'attached_garage',\n",
      "       'detached_garage', 'carport', 'off_street_parking', 'no_parking',\n",
      "       'EV_charging', 'air_condition', 'no_smoking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def remove_col_with_given_starting_name(df, col_starting_name):\n",
    "    \"\"\"Remove each column from df that has a given starting name substring.\"\"\"\n",
    "    return df.loc[:, ~df.columns.str.startswith(col_starting_name)] \n",
    "\n",
    "# remove 'Unnamed' columns, which might be imported errouneously via pd.read_csv()\n",
    "clist_rental = remove_col_with_given_starting_name(clist_rental, 'Unnamed')\n",
    "\n",
    "# remove listing_urls column since we do not want to store these data into the SQL Server table-- why?: a.) because listing urls are not relevent to rental prices and b.) the listing urls quickly become invalid or dead links, so we have no need to refer back to them at this stage in the webscraping project.\n",
    "clist_rental = remove_col_with_given_starting_name(clist_rental, 'listing_urls')\n",
    "\n",
    "# remove listing_descrip column since we do not want to store these data into the SQL Server table-- why?: a.) because listing urls are not relevent to rental prices and b.) the listing urls quickly become invalid or dead links, so we have no need to refer back to them at this stage in the webscraping project.\n",
    "clist_rental = remove_col_with_given_starting_name(clist_rental, 'listing_descrip')\n",
    "\n",
    "\n",
    "# sanity check\n",
    "print(f\"Sanity check--the remaining columns in the dataset are:\\n{clist_rental.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the new, cleaned data into SQL rental table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def insert_df_to_SQL_ETL(self, df):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        cursor.fast_executemany = True  # speed up data ingesting by reducing the numbers of calls to server for inserts\n",
    "\n",
    "        # convert all variables from dataframe to str to avoid following SQL Server pyodbc error: 'ProgrammingError: ('Invalid parameter type.  param-index=2 param-type=function', 'HY105')'\n",
    "        df = df.astype(str) # convert all df variables to str for ease of loading data into SQl Server table\n",
    "        \n",
    "        # insert scraped data from df to SQL table-- iterate over each row of each df col via .itertuples() method\n",
    "\n",
    "        # # # NB: since there are 45 cols we will insert, we will need 45 '?' char marks  \n",
    "        # q_mark_list = ['?']*45\n",
    "\n",
    "        # # # unpack list as string, and join() commas to each '?' char\n",
    "        # q_mark_str = ','.join(q_mark_list)\n",
    "        \n",
    "        # Get the number of needed '?' placeholders by looking up the # of cols (ie, len()) of the dataframe), and use .join() to have each question mark seprated by commas:\n",
    "        q_mark_str = ','.join('?'*len(clist_rental.columns))  \n",
    "\n",
    "        # specify INSERT INTO SQL statement--iterate over each row in df, and insert into SQL database:\n",
    "        for row in df.itertuples():  # iterate over each row from df\n",
    "            cursor.execute(f\"\"\"INSERT INTO rental (listing_id, sqft, city, price, bedrooms, bathrooms, attr_vars,\n",
    "            date_of_webcrawler, kitchen, date_posted, region, sub_region, cats_OK, dogs_OK, wheelchair_accessible,laundry_in_bldg, no_laundry, \n",
    "            washer_and_dryer, washer_and_dryer_hookup, laundry_on_site, full_kitchen, dishwasher, refrigerator,\n",
    "            oven,flooring_carpet, flooring_wood, flooring_tile, flooring_hardwood, flooring_other,apt_type, in_law_apt_type, condo_type, townhouse_type, cottage_or_cabin_type, single_fam_type, duplex_type, is_furnished, attached_garage,\n",
    "            detached_garage, carport, off_street_parking, no_parking, EV_charging, air_condition, no_smoking) \n",
    "            VALUES ({q_mark_str})\"\"\",\n",
    "            (row.ids,\n",
    "            row.sqft,\n",
    "            row.cities,\n",
    "            row.prices, \n",
    "            row.bedrooms,\n",
    "            row.bathrooms,\n",
    "            row.attr_vars, \n",
    "            row.date_of_webcrawler,\n",
    "            row.kitchen,\n",
    "            row.date_posted,\n",
    "            row.region,\n",
    "            row.sub_region,\n",
    "            row.cats_OK,\n",
    "            row.dogs_OK,\n",
    "            row.wheelchair_accessible,\n",
    "            row.laundry_in_bldg, \n",
    "            row.no_laundry,\n",
    "            row.washer_and_dryer,\n",
    "            row.washer_and_dryer_hookup,\n",
    "            row.laundry_on_site,\n",
    "            row.full_kitchen,\n",
    "            row.dishwasher,\n",
    "            row.refrigerator,\n",
    "            row.oven,\n",
    "            row.flooring_carpet,\n",
    "            row.flooring_wood,\n",
    "            row.flooring_tile,\n",
    "            row.flooring_hardwood,\n",
    "            row.flooring_other,\n",
    "            row.apt_type, \n",
    "            row.in_law_apt_type,\n",
    "            row.condo_type,\n",
    "            row.townhouse_type,\n",
    "            row.cottage_or_cabin_type,\n",
    "            row.single_fam_type, \n",
    "            row.duplex_type,\n",
    "            row.is_furnished,\n",
    "            row.attached_garage,\n",
    "            row.detached_garage,\n",
    "            row.carport,\n",
    "            row.off_street_parking,\n",
    "            row.no_parking,\n",
    "            row.EV_charging,\n",
    "            row.air_condition,\n",
    "            row.no_smoking)\n",
    "            )\n",
    "            \n",
    "        # save and commit changes to database\n",
    "        conn.commit()\n",
    "\n",
    "        # # sanity check-- ensure some data has been inserted into new SQL table\n",
    "        sql_table_count_records = conn.execute(\"\"\"SELECT COUNT(*) FROM rental;\"\"\").fetchall()\n",
    "        print(f\"The number of records stored in the SQL table is: {sql_table_count_records[0]}\")     \n",
    "        \n",
    "        sql_query_for_record_samples = conn.execute(\"\"\"SELECT TOP 3 * FROM rental;\"\"\").fetchall() # check out several of the records\n",
    "        print(f\"\\nA few of the inserted records are: {sql_query_for_record_samples}\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# Ingest data from pandas' dataframe to SQL server--data pipeline: \n",
    "SQL_db.insert_df_to_SQL_ETL(clist_rental)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95642f830b867bed1501b1b2459e1825a840e28cb86d2e6168b387f53ebdd3fb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('my_conda_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
