{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 in the pandas df to SQL Server data pipeline\n",
    "\n",
    "### NB: In Part 2, we assume that we've already run the Part 1 notebook (or equivalent scripts), the latter of which is intended to be run only *once*. However, the scripts, functions, and methods shown here can (and should) be re-used every time we have scraped more data and need to insert the new data into the SQL rental table. \n",
    "\n",
    "##### I.e.: the rental table has already been created within the craigslist database, and we have successfully inserted at least some data into the rental table!\n",
    "\n",
    "##### If so, then we will do the following here in Part 2: check for the last date of inserted date--ie, MAX() of date_possted--and then filter the scraped data > the MAX()date in the SQL table, clean the data using the various data type transformations, deduplications, removing nulls, etc. \n",
    "\n",
    "#### As a final data filtering step, we need to double-check whether any of these rental listings have already been inserted into the rental table using a SQL query in which we can employ the IN() operator . For example: rental listings might be taken down and then re-posted, and these listings--albeit quite rare--could have the same listing ids as an older listing that we've already stored in the rental table, even though these \"new\" listings would initially seem to be unique listings based purely on their more recent date_posted datetime value!\n",
    "\n",
    "#### After the data have been filtered and cleaned, we can then insert the new data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports-- file processing & json libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# SQL ODBC for API connection between Python & SQL Server\n",
    "import pyodbc\n",
    "import sqlalchemy as sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21095 entries, 0 to 21094\n",
      "Data columns (total 48 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   listing_urls             21095 non-null  object \n",
      " 1   ids                      19686 non-null  float64\n",
      " 2   sqft                     15164 non-null  float64\n",
      " 3   cities                   19667 non-null  object \n",
      " 4   prices                   19676 non-null  object \n",
      " 5   bedrooms                 19632 non-null  float64\n",
      " 6   bathrooms                19632 non-null  object \n",
      " 7   attr_vars                19668 non-null  object \n",
      " 8   listing_descrip          19668 non-null  object \n",
      " 9   date_of_webcrawler       19686 non-null  object \n",
      " 10  kitchen                  19668 non-null  float64\n",
      " 11  date_posted              19668 non-null  object \n",
      " 12  region                   21095 non-null  object \n",
      " 13  sub_region               21095 non-null  object \n",
      " 14  cats_OK                  21095 non-null  int64  \n",
      " 15  dogs_OK                  21095 non-null  int64  \n",
      " 16  wheelchair_accessible    21095 non-null  int64  \n",
      " 17  laundry_in_bldg          21095 non-null  int64  \n",
      " 18  no_laundry               21095 non-null  int64  \n",
      " 19  washer_and_dryer         21095 non-null  int64  \n",
      " 20  washer_and_dryer_hookup  21095 non-null  int64  \n",
      " 21  laundry_on_site          21095 non-null  int64  \n",
      " 22  full_kitchen             21095 non-null  int64  \n",
      " 23  dishwasher               21095 non-null  int64  \n",
      " 24  refrigerator             21095 non-null  int64  \n",
      " 25  oven                     21095 non-null  int64  \n",
      " 26  flooring_carpet          21095 non-null  int64  \n",
      " 27  flooring_wood            21095 non-null  int64  \n",
      " 28  flooring_tile            21095 non-null  int64  \n",
      " 29  flooring_hardwood        21095 non-null  int64  \n",
      " 30  flooring_other           21095 non-null  int64  \n",
      " 31  apt_type                 21095 non-null  int64  \n",
      " 32  in_law_apt_type          21095 non-null  int64  \n",
      " 33  condo_type               21095 non-null  int64  \n",
      " 34  townhouse_type           21095 non-null  int64  \n",
      " 35  cottage_or_cabin_type    21095 non-null  int64  \n",
      " 36  single_fam_type          21095 non-null  int64  \n",
      " 37  duplex_type              21095 non-null  int64  \n",
      " 38  is_furnished             21095 non-null  int64  \n",
      " 39  attached_garage          21095 non-null  int64  \n",
      " 40  detached_garage          21095 non-null  int64  \n",
      " 41  carport                  21095 non-null  int64  \n",
      " 42  off_street_parking       21095 non-null  int64  \n",
      " 43  no_parking               21095 non-null  int64  \n",
      " 44  EV_charging              21095 non-null  int64  \n",
      " 45  air_condition            21095 non-null  int64  \n",
      " 46  no_smoking               21095 non-null  int64  \n",
      " 47  Unnamed: 0               535 non-null    float64\n",
      "dtypes: float64(5), int64(33), object(10)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files to a single Pandas' df using pd.concat().\"\"\"\n",
    "    path =  parent_direc # specify parent path of directories containing the scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes??\n",
    "    df_concat = pd.concat((pd.read_csv(file, # import each CSV file from directory\n",
    "                                        sep=',',encoding = 'utf-8'  \n",
    "                                        ) for file in glob.iglob(\n",
    "                                            os.path.join(path, '**', fn_regex), \n",
    "                                            recursive=True)), ignore_index=True)  # os.path.join helps ensure this concatenation is OS independent\n",
    "    return df_concat\n",
    "\n",
    "## Import Dataset\n",
    "# import all scraped SF bay area rental listings data\n",
    "scraped_data_path = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\scraped_data\\\\sfbay\"\n",
    "\n",
    "clist_rental = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_path)\n",
    "clist_rental.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine last date (ie, MAX()) of the data stored in the rental table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SQL query on the date_posted col to determine the most recent date of data stored in the table  \n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def determine_latest_date(self, sql_query):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rentals' table\"\"\"\n",
    "\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        # specify SQL query\n",
    "        sql_query = sql_query \n",
    "\n",
    "        # perform query, and convert query results to Pandas' df\n",
    "        max_date = pd.read_sql(sql_query, conn)\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        ## sanity check:\n",
    "        print(f\"Latest date of scraped data inserted into the SQL table:\\n{max_date}\")\n",
    "\n",
    "        return max_date\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "\n",
    "# specify query to select the latest date based on date_posted:\n",
    "query = \"SELECT MAX(date_posted) AS latest_date FROM rental;\"\n",
    "\n",
    "latest_date = SQL_db.determine_latest_date(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we filter the scraped listings dataset (ie, dataframe), we need to transform the 'date_posted' column--ie, the one that we will filter--to a standardized datetime format\n",
    "\n",
    "#### Why?: To ensure consistency and replicability for this data pipeline (ie, of the pandas' DataFrame to SQL Server table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cols_to_datetime(df, col_to_convert):\n",
    "    \"\"\"Transform relevant column(s) to datetime using pd.to_datetime() method, and use infer_datetime_format=True to enable allow for datetime conversion using differing formats (ie, date_posted has a somewhat more precise format). \"\"\"\n",
    "    return pd.to_datetime(df[col_to_convert], infer_datetime_format=True)\n",
    "\n",
    "# apply transformations to datetime for the 2 relevant cols:\n",
    "clist_rental['date_of_webcrawler'] =  transform_cols_to_datetime(clist_rental,'date_of_webcrawler')\n",
    "clist_rental['date_posted'] = transform_cols_to_datetime(clist_rental,'date_posted')\n",
    "\n",
    "#sanity check\n",
    "clist_rental[['date_posted', 'date_of_webcrawler']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we need to filter the dataset to listings records whose date_posted dates are newer (read: greater than) the MAX() of the last date found from the query of the SQL rental table.\n",
    "\n",
    "### To this end, we can convert the MAX() value from the SQL query to a str value, and then use this as the argument for the filter_df_since_specified_date() function, which filters on the 'date_posted' column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next, convert this latest_date to a string value, so we can use this to filter the scraped dataframe dataset\n",
    "def datetime_col_to_str_of_datetime(df, datetime_col):\n",
    "    \"\"\"Given datetime col from pandas' DataFrame,\n",
    "    transform to a string of the datetime value.\"\"\"\n",
    "    return df[datetime_col].head(1).astype(str).reset_index().loc[0, datetime_col] \n",
    "\n",
    "# specify name of df and datetime col:\n",
    "df, dt_col = latest_date, 'latest_date' \n",
    "#apply function using the 2 arguments shown above\n",
    "latest_date_str = datetime_col_to_str_of_datetime(df, dt_col)\n",
    "# sanity check\n",
    "print(f\"The latest date among the scraped data stored in the SQL table is:\\n{latest_date_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, filter on the date_posted date, so that we only have the new listings data that we have not yet inserted into the SQL table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe > MAX() of latest_date stored in SQL rental table\n",
    "def filter_df_since_specified_date(df, target_date):\n",
    "    \"\"\"Filter the imported scraped dataset\n",
    "    to all data since user-specified date.\n",
    "    NB: specify date in YYYY-MM-DD format\"\"\"\n",
    "    df = df.loc[df['date_posted'] > target_date]\n",
    "    return df\n",
    "\n",
    "# get all data since the latest stored data from SQL table (via the query on MAX(posted_date)) \n",
    "clist_rental = filter_df_since_specified_date(clist_rental, latest_date_str)\n",
    "\n",
    "# sanity check\n",
    "print(f\"The newest scraped data not stored in the SQL table is--\\n*NB: this should be an empty df if we have stored all of the df's data into the SQL table*: \\n\\n{clist_rental['date_posted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, perform all additional data cleaning and wrangling features to prep the data for the SQL inserts, as in the Part 1 jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining price, city name, sqft, kitchen, & listing id nulls: \n",
      "prices     0\n",
      "ids        0\n",
      "sqft       0\n",
      "kitchen    0\n",
      "cities     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def remove_nulls_list(df, list_of_cols):\n",
    "    \"\"\"Remove rows that do not have price, city name, kitchen, sqft, or listing ID data, as these are essential variables in this rental listings dataset.\"\"\"\n",
    "    return df.dropna(subset=list_of_cols)\n",
    "\n",
    "list_cols_to_remove_nulls = ['prices', 'ids', 'sqft', 'kitchen', 'cities']  \n",
    "clist_rental = remove_nulls_list(clist_rental, list_cols_to_remove_nulls)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining price, listing id, sqft, kitchen, & city name nulls: \\n{clist_rental[list_cols_to_remove_nulls].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check--after cleaning the city names, let's examine some of the cleaned data: Discovery Bay     1\n",
      "Rockridge         1\n",
      "Montara           1\n",
      "Napa              1\n",
      "Cloverdale        1\n",
      "Rio Nido          1\n",
      "Benicia           1\n",
      "Bloomsdale        1\n",
      "Portola Valley    1\n",
      "Green Valley      1\n",
      "Name: cities, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def clean_split_city_names(df, address_critera: list, neighborhood_criteria:list, split_city_delimiters: list, incorrect_city_names:dict, cities_not_in_region:dict, cities_that_need_extra_cleaning:dict):\n",
    "    \"\"\"Clean city names data in several ways:\n",
    "    a.) Remove extraneous address & neighborhood data placed in the city names HTML object, such as 'Rd', 'Blvd', or 'Downtown'.\n",
    "    b.) Unsplit city names data that are split via ',' & '/' delimiters.\n",
    "    c.) Replace abbreviated or mispelled city names, and remove city names that do not exist within the SF Bay Area (e.g., 'Redding').\n",
    "    d.) Remove any digits/integers within the city names data--ie, by using a '\\d+' regex as the argument of str.replace() and replace it with empty strings.\n",
    "    e.) Remove any city names records thast are left with merely empty strings (ie, the other steps removed all data for that given cities record).\n",
    "    f.) Remove any whitespace to avoid the same city names from being treated as different entities by Pandas, Python, or SQL. \n",
    "    g.) Use str.capwords() to capitalize words (ie, excluding apostrophes).\n",
    "    h.) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena'. \"\"\"\n",
    "    # specify extraneous street & address data (e.g., 'Rd') that we want to remove from the city names column:\n",
    "    addr_criteria = '|'.join(address_critera) # Join pipe ('|') symbols to address list so we can str.split() on any one of these criteria (ie, 'or' condition splitting on each element separated by pipes):\n",
    "    # specify extraneous neighborhood criteria we should also remove from col\n",
    "    nbhood_criteria = '|'.join(neighborhood_criteria) # remove neighborhood names as well as state abbreviation (shown on website as 'Ca') that is shown without the usual comma delimiter!\n",
    "    # b.) specify delimiters we need to refer to un-split city names:\n",
    "    split_city_delimiters = '|'.join(split_city_delimiters) # join pipes to delimiters so we can use str.split() based on multiple 'or' criteria simultaneously\n",
    "    # clean city names data by removing extraneous address & neighborhood data, and unsplitting city names based on ',' & '\\' delimiters\n",
    "    df['cities'] =  df['cities'].str.split(addr_criteria).str[-1].str.replace(nbhood_criteria, '', case=True).str.lstrip()\n",
    "    df['cities'] = df['cities'].str.split(split_city_delimiters).str[0] #unsplit city names based on comma or forward-slash delimiters\n",
    "    # c.) replace specific abbreviated or mispelled city names, and remove cities that are not actually located in the sfbay region:\n",
    "    df = df.replace({'cities':incorrect_city_names}) # replace mispelled & abbreviated city names\n",
    "    df = df.replace({'cities':cities_not_in_region})  # remove (via empty string) cities that are not actually located in the sfbay region\n",
    "    # d.) Remove digits/integer-like data from cities column:\n",
    "    df['cities'] = df['cities'].str.replace('\\d+', '')  # remove any digits by using '/d+' regex to look up digits, and then replace with empty string\n",
    "    # e.) Remove any rows that have empty strings or null values for cities col (having performed the various data filtering and cleaning above)\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    # f.) Remove whitespace\n",
    "    df['cities'] = df['cities'].str.strip() \n",
    "    # g.) capitalize the city names using str.capwords() \n",
    "    df['cities'] = df['cities'].str.split().apply(lambda x: [val.capitalize() for val in x]).str.join(' ')\n",
    "    # h) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena' & 'San los' should be 'San Carlos'. Also, remove any non-Bay Area cities such as Redding:\n",
    "    # df['cities'] = df['cities'].str.lower() # transform all records to lower-case, for ease of cleaning the data\n",
    "    df = df.replace({'cities':cities_that_need_extra_cleaning})\n",
    "    return df\n",
    "\n",
    "# specify various address and street name that we need to remove from the city names \n",
    "address_criteria = ['Boulevard', 'Blvd', 'Road', 'Rd', 'Avenue', 'Ave', 'Street', 'St', 'Drive', 'Dr', 'Real', 'E Hillsdale Blvd'] \n",
    "# specify various extraneous neighborhood names such as 'Downtown' \n",
    "neighborhood_criteria = ['Downtown', 'Central/Downtown', 'North', 'California', 'Ca.', 'Bay Area', 'St. Helena', 'St', 'nyon', \n",
    "'Jack London Square', 'Walking Distance To', 'El Camino', 'Mendocino County', 'San Mateo County', 'Alameda County', 'Rio Nido Nr', 'Mission Elementary', \n",
    "'Napa County', 'Golden Gate', 'Jennings', 'South Lake Tahoe', 'Tahoe Paradise', 'Kingswood Estates', 'South Bay', 'Skyline', 'San Antonio Tx', \n",
    "'East Bay', 'Morton Dr'] \n",
    "\n",
    "# specify what delimiters we want to search for to unsplit the split city names data:\n",
    "split_city_delimiters =  [',', '/']\n",
    "# specify dictionary of abbreviated & mispelled cities:\n",
    "incorrect_city_names = {'Rohnert Pk':'Rohnert Park', 'Hillsborough Ca': 'Hillsborough', 'South Sf': 'South San Francisco', 'Ca':'', 'East San Jose':'San Jose', 'Vallejo Ca':'Vallejo', 'Westgate On Saratoga .':'San Jose', 'Bodega':'Bodega Bay', 'Briarwood At Central Park':'Fremont', 'Campbell Ca':'Campbell', 'Almaden':'San Jose', '.':'', 'East Foothills':'San Jose', 'Lake County':'', 'West End':'Alameda', 'Redwood Shores':'Redwood City'}\n",
    "\n",
    "# specify dictionary of cities that are not located in sfbay (ie, not located in the region):\n",
    "cities_not_in_region = {'Ketchum':'', 'Baypoinr':'', 'Quito': '', 'Redding':'', 'Bend' :''}\n",
    "\n",
    "# specify dictionary of city names that are mispelled after having removed various street and neighborhood substrings:\n",
    "cities_that_need_extra_cleaning = {'. Helena': 'St. Helena', '. Helena Deer Park': 'St. Helena', 'San Los':'San Carlos', 'Tro Valley':'Castro Valley', 'Rohnert Pk':'Rohnert Park',\n",
    "'Pbell':'Campbell', 'Pbell Ca':'Campbell', 'American Yon':'American Canyon'}\n",
    "\n",
    "# clean city names data:\n",
    "clist_rental = clean_split_city_names(clist_rental, address_criteria, neighborhood_criteria, split_city_delimiters, incorrect_city_names, cities_not_in_region, cities_that_need_extra_cleaning)\n",
    "# sanity check\n",
    "print(f\"Sanity check--after cleaning the city names, let's examine some of the cleaned data: {clist_rental.cities.value_counts().tail(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of ['kitchen', 'cats_OK', 'dogs_OK', 'wheelchair_accessible', 'laundry_in_bldg', 'no_laundry', 'washer_and_dryer', 'washer_and_dryer_hookup', 'laundry_on_site', 'full_kitchen', 'dishwasher', 'refrigerator', 'oven', 'flooring_carpet', 'flooring_wood', 'flooring_tile', 'flooring_hardwood', 'flooring_other', 'apt_type', 'in_law_apt_type', 'condo_type', 'townhouse_type', 'cottage_or_cabin_type', 'single_fam_type', 'duplex_type', 'is_furnished', 'attached_garage', 'detached_garage', 'carport', 'off_street_parking', 'no_parking', 'EV_charging', 'air_condition', 'no_smoking', 'Unnamed: 0'] are now: \n",
      "kitchen                    float64\n",
      "cats_OK                      uint8\n",
      "dogs_OK                      uint8\n",
      "wheelchair_accessible        uint8\n",
      "laundry_in_bldg              uint8\n",
      "no_laundry                   uint8\n",
      "washer_and_dryer             uint8\n",
      "washer_and_dryer_hookup      uint8\n",
      "laundry_on_site              uint8\n",
      "full_kitchen                 uint8\n",
      "dishwasher                   uint8\n",
      "refrigerator                 uint8\n",
      "oven                         uint8\n",
      "flooring_carpet              uint8\n",
      "flooring_wood                uint8\n",
      "flooring_tile                uint8\n",
      "flooring_hardwood            uint8\n",
      "flooring_other               uint8\n",
      "apt_type                     uint8\n",
      "in_law_apt_type              uint8\n",
      "condo_type                   uint8\n",
      "townhouse_type               uint8\n",
      "cottage_or_cabin_type        uint8\n",
      "single_fam_type              uint8\n",
      "duplex_type                  uint8\n",
      "is_furnished                 uint8\n",
      "attached_garage              uint8\n",
      "detached_garage              uint8\n",
      "carport                      uint8\n",
      "off_street_parking           uint8\n",
      "no_parking                   uint8\n",
      "EV_charging                  uint8\n",
      "air_condition                uint8\n",
      "no_smoking                   uint8\n",
      "Unnamed: 0                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def transform_cols_to_indicators(df, list_of_cols):\n",
    "    \"\"\" Transform relevant attribute columns to numeric, and specify NaNs for any missing or non-numeric data.\"\"\"\n",
    "    df[list_of_cols] = df[list_of_cols].astype('uint8', errors='ignore') # convert any missing data to NaN \n",
    "    print(f\"Sanity check: The data types of {list_of_cols} are now: \\n{df[list_of_cols].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "# specify a list of cols to convert to numeric -- # since there are many cols we want to transform to indicator variables, it's easier to simply drop the few cols that comprise str (aka, object) data \n",
    "cols_to_indicators = clist_rental.drop(columns =['ids', 'listing_urls', 'region', 'sub_region', 'cities', 'attr_vars', 'listing_descrip', 'sqft', 'prices', 'bedrooms', 'bathrooms', 'date_posted', 'date_of_webcrawler']) \n",
    "cols_to_indicators_lis = list(cols_to_indicators.columns)\n",
    "cols_to_indicators = [] # free space\n",
    "\n",
    "clist_rental = transform_cols_to_indicators(clist_rental, cols_to_indicators_lis)\n",
    "\n",
    "cols_to_indicators_lis = [] # free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of kitchen are now: \n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# also, transform kitchen var separately, since this tends to otherwise convert to float:\n",
    "clist_rental = transform_cols_to_indicators(clist_rental, 'kitchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: \n",
      "1      8675\n",
      "2      3782\n",
      "1.5     573\n",
      "2.5     411\n",
      "3       161\n",
      "3.5      42\n",
      "4        17\n",
      "9+        8\n",
      "5         2\n",
      "5.5       2\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# re: # of bathrooms data, transform any records containing 'shared' or 'split' to 1\n",
    "# Why?: Because we can assume that any rental units comprising a 'shared' bathroom is essentially 1 bathroom\n",
    "def transform_shared_and_split_to_ones(df, col_to_transform):\n",
    "    \"\"\"Transform any records (from given col) containing the string values of 'shared' or 'split' to a value of 1.\"\"\"\n",
    "    # transform col to object, so we can use Python str methods to transform the data\n",
    "    df[col_to_transform] = df[col_to_transform].astype('object') \n",
    "    bedroom_replace_criteria = ['shared', 'split']\n",
    "    bedroom_replace_criteria = '|'.join(bedroom_replace_criteria) # join pipe symbols so we can use str.replace() on multiple 'or' conditions simultaneously \n",
    "    return df[col_to_transform].str.replace(bedroom_replace_criteria,'1')\n",
    "\n",
    "# clean bathrooms data by replacing the 'split' and 'shared' string values:\n",
    "clist_rental['bathrooms'] = transform_shared_and_split_to_ones(clist_rental, 'bathrooms')\n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check: \\n{clist_rental['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value counts for bathrooms data--having cleaned ambiguous records: \n",
      "1      8675\n",
      "2      3782\n",
      "1.5     573\n",
      "2.5     411\n",
      "3       161\n",
      "3.5      42\n",
      "4        17\n",
      "9         8\n",
      "5         2\n",
      "5.5       2\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# replace any ambiguous # of bathrooms data--such as '9+' with empty strings (ie, essentially nulls) \n",
    "def replace_ambiguous_data_with_empty_str(df, col_to_transform):\n",
    "    \"\"\"Replace ambiguous rows of data (ie, any containing a plus sign) for bathrooms col with empty strings\"\"\"\n",
    "    return df[col_to_transform].str.replace(r'\\+', '')  # use str.replace() to use a regex to search for plus signs, and in effect remove these by replacing them with empty strings \n",
    "\n",
    "clist_rental['bathrooms']  = replace_ambiguous_data_with_empty_str(clist_rental, 'bathrooms')\n",
    "# sanity check\n",
    "print(f\"New value counts for bathrooms data--having cleaned ambiguous records: \\n{clist_rental['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining bedroom & bathroom nulls: \n",
      "bedrooms     0\n",
      "bathrooms    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# next, remove any bathroom or bedroom nulls:\n",
    "def remove_bedroom_and_br_nulls(df):\n",
    "    return df.dropna(subset=['bedrooms', 'bathrooms'])\n",
    "\n",
    "clist_rental = remove_bedroom_and_br_nulls(clist_rental)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining bedroom & bathroom nulls: \\n{clist_rental[['bedrooms', 'bathrooms']].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check on data type of bathrooms data: float64\n"
     ]
    }
   ],
   "source": [
    "# transform bathrooms data to float\n",
    "# Why float?: Because some listings specify half bathrooms--e.g., 1.5 denotes one-and-half bathrooms. Re: ids, integer data type not store the entire id value due to maximum (byte) storage constraints. \n",
    "def transform_cols_to_float(df, col_to_transform):\n",
    "    return df[col_to_transform].astype('float')\n",
    "\n",
    "# convert bathrooms to float:\n",
    "clist_rental['bathrooms'] = transform_cols_to_float(clist_rental, 'bathrooms')    \n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check on data type of bathrooms data: {clist_rental['bathrooms'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of ['prices', 'bedrooms', 'ids', 'sqft'] are now: \n",
      "prices      int64\n",
      "bedrooms    int64\n",
      "ids         int64\n",
      "sqft        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def transform_cols_to_int(df, list_of_cols_to_num):\n",
    "    \"\"\" Transform relevant attribute columns to numeric.\n",
    "    NB: Since the scraped 'prices' data can contain commas, \n",
    "    we need to use str.replace(',','') to remove them before converting to numeric.\"\"\"\n",
    "    df['prices'] = df['prices'].str.replace(\",\",\"\") # remove commas from prices data (e.g.: '2500' vs '$2,500')\n",
    "    df[list_of_cols_to_num] = df[list_of_cols_to_num].astype('int64') # use int64 due to a) the long id values & b.) the occasional null values contained within the sqft col\n",
    "    print(f\"Sanity check: The data types of {list_of_cols_to_num} are now: \\n{df[list_of_cols_to_num].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "# specify a list of cols to convert to integer\n",
    "cols_to_int = clist_rental[['prices', 'bedrooms', 'ids', 'sqft']]\n",
    "cols_to_int_lis = list(cols_to_int.columns)  # convert relevant cols to list of col names\n",
    "\n",
    "cols_to_int = [] # free space\n",
    "\n",
    "clist_rental = transform_cols_to_int(clist_rental, cols_to_int_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>date_of_webcrawler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-29 15:32:00</td>\n",
       "      <td>2021-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-29 15:31:00</td>\n",
       "      <td>2021-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-22 21:35:00</td>\n",
       "      <td>2021-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-10-19 10:06:00</td>\n",
       "      <td>2021-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-10-29 15:10:00</td>\n",
       "      <td>2021-10-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_posted date_of_webcrawler\n",
       "1 2021-10-29 15:32:00         2021-10-29\n",
       "2 2021-10-29 15:31:00         2021-10-29\n",
       "4 2021-10-22 21:35:00         2021-10-29\n",
       "5 2021-10-19 10:06:00         2021-10-29\n",
       "6 2021-10-29 15:10:00         2021-10-29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_cols_to_datetime(df, col_to_convert):\n",
    "    \"\"\"Transform relevant column(s) to datetime using pd.to_datetime() method, and use infer_datetime_format=True to enable allow for datetime conversion using differing formats (ie, date_posted has a somewhat more precise format). \"\"\"\n",
    "    return pd.to_datetime(df[col_to_convert], infer_datetime_format=True)\n",
    "\n",
    "# apply transformations to datetime for the 2 relevant cols:\n",
    "clist_rental['date_of_webcrawler'] =  transform_cols_to_datetime(clist_rental,'date_of_webcrawler')\n",
    "clist_rental['date_posted'] = transform_cols_to_datetime(clist_rental,'date_posted')\n",
    "\n",
    "#sanity check\n",
    "clist_rental[['date_posted', 'date_of_webcrawler']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be no remaining duplicate listing ids (ie, 0 rows): \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_df(df):\n",
    "    \"\"\"Remove duplicate rows based on listing ids\"\"\"\n",
    "    return df.drop_duplicates(keep='first', subset = ['ids'])\n",
    "\n",
    "clist_rental = deduplicate_df(clist_rental)\n",
    "\n",
    "# sanity check -- \n",
    "clist_duplicate_ids_check = clist_rental[clist_rental.duplicated(\"ids\", keep= False)]\n",
    "print(f\"There should be no remaining duplicate listing ids (ie, 0 rows): \\n{clist_duplicate_ids_check.shape[0]}\")  # check that number of duplicate rows is false (ie, wrt duplicate listing ids)\n",
    "\n",
    "# free memory\n",
    "clist_duplicate_ids_check = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check--The remaining columns in the dataset are:\n",
      " Index(['ids', 'sqft', 'cities', 'prices', 'bedrooms', 'bathrooms', 'attr_vars',\n",
      "       'listing_descrip', 'date_of_webcrawler', 'kitchen', 'date_posted',\n",
      "       'region', 'sub_region', 'cats_OK', 'dogs_OK', 'wheelchair_accessible',\n",
      "       'laundry_in_bldg', 'no_laundry', 'washer_and_dryer',\n",
      "       'washer_and_dryer_hookup', 'laundry_on_site', 'full_kitchen',\n",
      "       'dishwasher', 'refrigerator', 'oven', 'flooring_carpet',\n",
      "       'flooring_wood', 'flooring_tile', 'flooring_hardwood', 'flooring_other',\n",
      "       'apt_type', 'in_law_apt_type', 'condo_type', 'townhouse_type',\n",
      "       'cottage_or_cabin_type', 'single_fam_type', 'duplex_type',\n",
      "       'is_furnished', 'attached_garage', 'detached_garage', 'carport',\n",
      "       'off_street_parking', 'no_parking', 'EV_charging', 'air_condition',\n",
      "       'no_smoking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def remove_col_with_given_starting_name(df, col_starting_name):\n",
    "    \"\"\"Remove each column from df that has a given starting name substring.\"\"\"\n",
    "    return df.loc[:, ~df.columns.str.startswith(col_starting_name)] \n",
    "\n",
    "# remove 'Unnamed' columns, which might be imported errouneously via pd.read_csv()\n",
    "clist_rental = remove_col_with_given_starting_name(clist_rental, 'Unnamed')\n",
    "\n",
    "# remove listing_urls column since we do not want to store these data into the SQL Server table-- why?: a.) because listing urls are not relevent to rental prices and b.) the listing urls quickly become invalid or dead links, so we have no need to refer back to them at this stage in the webscraping project.\n",
    "clist_rental = remove_col_with_given_starting_name(clist_rental, 'listing_urls')\n",
    "\n",
    "# sanity check\n",
    "print(f\"Sanity check--The remaining columns in the dataset are:\\n {clist_rental.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data cleaning Step:\n",
    "\n",
    "## Determine if there are any rental listing duplicates between the rental table and the datetime-filtered DataFrame.\n",
    "\n",
    "## If any such duplicates exist, then we need to filter these duplicates out of the datetime-filtered DataFrame.\n",
    "\n",
    "## Implement SQL Query to determine whether there any duplicates in the datetime-filtered DataFrame relative to the SQL rental table:\n",
    "\n",
    "### I.e., use a query with an IN() operator to check for a list of all listing id values from the datetime-filtered DataFrame, and compare those values with all stored rental listings data in the rental table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify whether any of the listing id's in the filtered DataFrame dataset are already stored within the rental listing dataset\n",
    "\n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def check_for_listing_ids_via_SQL_in_operator(self, df):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        ## Perform SQL query on rental table to determine whether there are any rental listing duplicates--ie, listing ids from the filtered DataFrame that have already been inserted into the rental table--via SQL IN operator:\n",
    "        \n",
    "        ## get a list of all listing ids from the datetime-filtered dataframe:\n",
    "        df = df.astype(str)  # convert dataframe to string, so we can enable pandas' df data to be more compatible with pyodbc library \n",
    "        clist_rental_filtered_ids = df['ids'].to_list()  # obtain a list of all ids from the filtered df\n",
    "\n",
    "        # get various '?' SQL placeholders (ie, to prevent SQL injections) and a comma for each elements of the list of ids values--NB: reference the len() of the clist_rental_filtered_ids list to get the proper number of placeholders\n",
    "        q_placeholders = \",\".join(\"?\" * len(clist_rental_filtered_ids))  # get '?' placeholders and comma for each id element from the clist_rental_filtered_ids list\n",
    "\n",
    "        ## specify query, and add placeholders in between each value being checked via the IN operator\n",
    "        sql_query = \"SELECT * FROM rental WHERE listing_id IN (%s)\" % q_placeholders\n",
    "        \n",
    "        ## use Pandas' read_sql() method to parse the query, and use the id values of the df--ie, from the clist_rental_filtered_ids list--as the argument of the IN operator: \n",
    "        id_query_duplicates = pd.read_sql(sql_query, conn, params=clist_rental_filtered_ids)  # query for any listings that have duplicate id's from the rental table relative to the \n",
    "        \n",
    "        # sanity check-- Are there any duplicate listings in the SQL table vs the datetime-filtered DataFrame?\n",
    "        print(f\"Duplicate listings from the datetime-filtered dataframe relative to the SQL rental table: \\n{id_query_duplicates}\")        \n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return id_query_duplicates\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# perform query to check for any listings that are duplicate between the data in the rental table and the datetime-filtered dataframe \n",
    "id_query_duplicates = SQL_db.check_for_listing_ids_via_SQL_in_operator(clist_rental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Cleaning step-- cont'd:\n",
    "\n",
    "## If there are any rental listing duplicates between the rental table and the datetime-filtered DataFrame, then we clearly need to provide an additional filter on this DataFrame:\n",
    "\n",
    "## Namely: filter out any of these duplicate listings from the datetime-filtered pandas' DataFrame based on the above query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataframe to remove any duplicate ids\n",
    "def remove_duplicate_ids_relative_to_SQL_table(df, id_query_duplicates):\n",
    "    \"\"\"Remove any records from df whose ids are already inserted into (ie, duplicates of) the rental SQL table, as given by the id_query_duplicates query results\"\"\"\n",
    "    id_query_duplicates_list = id_query_duplicates['listing_id'].astype(float).apply(int).to_list() # derive a list of duplicate ids from the query results' listing_id column\n",
    "    # filtered_df = df[~df['ids].isin(id_query_duplicates_list)]  # remove (ie, filter out) all duplicate ids using the negation of isin() \n",
    "    filtered_df = df[~df['ids'].isin(id_query_duplicates_list)]  # remove (ie, filter out) all duplicate ids using the negation of isin() \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# apply the filter to remove duplicate ids, based on the query results (ie, id_query_duplicates)\n",
    "clist_rental = remove_duplicate_ids_relative_to_SQL_table(clist_rental, id_query_duplicates)\n",
    "\n",
    "clist_rental.info() # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step-- Data pipeline and data ingestion: \n",
    "\n",
    "## Insert the cleaned/filtered/deduplicated pandas' DataFrame into SQL rental table:\n",
    "\n",
    "#### NB: When using pyodbc to insert data from a dataframe into SQL Server table, we *may* need to transform all of the data from the dataframe to string, if SQL Server returns an error during an insertion attempt. This is because pyodbc will typically transform string values of numeric to SQL numeric data types such as int or float. \n",
    "\n",
    "#### Regardless, when using pyodbc to insert data into a SQL Server table, *always* use the  execute_many = True option so that the INSERT INTO statements will not be called upon as frequently. Ie, execute_many helps ensure that there will not be separate inserts for each row, as this is entirely unneccessary and extremely inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, insert all scraped data into the rental SQL Server table:\n",
    "\n",
    "\n",
    "\n",
    "### Insert scraped data from Pandas' dataframe to SQL Server rental table via pyodbc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n",
      "The number of records stored in the SQL table is: (11713, )\n",
      "\n",
      "A few of the inserted records are: [(7384250691, 1900, 'Oakland', 2950, 4, Decimal('1.5'), 'flooring: wood\\nhouse\\nw/d hookups\\nno smoking\\noff-street parking\\nrent period: monthly', datetime.datetime(2021, 10, 29, 0, 0), 1, datetime.datetime(2021, 9, 22, 22, 12), 'sfbay', 'eby', 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1), (7384475532, 800, 'Oakland', 2300, 2, Decimal('1.0'), 'cats are OK - purrr\\ndogs are OK - wooof\\nflooring: other\\napartment\\nlaundry on site\\nno smoking\\ncarport\\nrent period: monthly', datetime.datetime(2021, 10, 29, 0, 0), 0, datetime.datetime(2021, 9, 23, 11, 4), 'sfbay', 'eby', 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1), (7384613497, 635, 'Oakland', 2107, 1, Decimal('1.0'), 'application fee details: $45.00 per Applicant\\ncats are OK - purrr\\ndogs are OK - wooof\\nflooring: other\\napartment\\nw/d in unit\\nno smoking\\nattached garage\\nrent period: monthly\\nwheelchair accessible', datetime.datetime(2021, 10, 29, 0, 0), 0, datetime.datetime(2021, 9, 23, 15, 26), 'sfbay', 'eby', 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1)]\n"
     ]
    }
   ],
   "source": [
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def insert_df_to_SQL_ETL(self, df):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        cursor.fast_executemany = True  # speed up data ingesting by reducing the numbers of calls to server for inserts\n",
    "\n",
    "        # convert all variables from dataframe to str to avoid following SQL Server pyodbc error: 'ProgrammingError: ('Invalid parameter type.  param-index=2 param-type=function', 'HY105')'\n",
    "        df = df.astype(str) # convert all df variables to str for ease of loading data into SQl Server table\n",
    "        \n",
    "        # insert scraped data from df to SQL table-- iterate over each row of each df col via .itertuples() method\n",
    "\n",
    "        # # NB: since there are 45 cols we will insert, we will need 45 '?' char marks  \n",
    "        q_mark_list = ['?']*45\n",
    "\n",
    "        # # # unpack list as string, and join() commas to each '?' char\n",
    "        q_mark_str = ','.join(q_mark_list)\n",
    "        \n",
    "        # Get the number of needed '?' placeholders by looking up the # of cols (ie, len()) of the dataframe), and use .join() to have each question mark seprated by commas:\n",
    "        # q_mark_str = ','.join('?'*len(clist_rental.columns))  \n",
    "\n",
    "        # specify INSERT INTO SQL statement--iterate over each row in df, and insert into SQL database:\n",
    "        for row in df.itertuples():  # iterate over each row from df\n",
    "            cursor.execute(f\"\"\"INSERT INTO rental (listing_id, sqft, city, price, bedrooms, bathrooms, attr_vars,\n",
    "            date_of_webcrawler, kitchen, date_posted, region, sub_region, cats_OK, dogs_OK, wheelchair_accessible,laundry_in_bldg, no_laundry, \n",
    "            washer_and_dryer, washer_and_dryer_hookup, laundry_on_site, full_kitchen, dishwasher, refrigerator,\n",
    "            oven,flooring_carpet, flooring_wood, flooring_tile, flooring_hardwood, flooring_other,apt_type, in_law_apt_type, condo_type, townhouse_type, cottage_or_cabin_type, single_fam_type, duplex_type, is_furnished, attached_garage,\n",
    "            detached_garage, carport, off_street_parking, no_parking, EV_charging, air_condition, no_smoking) \n",
    "            VALUES ({q_mark_str})\"\"\",\n",
    "            (row.ids,\n",
    "            row.sqft,\n",
    "            row.cities,\n",
    "            row.prices, \n",
    "            row.bedrooms,\n",
    "            row.bathrooms,\n",
    "            row.attr_vars, \n",
    "            row.date_of_webcrawler,\n",
    "            row.kitchen,\n",
    "            row.date_posted,\n",
    "            row.region,\n",
    "            row.sub_region,\n",
    "            row.cats_OK,\n",
    "            row.dogs_OK,\n",
    "            row.wheelchair_accessible,\n",
    "            row.laundry_in_bldg, \n",
    "            row.no_laundry,\n",
    "            row.washer_and_dryer,\n",
    "            row.washer_and_dryer_hookup,\n",
    "            row.laundry_on_site,\n",
    "            row.full_kitchen,\n",
    "            row.dishwasher,\n",
    "            row.refrigerator,\n",
    "            row.oven,\n",
    "            row.flooring_carpet,\n",
    "            row.flooring_wood,\n",
    "            row.flooring_tile,\n",
    "            row.flooring_hardwood,\n",
    "            row.flooring_other,\n",
    "            row.apt_type, \n",
    "            row.in_law_apt_type,\n",
    "            row.condo_type,\n",
    "            row.townhouse_type,\n",
    "            row.cottage_or_cabin_type,\n",
    "            row.single_fam_type, \n",
    "            row.duplex_type,\n",
    "            row.is_furnished,\n",
    "            row.attached_garage,\n",
    "            row.detached_garage,\n",
    "            row.carport,\n",
    "            row.off_street_parking,\n",
    "            row.no_parking,\n",
    "            row.EV_charging,\n",
    "            row.air_condition,\n",
    "            row.no_smoking)\n",
    "            )\n",
    "            \n",
    "        # save and commit changes to database\n",
    "        conn.commit()\n",
    "\n",
    "        # # sanity check-- ensure some data has been inserted into new SQL table\n",
    "        sql_table_count_records = conn.execute(\"\"\"SELECT COUNT(*) FROM rental;\"\"\").fetchall()\n",
    "        print(f\"The number of records stored in the SQL table is: {sql_table_count_records[0]}\")     \n",
    "        \n",
    "        sql_query_for_record_samples = conn.execute(\"\"\"SELECT TOP 3 * FROM rental;\"\"\").fetchall() # check out several of the records\n",
    "        print(f\"\\nA few of the inserted records are: {sql_query_for_record_samples}\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# Ingest data from pandas' dataframe to SQL server--data pipeline: \n",
    "SQL_db.insert_df_to_SQL_ETL(clist_rental)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "868f069ec03d251bf5304722b36a079032b062039e590e3d2c740be1c52152d4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
