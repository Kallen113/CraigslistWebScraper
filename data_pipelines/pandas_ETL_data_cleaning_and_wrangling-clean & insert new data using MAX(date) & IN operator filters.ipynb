{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 in the pandas df to SQL Server data pipeline\n",
    "\n",
    "### NB: In Part 2, we assume that we've already run the Part 1 notebook (or equivalent scripts), the latter of which is intended to be run only *once*. However, the scripts, functions, and methods shown here can (and should) be re-used every time we have scraped more data and need to insert the new data into the SQL rental table. \n",
    "\n",
    "##### I.e.: the rental table has already been created within the craigslist database, and we have successfully inserted at least some data into the rental table!\n",
    "\n",
    "##### If so, then we will do the following here in Part 2: check for the last date of inserted date--ie, MAX() of date_possted--and then filter the scraped data > the MAX()date in the SQL table, clean the data using the various data type transformations, deduplications, removing nulls, etc. \n",
    "\n",
    "#### As a final data filtering step, we need to double-check whether any of these rental listings have already been inserted into the rental table using a SQL query in which we can employ the IN() operator . For example: rental listings might be taken down and then re-posted, and these listings--albeit quite rare--could have the same listing ids as an older listing that we've already stored in the rental table, even though these \"new\" listings would initially seem to be unique listings based purely on their more recent date_posted datetime value!\n",
    "\n",
    "#### After the data have been filtered and cleaned, we can then insert the new data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports-- file processing & json libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# SQL ODBC for API connection between Python & SQL Server\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22428 entries, 0 to 22427\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   listing_urls             22428 non-null  object \n",
      " 1   ids                      22259 non-null  float64\n",
      " 2   sqft                     17148 non-null  object \n",
      " 3   cities                   22311 non-null  object \n",
      " 4   prices                   22255 non-null  object \n",
      " 5   bedrooms                 22126 non-null  float64\n",
      " 6   bathrooms                22255 non-null  object \n",
      " 7   attr_vars                22256 non-null  object \n",
      " 8   listing_descrip          22256 non-null  object \n",
      " 9   date_of_webcrawler       22313 non-null  object \n",
      " 10  kitchen                  22311 non-null  float64\n",
      " 11  date_posted              22256 non-null  object \n",
      " 12  region                   22428 non-null  object \n",
      " 13  sub_region               22428 non-null  object \n",
      " 14  cats_OK                  22428 non-null  int64  \n",
      " 15  dogs_OK                  22428 non-null  int64  \n",
      " 16  wheelchair_accessible    22428 non-null  int64  \n",
      " 17  laundry_in_bldg          22428 non-null  int64  \n",
      " 18  no_laundry               22428 non-null  int64  \n",
      " 19  washer_and_dryer         22428 non-null  int64  \n",
      " 20  washer_and_dryer_hookup  22428 non-null  int64  \n",
      " 21  laundry_on_site          22428 non-null  int64  \n",
      " 22  full_kitchen             22428 non-null  int64  \n",
      " 23  dishwasher               22428 non-null  int64  \n",
      " 24  refrigerator             22428 non-null  int64  \n",
      " 25  oven                     22428 non-null  int64  \n",
      " 26  flooring_carpet          22428 non-null  int64  \n",
      " 27  flooring_wood            22428 non-null  int64  \n",
      " 28  flooring_tile            22428 non-null  int64  \n",
      " 29  flooring_hardwood        22428 non-null  int64  \n",
      " 30  flooring_other           22428 non-null  int64  \n",
      " 31  apt                      22428 non-null  int64  \n",
      " 32  in_law_apt               22428 non-null  int64  \n",
      " 33  condo                    22428 non-null  int64  \n",
      " 34  townhouse                22428 non-null  int64  \n",
      " 35  cottage_or_cabin         22428 non-null  int64  \n",
      " 36  single_fam               22428 non-null  int64  \n",
      " 37  duplex                   22428 non-null  int64  \n",
      " 38  flat                     17019 non-null  float64\n",
      " 39  land                     17019 non-null  float64\n",
      " 40  is_furnished             22428 non-null  int64  \n",
      " 41  attached_garage          22428 non-null  int64  \n",
      " 42  detached_garage          22428 non-null  int64  \n",
      " 43  carport                  22428 non-null  int64  \n",
      " 44  off_street_parking       22428 non-null  int64  \n",
      " 45  no_parking               22428 non-null  int64  \n",
      " 46  EV_charging              22428 non-null  int64  \n",
      " 47  air_condition            22428 non-null  int64  \n",
      " 48  no_smoking               22428 non-null  int64  \n",
      "dtypes: float64(5), int64(33), object(11)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files to a single Pandas' df using pd.concat().\"\"\"\n",
    "    path =  parent_direc # specify parent path of directories containing the scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes??\n",
    "    df_concat = pd.concat((pd.read_csv(file, # import each CSV file from directory\n",
    "                                        sep=',',encoding = 'utf-8'  \n",
    "                                        ) for file in glob.iglob(\n",
    "                                            os.path.join(path, '**', fn_regex), \n",
    "                                            recursive=True)), ignore_index=True)  # os.path.join helps ensure this concatenation is OS independent\n",
    "    return df_concat\n",
    "\n",
    "## Import Dataset\n",
    "# import all scraped SF bay area rental listings data\n",
    "scraped_data_path = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\scraped_data\\\\sfbay\"\n",
    "\n",
    "df = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine last date (ie, MAX()) of the data stored in the rental table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n",
      "Latest date of scraped data inserted into the SQL table:\n",
      "  latest_date\n",
      "0        None\n"
     ]
    }
   ],
   "source": [
    "# Perform SQL query on the date_posted col to determine the most recent date of data stored in the table  \n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def determine_latest_date(self, sql_query):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rentals' table\"\"\"\n",
    "\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        # specify SQL query\n",
    "        sql_query = sql_query \n",
    "\n",
    "        # perform query, and convert query results to Pandas' df\n",
    "        max_date = pd.read_sql(sql_query, conn)\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        ## sanity check:\n",
    "        print(f\"Latest date of scraped data inserted into the SQL table:\\n{max_date}\")\n",
    "\n",
    "        return max_date\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "\n",
    "# specify query to select the latest date based on date_posted:\n",
    "query = \"SELECT MAX(date_posted) AS latest_date FROM rental;\"\n",
    "\n",
    "latest_date = SQL_db.determine_latest_date(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we filter the scraped listings dataset (ie, dataframe), we need to transform the 'date_posted' column--ie, the one that we will filter--to a standardized datetime format\n",
    "\n",
    "#### Why?: To ensure consistency and replicability for this data pipeline (ie, of the pandas' DataFrame to SQL Server table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>date_of_webcrawler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-27 17:45:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-03 00:49:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-03 00:20:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-10 13:16:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-19 02:14:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_posted date_of_webcrawler\n",
       "0 2021-12-27 17:45:00         2022-01-03\n",
       "1 2022-01-03 00:49:00         2022-01-03\n",
       "2 2022-01-03 00:20:00         2022-01-03\n",
       "3 2021-12-10 13:16:00         2022-01-03\n",
       "4 2021-12-19 02:14:00         2022-01-03"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_cols_to_datetime(df, col_to_convert):\n",
    "    \"\"\"Transform relevant column(s) to datetime using pd.to_datetime() method, and use infer_datetime_format=True to enable allow for datetime conversion using differing formats (ie, date_posted has a somewhat more precise format). \"\"\"\n",
    "    return pd.to_datetime(df[col_to_convert], infer_datetime_format=True)\n",
    "\n",
    "# apply transformations to datetime for the 2 relevant cols:\n",
    "df['date_of_webcrawler'] =  transform_cols_to_datetime(df,'date_of_webcrawler')\n",
    "df['date_posted'] = transform_cols_to_datetime(df,'date_posted')\n",
    "\n",
    "#sanity check\n",
    "df[['date_posted', 'date_of_webcrawler']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we need to filter the dataset to listings records whose date_posted dates are newer (read: greater than) the MAX() of the last date found from the query of the SQL rental table.\n",
    "\n",
    "### To this end, we can convert the MAX() value from the SQL query to a str value, and then use this as the argument for the filter_df_since_specified_date() function, which filters on the 'date_posted' column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest date among the scraped data stored in the SQL table is:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## next, convert this latest_date to a string value, so we can use this to filter the scraped dataframe dataset\n",
    "def datetime_col_to_str_of_datetime(df, datetime_col):\n",
    "    \"\"\"Given datetime col from pandas' DataFrame,\n",
    "    transform to a string of the datetime value.\"\"\"\n",
    "    return df[datetime_col].head(1).astype(str).reset_index().loc[0, datetime_col] \n",
    "\n",
    "# specify name of query result DataFrame and the corresponding datetime col:\n",
    "latest_date, dt_col = latest_date, 'latest_date' \n",
    "#apply function using the 2 arguments shown above\n",
    "latest_date_str = datetime_col_to_str_of_datetime(latest_date, dt_col)\n",
    "# sanity check\n",
    "print(f\"The latest date among the scraped data stored in the SQL table is:\\n{latest_date_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, filter on the date_posted date, so that we only have the new listings data that we have not yet inserted into the SQL table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newest scraped data not stored in the SQL table is--\n",
      "*NB: this should be an empty df if we have already stored all of the df's data into the SQL table*: \n",
      "\n",
      "0       2021-12-27 17:45:00\n",
      "1       2022-01-03 00:49:00\n",
      "2       2022-01-03 00:20:00\n",
      "3       2021-12-10 13:16:00\n",
      "4       2021-12-19 02:14:00\n",
      "                ...        \n",
      "22423   2022-02-03 15:05:00\n",
      "22424   2022-02-03 14:52:00\n",
      "22425   2022-02-03 14:27:00\n",
      "22426   2022-02-03 14:45:00\n",
      "22427   2022-01-16 17:45:00\n",
      "Name: date_posted, Length: 22428, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# filter the dataframe > MAX() of latest_date stored in SQL rental table\n",
    "def filter_df_since_specified_date(df, target_date: str):\n",
    "    \"\"\"Filter the imported scraped dataset to all data newer than the specified date (as determined via the MAX(posted_date) query).\"\"\"\n",
    "    if target_date != \"None\":   # account for scenario in which *no data* has yet been inserted into the SQL table\n",
    "        df = df.loc[df['date_posted'] > target_date]  # filter to data greater than specified date\n",
    "    else: \n",
    "        pass  # do not apply filter, sin`ce no data has yet been inserted into SQL database\n",
    "    return df\n",
    "\n",
    "# get all data since the latest stored data from SQL table (via the query on MAX(posted_date)) \n",
    "df = filter_df_since_specified_date(df, latest_date_str)\n",
    "\n",
    "# sanity check\n",
    "print(f\"The newest scraped data not stored in the SQL table is--\\n*NB: this should be an empty df if we have already stored all of the df's data into the SQL table*: \\n\\n{df['date_posted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, perform all additional data cleaning and wrangling features to prep the data for the SQL inserts, as in the Part 1 jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining price, listing id, sqft, kitchen, & city name nulls: \n",
      "prices     0\n",
      "ids        0\n",
      "sqft       0\n",
      "kitchen    0\n",
      "cities     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def remove_nulls_list(df, list_of_cols):\n",
    "    \"\"\"Remove rows that do not have price, city name, kitchen, sqft, or listing ID data, as these are essential variables in this rental listings dataset.\"\"\"\n",
    "    return df.dropna(subset=list_of_cols)\n",
    "\n",
    "list_cols_to_remove_nulls = ['prices', 'ids', 'sqft', 'kitchen', 'cities']  \n",
    "df = remove_nulls_list(df, list_cols_to_remove_nulls)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining price, listing id, sqft, kitchen, & city name nulls: \\n{df[list_cols_to_remove_nulls].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check--after cleaning the city names, let's examine some of the cleaned data: Napa             1\n",
      "Summerville      1\n",
      "Martinez         1\n",
      "Pinole           1\n",
      "Orinda           1\n",
      "Discovery Bay    1\n",
      "Monte Rio        1\n",
      "Sonoma Valley    1\n",
      "San Pablo        1\n",
      "Bonny Doon       1\n",
      "Name: cities, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "def clean_split_city_names(df, address_critera: list, neighborhood_criteria:list, split_city_delimiters: list, incorrect_city_names:dict, cities_not_in_region:dict, cities_that_need_extra_cleaning:dict):\n",
    "    \"\"\"Clean city names data in several ways:\n",
    "    a.) Remove extraneous address & neighborhood data placed in the city names HTML object, such as 'Rd', 'Blvd', or 'Downtown'.\n",
    "    b.) Unsplit city names data that are split via ',' & '/' delimiters.\n",
    "    c.) Replace abbreviated or mispelled city names, and remove city names that do not exist within the SF Bay Area (e.g., 'Redding').\n",
    "    d.) Remove any digits/integers within the city names data--ie, by using a '\\d+' regex as the argument of str.replace() and replace it with empty strings.\n",
    "    e.) Remove any city names records thast are left with merely empty strings (ie, the other steps removed all data for that given cities record).\n",
    "    f.) Remove any whitespace to avoid the same city names from being treated as different entities by Pandas, Python, or SQL. \n",
    "    g.) Use str.capwords() to capitalize words (ie, excluding apostrophes).\n",
    "    h.) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena'. \"\"\"\n",
    "    # specify extraneous street & address data (e.g., 'Rd') that we want to remove from the city names column:\n",
    "    addr_criteria = '|'.join(address_critera) # Join pipe ('|') symbols to address list so we can str.split() on any one of these criteria (ie, 'or' condition splitting on each element separated by pipes):\n",
    "    # specify extraneous neighborhood criteria we should also remove from col\n",
    "    nbhood_criteria = '|'.join(neighborhood_criteria) # remove neighborhood names as well as state abbreviation (shown on website as 'Ca') that is shown without the usual comma delimiter!\n",
    "    # b.) specify delimiters we need to refer to un-split city names:\n",
    "    split_city_delimiters = '|'.join(split_city_delimiters) # join pipes to delimiters so we can use str.split() based on multiple 'or' criteria simultaneously\n",
    "    # clean city names data by removing extraneous address & neighborhood data, and unsplitting city names based on ',' & '\\' delimiters\n",
    "    df['cities'] =  df['cities'].str.split(addr_criteria).str[-1].str.replace(nbhood_criteria, '', case=True).str.lstrip()\n",
    "    df['cities'] = df['cities'].str.split(split_city_delimiters).str[0] #unsplit city names based on comma or forward-slash delimiters\n",
    "    # c.) replace specific abbreviated or mispelled city names, and remove cities that are not actually located in the sfbay region:\n",
    "    df = df.replace({'cities':incorrect_city_names}) # replace mispelled & abbreviated city names\n",
    "    df = df.replace({'cities':cities_not_in_region})  # remove (via empty string) cities that are not actually located in the sfbay region\n",
    "    # d.) Remove digits/integer-like data from cities column:\n",
    "    df['cities'] = df['cities'].str.replace('\\d+', '')  # remove any digits by using '/d+' regex to look up digits, and then replace with empty string\n",
    "    # e.) Remove any rows that have empty strings or null values for cities col (having performed the various data filtering and cleaning above)\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    # f.) Remove whitespace\n",
    "    df['cities'] = df['cities'].str.strip() \n",
    "    # g.) capitalize the city names using str.capwords() \n",
    "    df['cities'] = df['cities'].str.split().apply(lambda x: [val.capitalize() for val in x]).str.join(' ')\n",
    "    # h) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena' & 'San los' should be 'San Carlos'. Also, remove any non-Bay Area cities such as Redding:\n",
    "    # df['cities'] = df['cities'].str.lower() # transform all records to lower-case, for ease of cleaning the data\n",
    "    df = df.replace({'cities':cities_that_need_extra_cleaning})\n",
    "    return df\n",
    "\n",
    "# specify various address and street name that we need to remove from the city names \n",
    "address_criteria = ['Boulevard', 'Blvd', 'Road', 'Rd', 'Avenue', 'Ave', 'Street', 'St', 'Drive', 'Dr', 'Real', 'E Hillsdale Blvd'] \n",
    "# specify various extraneous neighborhood names such as 'Downtown' \n",
    "neighborhood_criteria = ['Downtown', 'Central/Downtown', 'North', 'California', 'Ca.', 'Bay Area', 'St. Helena', 'St', 'nyon', \n",
    "'Jack London Square', 'Walking Distance To', 'El Camino', 'Mendocino County', 'San Mateo County', 'Alameda County', 'Rio Nido Nr', 'Mission Elementary', \n",
    "'Napa County', 'Golden Gate', 'Jennings', 'South Lake Tahoe', 'Tahoe Paradise', 'Kingswood Estates', 'South Bay', 'Skyline', 'San Antonio Tx', \n",
    "'East Bay', 'Morton Dr'] \n",
    "\n",
    "# specify what delimiters we want to search for to unsplit the split city names data:\n",
    "split_city_delimiters =  [',', '/', ' - ']\n",
    "# specify dictionary of abbreviated & mispelled cities:\n",
    "incorrect_city_names = {'Rohnert Pk':'Rohnert Park', 'Hillsborough Ca': 'Hillsborough', 'South Sf': 'South San Francisco', 'Ca':'', 'East San Jose':'San Jose', 'Vallejo Ca':'Vallejo', 'Westgate On Saratoga .':'San Jose', 'Bodega':'Bodega Bay', 'Briarwood At Central Park':'Fremont', 'Campbell Ca':'Campbell', 'Almaden':'San Jose', '.':'', 'East Foothills':'San Jose', 'Lake County':'', 'West End':'Alameda', 'Redwood Shores':'Redwood City'}\n",
    "\n",
    "# specify dictionary of cities that are not located in sfbay (ie, not located in the region):\n",
    "cities_not_in_region = {'Ketchum':'', 'Baypoinr':'', 'Quito': '', 'Redding':'', 'Bend' :''}\n",
    "\n",
    "# specify dictionary of city names that are mispelled after having removed various street and neighborhood substrings:\n",
    "cities_that_need_extra_cleaning = {'. Helena': 'St. Helena', '. Helena Deer Park': 'St. Helena', 'San Los':'San Carlos', 'Tro Valley':'Castro Valley', 'Rohnert Pk':'Rohnert Park',\n",
    "'Pbell':'Campbell', 'Pbell Ca':'Campbell', 'American Yon':'American Canyon', 'Millbrae On The Burlingame Border':'Millbrae', 'Ockton Ca': 'Stockton', '. Rohnert Park': 'Rohnert Park'}\n",
    "\n",
    "# clean city names data:\n",
    "df = clean_split_city_names(df, address_criteria, neighborhood_criteria, split_city_delimiters, incorrect_city_names, cities_not_in_region, cities_that_need_extra_cleaning)\n",
    "# sanity check\n",
    "print(f\"Sanity check--after cleaning the city names, let's examine some of the cleaned data: {df.cities.value_counts().tail(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of ['kitchen', 'cats_OK', 'dogs_OK', 'wheelchair_accessible', 'laundry_in_bldg', 'no_laundry', 'washer_and_dryer', 'washer_and_dryer_hookup', 'laundry_on_site', 'full_kitchen', 'dishwasher', 'refrigerator', 'oven', 'flooring_carpet', 'flooring_wood', 'flooring_tile', 'flooring_hardwood', 'flooring_other', 'apt', 'in_law_apt', 'condo', 'townhouse', 'cottage_or_cabin', 'single_fam', 'duplex', 'flat', 'land', 'is_furnished', 'attached_garage', 'detached_garage', 'carport', 'off_street_parking', 'no_parking', 'EV_charging', 'air_condition', 'no_smoking'] are now: \n",
      "kitchen                    float64\n",
      "cats_OK                      uint8\n",
      "dogs_OK                      uint8\n",
      "wheelchair_accessible        uint8\n",
      "laundry_in_bldg              uint8\n",
      "no_laundry                   uint8\n",
      "washer_and_dryer             uint8\n",
      "washer_and_dryer_hookup      uint8\n",
      "laundry_on_site              uint8\n",
      "full_kitchen                 uint8\n",
      "dishwasher                   uint8\n",
      "refrigerator                 uint8\n",
      "oven                         uint8\n",
      "flooring_carpet              uint8\n",
      "flooring_wood                uint8\n",
      "flooring_tile                uint8\n",
      "flooring_hardwood            uint8\n",
      "flooring_other               uint8\n",
      "apt                          uint8\n",
      "in_law_apt                   uint8\n",
      "condo                        uint8\n",
      "townhouse                    uint8\n",
      "cottage_or_cabin             uint8\n",
      "single_fam                   uint8\n",
      "duplex                       uint8\n",
      "flat                       float64\n",
      "land                       float64\n",
      "is_furnished                 uint8\n",
      "attached_garage              uint8\n",
      "detached_garage              uint8\n",
      "carport                      uint8\n",
      "off_street_parking           uint8\n",
      "no_parking                   uint8\n",
      "EV_charging                  uint8\n",
      "air_condition                uint8\n",
      "no_smoking                   uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def transform_cols_to_indicators(df, list_of_cols):\n",
    "    \"\"\" Transform relevant attribute columns to numeric, and specify NaNs for any missing or non-numeric data.\"\"\"\n",
    "    df[list_of_cols] = df[list_of_cols].astype('uint8', errors='ignore') # convert any missing data to NaN \n",
    "    df[list_of_cols] = df[list_of_cols].fillna(0) # impute any NaN values with 0s since we can be quite certain that any rental listings not explicitly specifying a specific property type, amenity, or attribute do not actually contain said attribute, amenity, or are not said propery type  \n",
    "    print(f\"Sanity check: The data types of {list_of_cols} are now: \\n{df[list_of_cols].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "# specify a list of cols to convert to numeric -- # since there are many cols we want to transform to indicator variables, it's easier to simply drop the few cols that comprise str (aka, object) data \n",
    "cols_to_indicators = df.drop(columns =['ids', 'listing_urls', 'region', 'sub_region', 'cities', 'attr_vars', 'listing_descrip', 'sqft', 'prices', 'bedrooms', 'bathrooms', 'date_posted', 'date_of_webcrawler']) \n",
    "cols_to_indicators_lis = list(cols_to_indicators.columns)\n",
    "cols_to_indicators = [] # free space\n",
    "\n",
    "df = transform_cols_to_indicators(df, cols_to_indicators_lis)\n",
    "\n",
    "cols_to_indicators_lis = [] # free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of kitchen are now: \n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# also, transform kitchen var separately, since this tends to otherwise convert to float:\n",
    "df = transform_cols_to_indicators(df, 'kitchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: \n",
      "1      10518\n",
      "2       4235\n",
      "1.5      652\n",
      "1.0      452\n",
      "2.5      433\n",
      "3        205\n",
      "2.0      189\n",
      "3.5       74\n",
      "4         35\n",
      "5         25\n",
      "3.0        8\n",
      "9+         4\n",
      "8          3\n",
      "5.5        2\n",
      "4.5        2\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# re: # of bathrooms data, transform any records containing 'shared' or 'split' to 1\n",
    "# Why?: Because we can assume that any rental units comprising a 'shared' bathroom is essentially 1 bathroom\n",
    "def transform_shared_and_split_to_ones(df, col_to_transform):\n",
    "    \"\"\"Transform any records (from given col) containing the string values of 'shared' or 'split' to a value of 1.\"\"\"\n",
    "    # transform col to object, so we can use Python str methods to transform the data\n",
    "    df[col_to_transform] = df[col_to_transform].astype('object') \n",
    "    bedroom_replace_criteria = ['shared', 'split']\n",
    "    bedroom_replace_criteria = '|'.join(bedroom_replace_criteria) # join pipe symbols so we can use str.replace() on multiple 'or' conditions simultaneously \n",
    "    return df[col_to_transform].str.replace(bedroom_replace_criteria,'1')\n",
    "\n",
    "# clean bathrooms data by replacing the 'split' and 'shared' string values:\n",
    "df['bathrooms'] = transform_shared_and_split_to_ones(df, 'bathrooms')\n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check: \\n{df['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value counts for bathrooms data--having cleaned ambiguous records: \n",
      "1      10518\n",
      "2       4235\n",
      "1.5      652\n",
      "1.0      452\n",
      "2.5      433\n",
      "3        205\n",
      "2.0      189\n",
      "3.5       74\n",
      "4         35\n",
      "5         25\n",
      "3.0        8\n",
      "9          4\n",
      "8          3\n",
      "5.5        2\n",
      "4.5        2\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# replace any ambiguous # of bathrooms data--such as '9+' with empty strings (ie, essentially nulls) \n",
    "def replace_ambiguous_data_with_empty_str(df, col_to_transform):\n",
    "    \"\"\"Replace ambiguous rows of data (ie, any containing a plus sign) for bathrooms col with empty strings\"\"\"\n",
    "    return df[col_to_transform].str.replace(r'\\+', '')  # use str.replace() to use a regex to search for plus signs, and in effect remove these by replacing them with empty strings \n",
    "\n",
    "df['bathrooms']  = replace_ambiguous_data_with_empty_str(df, 'bathrooms')\n",
    "# sanity check\n",
    "print(f\"New value counts for bathrooms data--having cleaned ambiguous records: \\n{df['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining bedroom & bathroom nulls: \n",
      "bedrooms     0\n",
      "bathrooms    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# next, remove any bathroom or bedroom nulls:\n",
    "def remove_bedroom_and_br_nulls(df):\n",
    "    return df.dropna(subset=['bedrooms', 'bathrooms'])\n",
    "\n",
    "df = remove_bedroom_and_br_nulls(df)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining bedroom & bathroom nulls: \\n{df[['bedrooms', 'bathrooms']].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Cleaning step-- cont'd:\n",
    "\n",
    "## If there are any rental listing duplicates between the rental table and the datetime-filtered DataFrame, then we clearly need to provide an additional filter on this DataFrame:\n",
    "\n",
    "## Namely: filter out any of these duplicate listings from the datetime-filtered pandas' DataFrame based on the above query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check on data type of bathrooms data: float32\n"
     ]
    }
   ],
   "source": [
    "# transform bathrooms data to float\n",
    "# Why float?: Because some listings specify half bathrooms--e.g., 1.5 denotes one-and-half bathrooms. Re: ids, integer data type not store the entire id value due to maximum (byte) storage constraints. \n",
    "def transform_cols_to_float(df, col_to_transform):\n",
    "    return df[col_to_transform].astype('float32')\n",
    "\n",
    "# convert bathrooms to float:\n",
    "df['bathrooms'] = transform_cols_to_float(df, 'bathrooms')    \n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check on data type of bathrooms data: {df['bathrooms'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of ['bedrooms', 'prices', 'ids', 'sqft'] are now: \n",
      "bedrooms    int64\n",
      "prices      int64\n",
      "ids         int64\n",
      "sqft        int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\pandas\\core\\frame.py:3636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "def transform_cols_to_int(df, list_of_cols_to_num):\n",
    "    \"\"\" Transform relevant attribute columns to numeric.\n",
    "    NB: Since the scraped 'prices' data can contain commas, we need to use str.replace(',','') to remove them before converting to numeric.\"\"\"\n",
    "    df['prices'] = df['prices'].str.replace(\",\",\"\") # remove commas from prices data (e.g.: '2500' vs '$2,500')\n",
    "    # clean sqft data-- namely: remove all non-numeric data\n",
    "    df['sqft'] = df['sqft'].astype(str).str.replace(r'\\D+', '', regex=True) # remove all non-numeric data from 'sqft' col by using regex to replace any non-numeric data from col to null ('NaN') values via the str.replace() Pandas method\n",
    "    df['sqft'] = df['sqft'].replace(r'^\\s*$', np.nan, regex=True)  # replace all empty str sqft values with  null ('NaN') values \n",
    "    df = df.dropna(subset=['sqft']) # remove rows with null sqft data, so we can readily convert this col to int, without any non-numeric or missing data\n",
    "    # finally, convert all cols from list to 'int64' integer data type:\n",
    "    df[list_of_cols_to_num] = df[list_of_cols_to_num].astype('int64') # use int64 due to a) the long id values & b.) the occasional null values contained within the sqft col\n",
    "    print(f\"Sanity check: The data types of {list_of_cols_to_num} are now: \\n{df[list_of_cols_to_num].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "\n",
    "# specify a list of cols to convert to integer\n",
    "cols_to_int = df[['bedrooms', 'prices', 'ids', 'sqft']]\n",
    "cols_to_int_lis = list(cols_to_int.columns)  # convert relevant cols to list of col names\n",
    "\n",
    "cols_to_int = [] # free space\n",
    "\n",
    "df = transform_cols_to_int(df, cols_to_int_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be no remaining duplicate listing ids (ie, 0 rows): \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_df(df):\n",
    "    \"\"\"Remove duplicate rows based on listing ids\"\"\"\n",
    "    return df.drop_duplicates(keep='first', subset = ['ids'])\n",
    "\n",
    "df = deduplicate_df(df)\n",
    "\n",
    "# sanity check -- \n",
    "clist_duplicate_ids_check = df[df.duplicated(\"ids\", keep= False)]\n",
    "print(f\"There should be no remaining duplicate listing ids (ie, 0 rows): \\n{clist_duplicate_ids_check.shape[0]}\")  # check that number of duplicate rows is false (ie, wrt duplicate listing ids)\n",
    "\n",
    "# free memory\n",
    "clist_duplicate_ids_check = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check--The remaining columns in the dataset are:\n",
      " Index(['ids', 'sqft', 'cities', 'prices', 'bedrooms', 'bathrooms', 'attr_vars',\n",
      "       'date_of_webcrawler', 'kitchen', 'date_posted', 'region', 'sub_region',\n",
      "       'cats_OK', 'dogs_OK', 'wheelchair_accessible', 'laundry_in_bldg',\n",
      "       'no_laundry', 'washer_and_dryer', 'washer_and_dryer_hookup',\n",
      "       'laundry_on_site', 'full_kitchen', 'dishwasher', 'refrigerator', 'oven',\n",
      "       'flooring_carpet', 'flooring_wood', 'flooring_tile',\n",
      "       'flooring_hardwood', 'flooring_other', 'apt', 'in_law_apt', 'condo',\n",
      "       'townhouse', 'cottage_or_cabin', 'single_fam', 'duplex', 'flat', 'land',\n",
      "       'is_furnished', 'attached_garage', 'detached_garage', 'carport',\n",
      "       'off_street_parking', 'no_parking', 'EV_charging', 'air_condition',\n",
      "       'no_smoking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def remove_col_with_given_starting_name(df, col_starting_name):\n",
    "    \"\"\"Remove each column from df that has a given starting name substring.\"\"\"\n",
    "    return df.loc[:, ~df.columns.str.startswith(col_starting_name)] \n",
    "\n",
    "# remove 'Unnamed' columns, which might be imported errouneously via pd.read_csv()\n",
    "df = remove_col_with_given_starting_name(df, 'Unnamed')\n",
    "\n",
    "# remove listing_urls column since we do not want to store these data into the SQL Server table-- why?: a.) because listing urls are not relevent to rental prices and b.) the listing urls quickly become invalid or dead links, so we have no need to refer back to them at this stage in the webscraping project.\n",
    "df = remove_col_with_given_starting_name(df, 'listing_urls')\n",
    "\n",
    "# remove listing_descrip col since we do not want to store these data in SQL table either;\n",
    "df = remove_col_with_given_starting_name(df, 'listing_descrip')\n",
    "\n",
    "# sanity check\n",
    "print(f\"Sanity check--The remaining columns in the dataset are:\\n {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data cleaning Step:\n",
    "\n",
    "## Determine if there are any rental listing duplicates between the rental table and the datetime-filtered DataFrame.\n",
    "\n",
    "## If any such duplicates exist, then we need to filter these duplicates out of the datetime-filtered DataFrame.\n",
    "\n",
    "## Implement SQL Query to determine whether there any duplicates in the datetime-filtered DataFrame relative to the SQL rental table:\n",
    "\n",
    "### I.e., use a query with an IN() operator to check for a list of all listing id values from the datetime-filtered DataFrame, and compare those values with all stored rental listings data in the rental table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n",
      "Duplicate listings from the datetime-filtered dataframe relative to the SQL rental table: \n",
      "Empty DataFrame\n",
      "Columns: [listing_id, sqft, city, price, bedrooms, bathrooms, attr_vars, date_of_webcrawler, kitchen, date_posted, region, sub_region, cats_OK, dogs_OK, wheelchair_accessible, laundry_in_bldg, no_laundry, washer_and_dryer, washer_and_dryer_hookup, laundry_on_site, full_kitchen, dishwasher, refrigerator, oven, flooring_carpet, flooring_wood, flooring_tile, flooring_hardwood, flooring_other, apt, in_law_apt, condo, townhouse, cottage_or_cabin, single_fam, duplex, flat, land, is_furnished, attached_garage, detached_garage, carport, off_street_parking, no_parking, EV_charging, air_condition, no_smoking]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "# verify whether any of the listing id's in the filtered DataFrame dataset are already stored within the rental listing dataset\n",
    "\n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def check_for_listing_ids_via_SQL_in_operator(self, df, target_date):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        try:  # try to establish connection to SQL Server table via pyodbc connector\n",
    "            conn = pyodbc.connect(\n",
    "            f'DRIVER={self.driver};'\n",
    "            f'SERVER={self.server};'\n",
    "            f'DATABASE={self.database};'\n",
    "            f'UID={self.username};'\n",
    "            f'PWD={self.password};'\n",
    "            'Trusted_Connection=yes;'\n",
    "            )\n",
    "        \n",
    "        except pyodbc.Error as err:  # account for possible pyodbc SQL Server connection error\n",
    "            print(\"Python was not able to connect to SQL server database and. Please try again.\") \n",
    "\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        ## Perform SQL query on rental table to determine whether there are any rental listing duplicates--ie, listing ids from the filtered DataFrame that have already been inserted into the rental table--via SQL IN operator:\n",
    "        \n",
    "        # NB: *only* perform SQL query if at least some data has been inserted into SQL table--we can check whether any data exists based on whether the target_date has a value other than \"None\":\n",
    "        if target_date != \"None\":   # account for scenario in which *no data* has yet been inserted into the SQL table, by ensuring the target_date (ie, latest_date_str) is not equal to \"None\"\n",
    "\n",
    "            ## get a list of all listing ids from the datetime-filtered dataframe:\n",
    "            df = df.astype(str)  # convert dataframe to string, so we can enable pandas' df data to be more compatible with pyodbc library \n",
    "            df_filtered_ids = df['ids'].to_list()  # obtain a list of all ids from the filtered df\n",
    "\n",
    "            # get various '?' SQL placeholders (ie, to prevent SQL injections) and a comma for each elements of the list of ids values--NB: reference the len() of the df_filtered_ids list to get the proper number of placeholders\n",
    "            q_placeholders = \", \".join([\"?\"] * len(df_filtered_ids))   # get '?' placeholders and comma for each id element from the df_filtered_ids list\n",
    "\n",
    "            # q_placeholders = \",\".join(\"?\" * len(df_filtered_ids))  # get '?' placeholders and comma for each id element from the df_filtered_ids list\n",
    "\n",
    "\n",
    "            ## specify query, and add placeholders in between each value being checked via the IN operator\n",
    "            # sql_query = \"\"\"SELECT * FROM rental WHERE listing_id IN (\"\"\"+ q_placeholders + \") \"\n",
    "\n",
    "            sql_query = \"SELECT * FROM rental WHERE listing_id IN (%s)\" % q_placeholders\n",
    "\n",
    "            ## use Pandas' read_sql() method to parse the query, and use the id values of the df--ie, from the df_filtered_ids list--as the argument of the IN operator: \n",
    "            # id_query_duplicates = cursor.execute(sql_query, df_filtered_ids).fetchall()   # execute SQL query\n",
    "            \n",
    "            id_query_duplicates = pd.read_sql(sql_query, conn, params=df_filtered_ids)  # query for any listings that have duplicate id's from the rental table relative to the \n",
    "            \n",
    "            # sanity check-- Are there any duplicate listings in the SQL table vs the datetime-filtered DataFrame?\n",
    "            print(f\"Duplicate listings from the datetime-filtered dataframe relative to the SQL rental table: \\n{id_query_duplicates}\")        \n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "            return id_query_duplicates\n",
    "        \n",
    "        else:  # ie: if *no data* has yet been inserted into SQL table\n",
    "            pass\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# perform query to check for any listings that are duplicate between the data in the rental table and the datetime-filtered dataframe \n",
    "id_query_duplicates = SQL_db.check_for_listing_ids_via_SQL_in_operator(df, latest_date_str)   # only apply query if latest_date_str is *not* \"None\", to ensure at least some data has been inserted into SQL table before trying to perform the query on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 162 entries, 2870 to 15506\n",
      "Data columns (total 47 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   ids                      162 non-null    int64         \n",
      " 1   sqft                     162 non-null    int64         \n",
      " 2   cities                   162 non-null    object        \n",
      " 3   prices                   162 non-null    int64         \n",
      " 4   bedrooms                 162 non-null    int64         \n",
      " 5   bathrooms                162 non-null    float32       \n",
      " 6   attr_vars                162 non-null    object        \n",
      " 7   date_of_webcrawler       162 non-null    datetime64[ns]\n",
      " 8   kitchen                  162 non-null    uint8         \n",
      " 9   date_posted              162 non-null    datetime64[ns]\n",
      " 10  region                   162 non-null    object        \n",
      " 11  sub_region               162 non-null    object        \n",
      " 12  cats_OK                  162 non-null    uint8         \n",
      " 13  dogs_OK                  162 non-null    uint8         \n",
      " 14  wheelchair_accessible    162 non-null    uint8         \n",
      " 15  laundry_in_bldg          162 non-null    uint8         \n",
      " 16  no_laundry               162 non-null    uint8         \n",
      " 17  washer_and_dryer         162 non-null    uint8         \n",
      " 18  washer_and_dryer_hookup  162 non-null    uint8         \n",
      " 19  laundry_on_site          162 non-null    uint8         \n",
      " 20  full_kitchen             162 non-null    uint8         \n",
      " 21  dishwasher               162 non-null    uint8         \n",
      " 22  refrigerator             162 non-null    uint8         \n",
      " 23  oven                     162 non-null    uint8         \n",
      " 24  flooring_carpet          162 non-null    uint8         \n",
      " 25  flooring_wood            162 non-null    uint8         \n",
      " 26  flooring_tile            162 non-null    uint8         \n",
      " 27  flooring_hardwood        162 non-null    uint8         \n",
      " 28  flooring_other           162 non-null    uint8         \n",
      " 29  apt                      162 non-null    uint8         \n",
      " 30  in_law_apt               162 non-null    uint8         \n",
      " 31  condo                    162 non-null    uint8         \n",
      " 32  townhouse                162 non-null    uint8         \n",
      " 33  cottage_or_cabin         162 non-null    uint8         \n",
      " 34  single_fam               162 non-null    uint8         \n",
      " 35  duplex                   162 non-null    uint8         \n",
      " 36  flat                     162 non-null    uint8         \n",
      " 37  land                     162 non-null    uint8         \n",
      " 38  is_furnished             162 non-null    uint8         \n",
      " 39  attached_garage          162 non-null    uint8         \n",
      " 40  detached_garage          162 non-null    uint8         \n",
      " 41  carport                  162 non-null    uint8         \n",
      " 42  off_street_parking       162 non-null    uint8         \n",
      " 43  no_parking               162 non-null    uint8         \n",
      " 44  EV_charging              162 non-null    uint8         \n",
      " 45  air_condition            162 non-null    uint8         \n",
      " 46  no_smoking               162 non-null    uint8         \n",
      "dtypes: datetime64[ns](2), float32(1), int64(4), object(4), uint8(36)\n",
      "memory usage: 17.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# filter dataframe to remove any duplicate ids\n",
    "def remove_duplicate_ids_relative_to_SQL_table(df, id_query_duplicates):\n",
    "    \"\"\"Remove any records from df whose ids are already inserted into (ie, duplicates of) the rental SQL table, as given by the id_query_duplicates query results\"\"\"\n",
    "    if id_query_duplicates is not None:\n",
    "        id_query_duplicates_list = id_query_duplicates['listing_id'].astype(float).apply(int).to_list() # derive a list of duplicate ids from the query results' listing_id column\n",
    "        filtered_df = df[~df['ids'].isin(id_query_duplicates_list)]  # remove (ie, filter out) all duplicate ids using the negation of isin() \n",
    "        return filtered_df\n",
    "    else:   # do *not* apply filter if no data has yet been inserted into SQL table (ie, if id_query_duplicates is None)\n",
    "        return df\n",
    "    \n",
    "\n",
    "# apply the filter to remove duplicate ids, based on the query results (ie, id_query_duplicates)\n",
    "df = remove_duplicate_ids_relative_to_SQL_table(df, id_query_duplicates)\n",
    "\n",
    "df.info() # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step-- Data pipeline and data ingestion: \n",
    "\n",
    "## Insert the cleaned/filtered/deduplicated pandas' DataFrame into SQL rental table:\n",
    "\n",
    "#### NB: When using pyodbc to insert data from a dataframe into SQL Server table, we *may* need to transform all of the data from the dataframe to string, if SQL Server returns an error during an insertion attempt. This is because pyodbc will typically transform string values of numeric to SQL numeric data types such as int or float. \n",
    "\n",
    "#### Regardless, when using pyodbc to insert data into a SQL Server table, *always* use the  execute_many = True option so that the INSERT INTO statements will not be called upon as frequently. Ie, execute_many helps ensure that there will not be separate inserts for each row, as this is entirely unneccessary and extremely inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, insert all scraped data into the rental SQL Server table:\n",
    "\n",
    "\n",
    "\n",
    "### Insert scraped data from Pandas' dataframe to SQL Server rental table via pyodbc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def insert_df_to_SQL_ETL(self, df):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        try:  # try to establish connection to SQL Server table via pyodbc connector\n",
    "            conn = pyodbc.connect(\n",
    "            f'DRIVER={self.driver};'\n",
    "            f'SERVER={self.server};'\n",
    "            f'DATABASE={self.database};'\n",
    "            f'UID={self.username};'\n",
    "            f'PWD={self.password};'\n",
    "            'Trusted_Connection=yes;'\n",
    "            )\n",
    "        \n",
    "        except pyodbc.Error as err:  # account for possible pyodbc SQL Server connection error\n",
    "            print(\"Python was not able to connect to SQL server database and. Please try again.\") \n",
    "\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        cursor.fast_executemany = True  # speed up data ingesting by reducing the numbers of calls to server for inserts\n",
    "\n",
    "        # convert all variables from dataframe to str to avoid following SQL Server pyodbc error: 'ProgrammingError: ('Invalid parameter type.  param-index=2 param-type=function', 'HY105')'\n",
    "        # df = df.astype(str) # convert all df variables to str for ease of loading data into SQl Server table\n",
    "        \n",
    "        # insert scraped data from df to SQL table-- iterate over each row of each df col via .itertuples() method\n",
    "        \n",
    "        # Get the number of needed '?' placeholders by looking up the # of cols (ie, len()) of the dataframe), and use .join() to have each question mark separated by commas (NB: these placeholders are used to mitigate--and ideally avoid altogether--SQL injections)\n",
    "        q_mark_str = ','.join('?'*len(df.columns))  \n",
    "\n",
    "        # specify INSERT INTO SQL statement--iterate over each row in df, and insert into SQL database:\n",
    "        for row in df.itertuples():  # iterate over each row from df\n",
    "            cursor.execute(f\"\"\"INSERT INTO rental (listing_id, sqft, city, price, bedrooms, bathrooms, attr_vars,\n",
    "            date_of_webcrawler, kitchen, date_posted, region, sub_region, cats_OK, dogs_OK, wheelchair_accessible,laundry_in_bldg, no_laundry, \n",
    "            washer_and_dryer, washer_and_dryer_hookup, laundry_on_site, full_kitchen, dishwasher, refrigerator,\n",
    "            oven,flooring_carpet, flooring_wood, flooring_tile, flooring_hardwood, flooring_other,apt, in_law_apt, condo, townhouse, cottage_or_cabin, single_fam, duplex, flat, land, is_furnished, attached_garage,\n",
    "            detached_garage, carport, off_street_parking, no_parking, EV_charging, air_condition, no_smoking) \n",
    "            VALUES ({q_mark_str})\"\"\",\n",
    "            (row.ids,\n",
    "            row.sqft,\n",
    "            row.cities,\n",
    "            row.prices, \n",
    "            row.bedrooms,\n",
    "            row.bathrooms,\n",
    "            row.attr_vars, \n",
    "            row.date_of_webcrawler,\n",
    "            row.kitchen,\n",
    "            row.date_posted,\n",
    "            row.region,\n",
    "            row.sub_region,\n",
    "            row.cats_OK,\n",
    "            row.dogs_OK,\n",
    "            row.wheelchair_accessible,\n",
    "            row.laundry_in_bldg, \n",
    "            row.no_laundry,\n",
    "            row.washer_and_dryer,\n",
    "            row.washer_and_dryer_hookup,\n",
    "            row.laundry_on_site,\n",
    "            row.full_kitchen,\n",
    "            row.dishwasher,\n",
    "            row.refrigerator,\n",
    "            row.oven,\n",
    "            row.flooring_carpet,\n",
    "            row.flooring_wood,\n",
    "            row.flooring_tile,\n",
    "            row.flooring_hardwood,\n",
    "            row.flooring_other,\n",
    "            row.apt, \n",
    "            row.in_law_apt,\n",
    "            row.condo,\n",
    "            row.townhouse,\n",
    "            row.cottage_or_cabin,\n",
    "            row.single_fam, \n",
    "            row.duplex,\n",
    "            row.flat,\n",
    "            row.land,\n",
    "            row.is_furnished,\n",
    "            row.attached_garage,\n",
    "            row.detached_garage,\n",
    "            row.carport,\n",
    "            row.off_street_parking,\n",
    "            row.no_parking,\n",
    "            row.EV_charging,\n",
    "            row.air_condition,\n",
    "            row.no_smoking)\n",
    "            )\n",
    "            \n",
    "        # save and commit changes to database\n",
    "        conn.commit()\n",
    "\n",
    "        # # sanity check-- ensure some data has been inserted into new SQL table\n",
    "        sql_table_count_records = conn.execute(\"\"\"SELECT COUNT(*) FROM rental;\"\"\").fetchall()\n",
    "        print(f\"The number of records stored in the SQL table is: {sql_table_count_records[0]}\")     \n",
    "        \n",
    "        sql_query_for_record_samples = conn.execute(\"\"\"SELECT TOP 3 * FROM rental;\"\"\").fetchall() # check out several of the records\n",
    "        print(f\"\\nA few of the inserted records are: {sql_query_for_record_samples}\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# Ingest data from pandas' dataframe to SQL server--data pipeline: \n",
    "SQL_db.insert_df_to_SQL_ETL(df)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "868f069ec03d251bf5304722b36a079032b062039e590e3d2c740be1c52152d4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
