{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 in the pandas df to SQL Server data pipeline\n",
    "\n",
    "### NB: In Part 2, we assume that we've already run the Part 1 notebook (or equivalent scripts), the latter of which is intended to be run only *once*. However, the scripts, functions, and methods shown here can (and should) be re-used every time we have scraped more data and need to insert the new data into the SQL rental table. \n",
    "\n",
    "##### I.e.: the rental table has already been created within the craigslist database, and we have successfully inserted at least some data into the rental table!\n",
    "\n",
    "##### If so, then we will do the following here in Part 2: check for the last date of inserted date--ie, MAX() of date_possted--and then filter the scraped data > the MAX()date in the SQL table, clean the data using the various data type transformations, deduplications, removing nulls, etc. \n",
    "\n",
    "#### As a final data filtering step, we need to double-check whether any of these rental listings have already been inserted into the rental table using a SQL query in which we can employ the IN() operator . For example: rental listings might be taken down and then re-posted, and these listings--albeit quite rare--could have the same listing ids as an older listing that we've already stored in the rental table, even though these \"new\" listings would initially seem to be unique listings based purely on their more recent date_posted datetime value!\n",
    "\n",
    "#### After the data have been filtered and cleaned, we can then insert the new data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports-- file processing & json libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# SQL ODBC for API connection between Python & SQL Server\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29136 entries, 0 to 29135\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   listing_urls             29136 non-null  object \n",
      " 1   ids                      28931 non-null  float64\n",
      " 2   sqft                     22294 non-null  object \n",
      " 3   cities                   29019 non-null  object \n",
      " 4   prices                   28927 non-null  object \n",
      " 5   bedrooms                 28615 non-null  object \n",
      " 6   bathrooms                28927 non-null  object \n",
      " 7   attr_vars                28928 non-null  object \n",
      " 8   listing_descrip          28928 non-null  object \n",
      " 9   date_of_webcrawler       29021 non-null  object \n",
      " 10  kitchen                  29019 non-null  float64\n",
      " 11  date_posted              28928 non-null  object \n",
      " 12  region                   29136 non-null  object \n",
      " 13  sub_region               29136 non-null  object \n",
      " 14  cats_OK                  29136 non-null  int64  \n",
      " 15  dogs_OK                  29136 non-null  int64  \n",
      " 16  wheelchair_accessible    29136 non-null  int64  \n",
      " 17  laundry_in_bldg          29136 non-null  int64  \n",
      " 18  no_laundry               29136 non-null  int64  \n",
      " 19  washer_and_dryer         29136 non-null  int64  \n",
      " 20  washer_and_dryer_hookup  29136 non-null  int64  \n",
      " 21  laundry_on_site          29136 non-null  int64  \n",
      " 22  full_kitchen             29136 non-null  int64  \n",
      " 23  dishwasher               29136 non-null  int64  \n",
      " 24  refrigerator             29136 non-null  int64  \n",
      " 25  oven                     29136 non-null  int64  \n",
      " 26  flooring_carpet          29136 non-null  int64  \n",
      " 27  flooring_wood            29136 non-null  int64  \n",
      " 28  flooring_tile            29136 non-null  int64  \n",
      " 29  flooring_hardwood        29136 non-null  int64  \n",
      " 30  flooring_other           29136 non-null  int64  \n",
      " 31  apt                      29136 non-null  int64  \n",
      " 32  in_law_apt               29136 non-null  int64  \n",
      " 33  condo                    29136 non-null  int64  \n",
      " 34  townhouse                29136 non-null  int64  \n",
      " 35  cottage_or_cabin         29136 non-null  int64  \n",
      " 36  single_fam               29136 non-null  int64  \n",
      " 37  duplex                   29136 non-null  int64  \n",
      " 38  flat                     23727 non-null  float64\n",
      " 39  land                     23727 non-null  float64\n",
      " 40  is_furnished             29136 non-null  int64  \n",
      " 41  attached_garage          29136 non-null  int64  \n",
      " 42  detached_garage          29136 non-null  int64  \n",
      " 43  carport                  29136 non-null  int64  \n",
      " 44  off_street_parking       29136 non-null  int64  \n",
      " 45  no_parking               29136 non-null  int64  \n",
      " 46  EV_charging              29136 non-null  int64  \n",
      " 47  air_condition            29136 non-null  int64  \n",
      " 48  no_smoking               29136 non-null  int64  \n",
      "dtypes: float64(4), int64(33), object(12)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files to a single Pandas' df using pd.concat().\"\"\"\n",
    "    path =  parent_direc # specify parent path of directories containing the scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes??\n",
    "    df_concat = pd.concat((pd.read_csv(file, # import each CSV file from directory\n",
    "                                        sep=',',encoding = 'utf-8'  \n",
    "                                        ) for file in glob.iglob(\n",
    "                                            os.path.join(path, '**', fn_regex), \n",
    "                                            recursive=True)), ignore_index=True)  # os.path.join helps ensure this concatenation is OS independent\n",
    "    return df_concat\n",
    "\n",
    "## Import Dataset\n",
    "# import all scraped SF bay area rental listings data\n",
    "scraped_data_path = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\scraped_data\\\\sfbay\"\n",
    "\n",
    "df = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine last date (ie, MAX()) of the data stored in the rental table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n",
      "Latest date of scraped data inserted into the SQL table:\n",
      "  latest_date\n",
      "0        None\n"
     ]
    }
   ],
   "source": [
    "# Perform SQL query on the date_posted col to determine the most recent date of data stored in the table  \n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def determine_latest_date(self, sql_query):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rentals' table\"\"\"\n",
    "\n",
    "        conn = pyodbc.connect(\n",
    "        f'DRIVER={self.driver};'\n",
    "        f'SERVER={self.server};'\n",
    "        f'DATABASE={self.database};'\n",
    "        f'UID={self.username};'\n",
    "        f'PWD={self.password};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        # specify SQL query\n",
    "        sql_query = sql_query \n",
    "\n",
    "        # perform query, and convert query results to Pandas' df\n",
    "        max_date = pd.read_sql(sql_query, conn)\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        ## sanity check:\n",
    "        print(f\"Latest date of scraped data inserted into the SQL table:\\n{max_date}\")\n",
    "\n",
    "        return max_date\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "\n",
    "# specify query to select the latest date based on date_posted:\n",
    "query = \"SELECT MAX(date_posted) AS latest_date FROM rental;\"\n",
    "\n",
    "latest_date = SQL_db.determine_latest_date(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we filter the scraped listings dataset (ie, dataframe), we need to transform the 'date_posted' column--ie, the one that we will filter--to a standardized datetime format\n",
    "\n",
    "#### Why?: To ensure consistency and replicability for this data pipeline (ie, of the pandas' DataFrame to SQL Server table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>date_of_webcrawler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-27 17:45:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-03 00:49:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-03 00:20:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-10 13:16:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-19 02:14:00</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_posted date_of_webcrawler\n",
       "0 2021-12-27 17:45:00         2022-01-03\n",
       "1 2022-01-03 00:49:00         2022-01-03\n",
       "2 2022-01-03 00:20:00         2022-01-03\n",
       "3 2021-12-10 13:16:00         2022-01-03\n",
       "4 2021-12-19 02:14:00         2022-01-03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_cols_to_datetime(df, col_to_convert):\n",
    "    \"\"\"Transform relevant column(s) to datetime using pd.to_datetime() method, and use infer_datetime_format=True to enable allow for datetime conversion using differing formats (ie, date_posted has a somewhat more precise format). \"\"\"\n",
    "    return pd.to_datetime(df[col_to_convert], infer_datetime_format=True)\n",
    "\n",
    "# apply transformations to datetime for the 2 relevant cols:\n",
    "df['date_of_webcrawler'] =  transform_cols_to_datetime(df,'date_of_webcrawler')\n",
    "df['date_posted'] = transform_cols_to_datetime(df,'date_posted')\n",
    "\n",
    "#sanity check\n",
    "df[['date_posted', 'date_of_webcrawler']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we need to filter the dataset to listings records whose date_posted dates are newer (read: greater than) the MAX() of the last date found from the query of the SQL rental table.\n",
    "\n",
    "### To this end, we can convert the MAX() value from the SQL query to a str value, and then use this as the argument for the filter_df_since_specified_date() function, which filters on the 'date_posted' column: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest date among the scraped data stored in the SQL table is:\n",
      "None,\n",
      "data type of this variable is (NB\"\" should be str): <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "## next, convert this latest_date to a string value, so we can use this to filter the scraped dataframe dataset\n",
    "def datetime_col_to_str_of_datetime(df, datetime_col):\n",
    "    \"\"\"Given datetime col from pandas' DataFrame,\n",
    "    transform to a string of the datetime value.\"\"\"\n",
    "    return df[datetime_col].head(1).astype(str).reset_index().loc[0, datetime_col] \n",
    "\n",
    "# specify name of query result DataFrame and the corresponding datetime col:\n",
    "latest_date, dt_col = latest_date, 'latest_date' \n",
    "#apply function using the 2 arguments shown above\n",
    "latest_date_str = datetime_col_to_str_of_datetime(latest_date, dt_col)\n",
    "# sanity check\n",
    "print(f'The latest date among the scraped data stored in the SQL table is:\\n{latest_date_str},\\ndata type of this variable is (NB\"\" should be str): {type(latest_date_str)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, filter on the date_posted date, so that we only have the new listings data that we have not yet inserted into the SQL table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newest scraped data not stored in the SQL table is--\n",
      "*NB: this should be an empty df if we have already stored all of the df's data into the SQL table*: \n",
      "\n",
      "0       2021-12-27 17:45:00\n",
      "1       2022-01-03 00:49:00\n",
      "2       2022-01-03 00:20:00\n",
      "3       2021-12-10 13:16:00\n",
      "4       2021-12-19 02:14:00\n",
      "                ...        \n",
      "29131   2022-02-03 11:08:00\n",
      "29132   2022-02-10 09:52:00\n",
      "29133   2022-02-10 08:51:00\n",
      "29134   2022-02-10 06:03:00\n",
      "29135   2022-02-10 02:41:00\n",
      "Name: date_posted, Length: 29136, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# filter the dataframe > MAX() of latest_date stored in SQL rental table\n",
    "def filter_df_since_specified_date(df, target_date: str):\n",
    "    \"\"\"Filter the imported scraped dataset to all data newer than the specified date (as determined via the MAX(posted_date) query).\"\"\"\n",
    "    if target_date != \"None\":   # account for scenario in which *no data* has yet been inserted into the SQL table\n",
    "        df = df.loc[df['date_posted'] > target_date]  # filter to data greater than specified date\n",
    "    else: \n",
    "        pass  # do not apply filter, sin`ce no data has yet been inserted into SQL database\n",
    "    return df\n",
    "\n",
    "# get all data since the latest stored data from SQL table (via the query on MAX(posted_date)) \n",
    "df = filter_df_since_specified_date(df, latest_date_str)\n",
    "\n",
    "# sanity check\n",
    "print(f\"The newest scraped data not stored in the SQL table is--\\n*NB: this should be an empty df if we have already stored all of the df's data into the SQL table*: \\n\\n{df['date_posted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, perform all additional data cleaning and wrangling features to prep the data for the SQL inserts, as in the Part 1 jupyter notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement several small-scale webcrawlers to obtain a list of all SF Bay Area cities, including unincorporated areas. Do the same for Santa Cruz county cities as well.\n",
    "\n",
    "### Then, combine these into a single list, and we will use these data to clean the cities column, and ensure that each column with valid data (ie, has a listing IDs) has a correct city name by extracting matching city names from the given listing's URL relative to that of the list of city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webcrawler to extract SF Bay Area city names\n",
    "\n",
    "#web crawling, web scraping & webdriver libraries and modules\n",
    "from selenium import webdriver  # NB: this is the main module we will use to implement the webcrawler and webscraping. A webdriver is an automated browser.\n",
    "from webdriver_manager.chrome import ChromeDriverManager # import webdriver_manager package to automatically take care of any needed updates to Chrome webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options  # Options enables us to tell Selenium to open WebDriver browsers using maximized mode, and we can also disable any extensions or infobars\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# sf bay area city names wiki page:\n",
    "sfbay_cities_wiki_url = 'https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_the_San_Francisco_Bay_Area'\n",
    "\n",
    "\n",
    "# access page, and grab city names, append to list\n",
    "\n",
    "def obtain_cities_from_wiki_sfbay(webpage_url,list_of_cities):\n",
    "    # initialize web driver\n",
    "            \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "    \n",
    "    # access webpage\n",
    "    driver.get(webpage_url)\n",
    "\n",
    "    xpaths_table = '//table[@class=\"wikitable plainrowheaders sortable jquery-tablesorter\"]'\n",
    "\n",
    "    # search for wiki data tables:\n",
    "    table = driver.find_element(By.XPATH, xpaths_table)\n",
    "\n",
    "\n",
    "    # iterate over each table row and then row_val within each row to get data from the given table, pertaining to the city names\n",
    "    for row in table.find_elements(By.CSS_SELECTOR, 'tr'): # iterate over each row in the table\n",
    "        \n",
    "        \n",
    "        city_names =  row.find_elements(By.TAG_NAME, 'th')  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "        # city_names =  row.find_elements(By.TAG_NAME, 'td')[0]  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "\n",
    "        # extract text, but *skip* the first 2 rows of the table  rows' values since these are only the column names!\n",
    "        for city_name in city_names[:2]: # skip first 2 rows \n",
    "\n",
    "            # append the remaining data to list\n",
    "            list_of_cities.append(city_name.text)\n",
    "\n",
    "\n",
    "    # exit webpage \n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    return list_of_cities\n",
    "\n",
    "\n",
    "\n",
    "# initialize list:\n",
    "sfbay_city_names = []\n",
    "\n",
    "\n",
    "#sfbay data\n",
    "obtain_cities_from_wiki_sfbay(sfbay_cities_wiki_url, sfbay_city_names)\n",
    "\n",
    "# remove remaining col names:\n",
    "sfbay_city_names = sfbay_city_names[4:]\n",
    "\n",
    "# sanity check\n",
    "print(f'sfbay city names:{sfbay_city_names}')\n",
    "\n",
    "print(f'There are {len(sfbay_city_names)} city names\\nNB: There should be 101.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 111.0.5563\n",
      "Get LATEST driver version for 111.0.5563\n",
      "Driver [C:\\Users\\Kevin Allen\\.wdm\\drivers\\chromedriver\\win32\\111.0.5563.64\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links to unincorporated city pages:\n",
      "['https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Alameda_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Contra_Costa_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Marin_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Napa_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_San_Mateo_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Santa_Clara_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Solano_County,_California', 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Sonoma_County,_California']\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e2973985-931a-4c04-82a5-ece8af029545\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e9f0e38e-643d-4e10-9f65-f192969949a6\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"b08815a8-2a8f-4872-b5b0-980a0c00dbff\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"caa9bace-7873-4c24-9aca-3ec7c8888448\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"4fec8382-c7b2-4a26-9cf9-d519b4be5154\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"00baaad5-fac6-4aaa-9888-5f9347747d09\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"ea613f28-8fee-4ea1-beb6-bd156df622df\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"2c6008b1-89d5-44f8-9d9e-df461b3665d9\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"da44c39c-4338-4025-a39d-f8acd80bb77d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"6e9dc719-52ed-4922-a78b-88719b6f1b90\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"ac481062-4341-4bb7-a98d-d95f16cf9b4e\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"a185b4fe-e2e3-4455-bdc6-be7c2374b63a\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"d5d67174-590c-4797-9147-d2f849e9309c\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"3b03678b-1907-4e0a-b22b-b24eb2fc17a4\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"42b70a8b-7b5b-41e1-9457-1408f9233265\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"1db2a66b-8a42-483b-9685-2ac58d686159\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"0cfe969c-998d-4f24-ac73-782b287295e4\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"14663271-70cb-4ad0-8e7e-9885d887342f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"c2627b65-7bb7-486c-ad4b-456ed26f67cc\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"c3b6892b-ed42-4a7e-99af-34d03c3d1690\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"d7766ebe-7f3b-42e4-a6b4-c8f9953fa05d\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"a92a0f93-f758-4b9c-95e9-02f9a50f33d4\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"21ee9cf1-1472-4b70-9e54-b243bf6d5736\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"aee189c1-4a4f-4b9c-b4aa-4e5f5f435393\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"41377f61-2182-4fc4-a64f-60b856550838\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"ab59479f-32e8-4134-9983-8de56b798acf\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"abad77ec-9d2e-49b5-aefa-2ffc18ad4a3f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"376d20ad-683d-4fb1-8507-6994010086b3\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e067c448-655b-4b0f-b868-fa6b49bd5635\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"62d1f131-09ca-4ff6-87d0-6b47bb51f696\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"f01a0bdb-9da5-4003-866f-99a181253119\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"a5ce0471-78ce-4345-bd2c-792961dc61bb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"b385bff6-563b-49b7-ab21-4733c4a27106\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"b7163178-5f65-47d1-b746-5ce0fa580dac\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e66ca399-81f8-4486-aa18-1b42ea367b5c\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"0bfc9f28-7b71-4b5b-bf0c-97438325e722\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"f2454784-b23e-47c3-ba0d-b27bb3d149db\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"5e6a1fa4-0159-462f-98d8-48094339c480\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"4065894e-e7d0-49d9-9236-525df673a62a\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"15802539-dd0d-4d17-ae57-64a1f91d7676\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"c31da763-6258-4856-9ec6-1a2baf903d0e\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"a79a81e1-71d9-4ea6-8e49-2269092af4a5\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"352c9a4e-61c1-4cd1-b143-14570a0d7d1d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"2c465ff6-bac5-4ffe-8646-c6ca6c446adf\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"32e161af-61a0-4f52-a524-8c91173e4926\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"476eb58b-b7ba-4533-92b8-1db3b51fd3c7\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"3b7fdae8-798e-432b-a8ae-e2b0c9c3d11e\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"6612fb1f-5f98-4833-ade3-b3dbe1aaf8b0\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"2cd600ef-3bd6-460a-9975-ce6687b6d6c6\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"74502ee4-8b80-4094-ae37-bb91b753623c\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"5ca1d629-7f2b-4591-95d3-464af49ad1c1\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"a15d1a77-63a3-4ee6-8ae2-cc5c44ef9e27\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e2a3124a-7e0c-411a-b96f-8a20c34e1b64\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"4986a3d6-33d2-41a8-87a4-bddeb5dc99e6\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"3a2e9834-ffc5-404b-9489-be55e55df41d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"d34ec259-136d-4c2b-be4d-65857a10f828\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"289072ab-8857-4915-a10e-5434d0725170\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"05481a32-6f87-44a5-a15c-7cfac8120d95\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"d75cd91c-1002-4406-92ba-2c196166fe26\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"246705c4-a86e-4d36-b719-960c72646bc8\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"d88cf011-4b99-4e2a-af60-ef1c40709ffa\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"37555bbe-d419-45fc-9279-98defcae98ec\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"19ca6fe1-4f7d-4751-8ad6-372ef1afe32c\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"1c641c15-869f-4586-9a13-7198480f52fe\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e9d3b124-b0be-4ade-b085-5eed941e6135\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"3390a70d-3b9f-4c2e-b481-8b62021eaabb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"78a7ddc4-a8b3-479e-8078-96cb564cc60b\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"c8951dfd-20d8-4281-85b9-6ed566879b88\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"d1cd5fe2-d754-4792-aafb-2b62ee82106a\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"f32b0d6f-82f6-47ad-9cfa-e84b4b8004eb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"b0602d68-89db-4d23-93a1-916703d00ba3\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"737a62d5-4aa5-4689-b7d7-278ee017cdbc\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"40407a43-cadd-47f5-816a-e93deb240086\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"41b1223c-58f3-460b-8985-555d1319ad5d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"22942a26-59ae-4656-af34-130b956c7629\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"3620cbc0-2d19-4121-a01c-f580c3aa98d9\")>]\n",
      "\n",
      "\n",
      "Unincorporated sfbay city names:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"33d3a71a-633f-464b-92d6-f626a76053a6\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"bd0f02c5-1a89-4415-896c-87dc1b5b1274\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"46ae70d2-c8b5-48fe-9be8-7549b4848463\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"7d106014-24cd-466b-bfcf-c389337a27f5\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"76dc67c4-e5e5-46e8-9c15-82b017f25188\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"bf206df8-465d-470e-b670-45bea8dec1ec\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"b7dc4537-fa60-4674-8014-dc4a1869faba\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"e060ea0c-1df4-4d62-9e50-183279480998\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"aef532f7-245a-4fa9-a748-d1cad68feb73\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"81148615-5d0d-40dd-a9fb-9593aed33079\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"f8efecb1-fe08-4984-ba00-a9eb43ba26be\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"16452b0d-5064-4268-a638-b9f25fef1871\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"625eba22-b30b-49cb-b8f3-e98b6d1efc8f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"0439d298-4ac3-4bc2-922f-b6e548bbb669\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"7f2c15394506c2b45370e77a89b0172f\", element=\"3dbe51d6-11a2-4022-b032-335f56108ad6\")>]\n",
      "\n",
      "\n",
      "unincorporated city names:/n[]\n"
     ]
    }
   ],
   "source": [
    "# webcrawler to extract SF Bay Area unincorporated areas\n",
    "\n",
    "#web crawling, web scraping & webdriver libraries and modules\n",
    "from selenium import webdriver  # NB: this is the main module we will use to implement the webcrawler and webscraping. A webdriver is an automated browser.\n",
    "from webdriver_manager.chrome import ChromeDriverManager # import webdriver_manager package to automatically take care of any needed updates to Chrome webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options  # Options enables us to tell Selenium to open WebDriver browsers using maximized mode, and we can also disable any extensions or infobars\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# url for \"Unincorporated communities in the San Francisco Bay Area\", including each SF Bay county:\n",
    "unincorporated_sfbay_all_counties_homepage = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_the_San_Francisco_Bay_Area' \n",
    "\n",
    "# # url for san mateo county\n",
    "# unincorporated_san_mateo = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_San_Mateo_County,_California'\n",
    "\n",
    "# # Alameda County\n",
    "# unincorporated_alameda = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Alameda_County,_California'\n",
    "\n",
    "# # Sonoma County\n",
    "\n",
    "# # Santa Clara\n",
    "\n",
    "# access each county page from unincorporated_sfbay_all_counties_homepage\n",
    "\n",
    "\n",
    "# initialize webdriver and install latest Chrome webdriver \n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "\n",
    "# go to wiki homepage of SF Bay Area unincorporated area pages\n",
    "driver.get(unincorporated_sfbay_all_counties_homepage)\n",
    "\n",
    "# find each href from homepage\n",
    "# unincorporated_sfbay_hrefs = [el.get_attribute(\"href\") for el in driver.find_elements(\"xpath\", '//*[@id=\"mw-subcategories\"]/div/div/div')]\n",
    "\n",
    "# initialize lists\n",
    "unincorporated_sfbay_hrefs = []\n",
    "\n",
    "#  to contain city names data\n",
    "unincorporated_sfbay_city_names = []\n",
    "\n",
    "\n",
    "# find the 'a' tags from the divs with class name of \"CategoryTreeItem\", which contain the desired hrefs\n",
    "unincorporated_sfbay_a_elements = driver.find_elements(By.XPATH, '//div[@class=\"CategoryTreeItem\"]/a')\n",
    "\n",
    "\n",
    "# next, get hrefs from the a elements\n",
    "unincorporated_sfbay_hrefs = [el.get_attribute('href') \n",
    "                              for el in unincorporated_sfbay_a_elements]\n",
    " \n",
    "print(f'Links to unincorporated city pages:\\n{unincorporated_sfbay_hrefs}')\n",
    "\n",
    "# access each page of county-level unincorporated areas, and grab each city name\n",
    "for href in unincorporated_sfbay_hrefs:\n",
    "    # access page\n",
    "    driver.get(href)\n",
    "    # grab city name data\n",
    "\n",
    "    unincorporated_sfbay_city_names_selenium = driver.find_elements(\"xpath\", '//div[@class=\"mw-category-group\"]')\n",
    "    # unincorporated_sfbay_city_names_selenium = driver.find_elements(\"xpath\", '//*[@id=\"mw-pages\"]/div/div/div[2]/ul/li/a')\n",
    "\n",
    "\n",
    "    # unincorporated_sfbay_city_names_selenium = driver.find_elements(\"xpath\", '//*[@id=\"mw-pages\"]/div/div/div[2]/ul/li/a')\n",
    "\n",
    "    print(f'Unincorporated sfbay city names:\\n{unincorporated_sfbay_city_names_selenium}\\n\\n')\n",
    "\n",
    "    # # get text data\n",
    "    # for el in unincorporated_sfbay_city_names:\n",
    "    #     unincorporated_sfbay_city_names.append(el.get_attribute(\"title\").text)\n",
    "\n",
    "    for el in unincorporated_sfbay_city_names:\n",
    "        unincorporated_sfbay_city_names.append(el.text)\n",
    "\n",
    "print(f'unincorporated city names:/n{unincorporated_sfbay_city_names}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # webcrawler to extract SF Bay Area unicorporated areas\n",
    "\n",
    "# #web crawling, web scraping & webdriver libraries and modules\n",
    "# from selenium import webdriver  # NB: this is the main module we will use to implement the webcrawler and webscraping. A webdriver is an automated browser.\n",
    "# from webdriver_manager.chrome import ChromeDriverManager # import webdriver_manager package to automatically take care of any needed updates to Chrome webdriver\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException, ElementClickInterceptedException\n",
    "# from selenium.webdriver.chrome.options import Options  # Options enables us to tell Selenium to open WebDriver browsers using maximized mode, and we can also disable any extensions or infobars\n",
    "\n",
    "# import requests\n",
    "\n",
    "\n",
    "# # url for \"Unincorporated communities in the San Francisco Bay Area\", including each SF Bay county:\n",
    "# unincorporated_sfbay_all_counties_homepage = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_the_San_Francisco_Bay_Area' \n",
    "\n",
    "# # # url for san mateo county\n",
    "# # unincorporated_san_mateo = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_San_Mateo_County,_California'\n",
    "\n",
    "# # # Alameda County\n",
    "# # unincorporated_alameda = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Alameda_County,_California'\n",
    "\n",
    "# # # Sonoma County\n",
    "\n",
    "# # # Santa Clara\n",
    "\n",
    "# # access each county page from unincorporated_sfbay_all_counties_homepage\n",
    "\n",
    "\n",
    "# # initialize webdriver and install latest Chrome webdriver \n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "\n",
    "# # go to wiki homepage of SF Bay Area unincorporated area pages\n",
    "# driver.get(unincorporated_sfbay_all_counties_homepage)\n",
    "\n",
    "# # find each href from homepage\n",
    "# # unincorporated_sfbay_hrefs = [el.get_attribute(\"href\") for el in driver.find_elements(\"xpath\", '//*[@id=\"mw-subcategories\"]/div/div/div')]\n",
    "\n",
    "# unincorporated_sfbay_hrefs = [el.get_attribute(\"href\") for el in \n",
    "#                               driver.find_elements(\n",
    "#     \"xpath\", \"//a[starts-with(@title, 'Unincorporated communities')]\"\n",
    "#     )]\n",
    "\n",
    "# # access each page of county-level unincorporated areas, and grab each city name\n",
    "# for href in unincorporated_sfbay_hrefs:\n",
    "#     # access page\n",
    "#     driver.get(href)\n",
    "#     # grab city name data\n",
    "#     unincorporated_sfbay_city_names = [el.get_attribute(\"title\") for el in driver.find_elements(\"xpath\", '//*[@id=\"mw-pages\"]/div')]\n",
    "#     print(f'Unincorporated sfbay city names:\\n{unincorporated_sfbay_city_names}')\n",
    "#     # # get text data\n",
    "#     # unincorporated_sfbay_city_names.text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_unincorporated_areas_from_wiki_sfbay(webpage_url,list_to_append):\n",
    "\n",
    "    # initialize webdriver and install latest Chrome webdriver \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "\n",
    "    # go to wiki homepage of SF Bay Area unincorporated area pages\n",
    "    driver.get(unincorporated_sfbay_all_counties_homepage)\n",
    "\n",
    "    # find each href from homepage\n",
    "    unincorporated_sfbay_hrefs = [el.get_attribute(\"href\") for el in driver.get_elements(\"xpath\", '//*[@id=\"mw-subcategories\"]/div/div/div')]\n",
    "\n",
    "    # access each page of county-level unincorporated areas, and grab each city name\n",
    "    for href in unincorporated_sfbay_hrefs:\n",
    "        # access page\n",
    "        driver.get(href)\n",
    "        # grab city name data\n",
    "        unincorporated_sfbay_city_names = [el.get_attribute(\"title\") for el in driver.get_elements(\"xpath\", '//*[@id=\"mw-pages\"]/div')]\n",
    "        print(f'Unincorporated sfbay city names:\\n{unincorporated_sfbay_city_names}')\n",
    "        # # get text data\n",
    "        # unincorporated_sfbay_city_names.text \n",
    "\n",
    "    return list_to_append\n",
    "\n",
    "\n",
    "\n",
    "#initialize list\n",
    "sfbay_unincoporated = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unincorporated names by removing \", California\"\n",
    "def clean_unincorporatd_names(list):\n",
    "    return [el.replace(', California', '') for el in list]\n",
    "\n",
    "clean_unincorporatd_names(sfbay_unincoporated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine each list of sfbay names\n",
    "def combine_lists(list1, list2):\n",
    "    return list1.extend(list2)\n",
    "\n",
    "combine_lists(sfbay_city_names, sfbay_unincoporated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webcrawler to extract SC county city names\n",
    "\n",
    "# sc county wiki page url\n",
    "sc_county_cities_wiki_url = 'https://en.wikipedia.org/wiki/Santa_Cruz_County,_California#Population_ranking'\n",
    "\n",
    "\n",
    "sc_county_city_names = []\n",
    "\n",
    "\n",
    "def obtain_cities_from_wiki_sc(webpage_url,list_of_cities):\n",
    "    # initialize web driver\n",
    "            \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "    \n",
    "    # access webpage\n",
    "    driver.get(webpage_url)\n",
    "\n",
    "\n",
    "    # NB!: there are 2 tables with the same class name; only select data from the 2nd one\n",
    "    xpaths_table = '//table[@class=\"wikitable sortable jquery-tablesorter\"][2]//tr//td[2]'  # 2nd table on webpage with this class name\n",
    "\n",
    "\n",
    "    # search for given wiki data tables:\n",
    "    table = driver.find_elements(By.XPATH, xpaths_table)\n",
    "\n",
    "\n",
    "    print(f'Full table:\\n\\n{table}\\n\\n\\n\\n\\n')\n",
    "\n",
    "    for row in table:\n",
    "        print(f'City names:{row.text}')\n",
    "        list_of_cities.append(row.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # exit webpage \n",
    "    driver.close()\n",
    "\n",
    "    # # sanity check\n",
    "    # print(f'List of city names:\\n{list_of_cities}')\n",
    "\n",
    "    return list_of_cities\n",
    "\n",
    "obtain_cities_from_wiki_sc(sc_county_cities_wiki_url, sc_county_city_names)\n",
    "\n",
    "\n",
    "#  # clean data by removing extraneous '†' char from city names list\n",
    "sc_county_city_names = list(map(lambda x: x.replace('†',''), sc_county_city_names))\n",
    "\n",
    "## finally, remove any whitespace from list-- use list comprehension\n",
    "sc_county_city_names = [s for s in sc_county_city_names if s.strip()]\n",
    "\n",
    "# sanity check\n",
    "print(f'\\n\\nsc county city names:{sc_county_city_names}')\n",
    "print(f'There are {len(sc_county_city_names)} city names for SC county.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webcrawler to extract SC county unicorporated areas\n",
    "\n",
    "# url\n",
    "sc_county_unincorporated_url = 'https://en.wikipedia.org/wiki/Category:Unincorporated_communities_in_Santa_Cruz_County,_California'\n",
    "\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the SF bay city names with the SC county names\n",
    "combine_lists(sfbay_city_names, sc_county_city_names)\n",
    "\n",
    "# sanity check\n",
    "print(f'New length of city names list (after combining SF Bay with SC county):\\n{len(sfbay_city_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, use the list of SF Bay Area & SC county city names to clean the cities city names column, by matching city names parsed from the listing_urls to the list of city names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_names(df, list_of_city_names: list):\n",
    "    \"\"\"Clean city names data:\n",
    "    1) Add dash delimiters (uie, '-' in between each word) to each element of the list of city names (*to match the formatting of the listing_urls column)\n",
    "    2) Apply lower-case to all elements of the list of city names, and the listing_urls column as well (since we will parse the city names from this col)\n",
    "    3) Parse the main city name from listing URL: ie, the city name should always be present in the URL.\n",
    "    --use str.split() on '/apa/d', and extract the 2nd element after performing the split \n",
    "    &\n",
    "    4) Match the wrangled listing_urls' data to the city names list:\n",
    "    use str.extract()  to compare the wrangled listing URLs to the list of all possible city names.\n",
    "    \"\"\"\n",
    "    # 1) add dash delimiter (use. strip() first to remove leading or lagging whitespace) to list of SF Bay + SC county names:\n",
    "    list_of_city_names = [el.strip().replace(' ', '-') for el in list_of_city_names]\n",
    "\n",
    "    ## 2) apply lower-case for the list of SF Bay + SC county names:\n",
    "    list_of_city_names  = [el.lower() for el in list_of_city_names]\n",
    "    # apply lower-case to listing_urls col\n",
    "    df['listing_urls'] = df['listing_urls'].str.lower()\n",
    "\n",
    "    # 3 a): use str.split() on '/apa/d' and get the 2nd element after performing the split:\n",
    "    df['listing_urls_for_str_match'] = df['listing_urls'].str.split('/apa/d/').str[1]  # obtain the 2nd resulting element\n",
    "\n",
    "    # step 4: match a substring from this newly-parsed column-- ie, 'listing_urls_for_str_match'\n",
    "    # -- to matching substrings from the  sfbay_city_names list:\n",
    "    # How?: use str.contains() and join pipe operators to each element of the list to perform an essentially  boolean \"OR\" str.contains() search for any matching city names\n",
    "\n",
    "    # pipe operator\n",
    "    pipe_operator = '|'\n",
    "\n",
    "    # specify a regex pattern for a str.extract() method--NB: we need to wrap the pattern within a sort of tuple by using parentheses in strings--ie, '( )', so like the following format: '( regex_pattern...)'\n",
    "    unique_city_names_dash_delim_pattern = '(' + pipe_operator.join(list_of_city_names)+')'  # wrap the city names regex pattern within a 'string' tuple: ie, '(...)'\n",
    "\n",
    "    # clean city column by matching city names list with parsed listing_urls_for_str_match from regex pattern (ie, derived from list of names), using str.extract() \n",
    "    df['cities'] = df['listing_urls_for_str_match'].str.extract(unique_city_names_dash_delim_pattern, expand=False)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# clean city names data:\n",
    "df = clean_city_names(df, sfbay_city_names)\n",
    "# sanity check\n",
    "print(f\"Sanity check--after cleaning the city names, let's examine some of the cleaned data: {df.cities.value_counts().tail(10)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, remove any rows of data that have missing data for any of several critical columns, such as listing IDs, city names, prices, sqft, or kitchen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining price, listing id, sqft, kitchen, & city name nulls: \n",
      "prices     0\n",
      "ids        0\n",
      "sqft       0\n",
      "kitchen    0\n",
      "cities     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def remove_nulls_list(df, list_of_cols):\n",
    "    \"\"\"Remove rows that do not have price, city name, kitchen, sqft, or listing ID data, as these are essential variables in this rental listings dataset.\"\"\"\n",
    "    return df.dropna(subset=list_of_cols)\n",
    "\n",
    "list_cols_to_remove_nulls = ['prices', 'ids', 'sqft', 'kitchen', 'cities']  \n",
    "df = remove_nulls_list(df, list_cols_to_remove_nulls)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining price, listing id, sqft, kitchen, & city name nulls: \\n{df[list_cols_to_remove_nulls].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of ['kitchen', 'cats_OK', 'dogs_OK', 'wheelchair_accessible', 'laundry_in_bldg', 'no_laundry', 'washer_and_dryer', 'washer_and_dryer_hookup', 'laundry_on_site', 'full_kitchen', 'dishwasher', 'refrigerator', 'oven', 'flooring_carpet', 'flooring_wood', 'flooring_tile', 'flooring_hardwood', 'flooring_other', 'apt', 'in_law_apt', 'condo', 'townhouse', 'cottage_or_cabin', 'single_fam', 'duplex', 'flat', 'land', 'is_furnished', 'attached_garage', 'detached_garage', 'carport', 'off_street_parking', 'no_parking', 'EV_charging', 'air_condition', 'no_smoking'] are now: \n",
      "kitchen                    uint8\n",
      "cats_OK                    uint8\n",
      "dogs_OK                    uint8\n",
      "wheelchair_accessible      uint8\n",
      "laundry_in_bldg            uint8\n",
      "no_laundry                 uint8\n",
      "washer_and_dryer           uint8\n",
      "washer_and_dryer_hookup    uint8\n",
      "laundry_on_site            uint8\n",
      "full_kitchen               uint8\n",
      "dishwasher                 uint8\n",
      "refrigerator               uint8\n",
      "oven                       uint8\n",
      "flooring_carpet            uint8\n",
      "flooring_wood              uint8\n",
      "flooring_tile              uint8\n",
      "flooring_hardwood          uint8\n",
      "flooring_other             uint8\n",
      "apt                        uint8\n",
      "in_law_apt                 uint8\n",
      "condo                      uint8\n",
      "townhouse                  uint8\n",
      "cottage_or_cabin           uint8\n",
      "single_fam                 uint8\n",
      "duplex                     uint8\n",
      "flat                       uint8\n",
      "land                       uint8\n",
      "is_furnished               uint8\n",
      "attached_garage            uint8\n",
      "detached_garage            uint8\n",
      "carport                    uint8\n",
      "off_street_parking         uint8\n",
      "no_parking                 uint8\n",
      "EV_charging                uint8\n",
      "air_condition              uint8\n",
      "no_smoking                 uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def transform_cols_to_indicators(df, list_of_cols):\n",
    "    \"\"\" Transform relevant attribute columns to numeric, and specify NaNs for any missing or non-numeric data.\"\"\"\n",
    "    df[list_of_cols] = df[list_of_cols].astype('uint8', errors='ignore') # convert any missing data to NaN \n",
    "    df[list_of_cols] = df[list_of_cols].fillna(0) # impute any NaN values with 0s since we can be quite certain that any rental listings not explicitly specifying a specific property type, amenity, or attribute do not actually contain said attribute, amenity, or are not said propery type  \n",
    "    print(f\"Sanity check: The data types of {list_of_cols} are now: \\n{df[list_of_cols].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "# specify a list of cols to convert to numeric -- # since there are many cols we want to transform to indicator variables, it's easier to simply drop the few cols that comprise str (aka, object) data \n",
    "cols_to_indicators = df.drop(columns =['ids', 'listing_urls', 'region', 'sub_region', 'cities', 'attr_vars', 'listing_descrip', 'sqft', 'prices', 'bedrooms', 'bathrooms', 'date_posted', 'date_of_webcrawler']) \n",
    "cols_to_indicators_lis = list(cols_to_indicators.columns)\n",
    "cols_to_indicators = [] # free space\n",
    "\n",
    "df = transform_cols_to_indicators(df, cols_to_indicators_lis)\n",
    "\n",
    "cols_to_indicators_lis = [] # free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of kitchen are now: \n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# also, transform kitchen var separately, since this tends to otherwise convert to float:\n",
    "df = transform_cols_to_indicators(df, 'kitchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: \n",
      "1      2688\n",
      "2      1107\n",
      "1.5     134\n",
      "2.5      80\n",
      "3        58\n",
      "3.5       8\n",
      "4         6\n",
      "6         2\n",
      "9+        1\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Allen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# re: # of bathrooms data, transform any records containing 'shared' or 'split' to 1\n",
    "# Why?: Because we can assume that any rental units comprising a 'shared' bathroom is essentially 1 bathroom\n",
    "def transform_shared_and_split_to_ones(df, col_to_transform):\n",
    "    \"\"\"Transform any records (from given col) containing the string values of 'shared' or 'split' to a value of 1.\"\"\"\n",
    "    # transform col to object, so we can use Python str methods to transform the data\n",
    "    df[col_to_transform] = df[col_to_transform].astype('object') \n",
    "    bedroom_replace_criteria = ['shared', 'split']\n",
    "    bedroom_replace_criteria = '|'.join(bedroom_replace_criteria) # join pipe symbols so we can use str.replace() on multiple 'or' conditions simultaneously \n",
    "    return df[col_to_transform].str.replace(bedroom_replace_criteria,'1')\n",
    "\n",
    "# clean bathrooms data by replacing the 'split' and 'shared' string values:\n",
    "df['bathrooms'] = transform_shared_and_split_to_ones(df, 'bathrooms')\n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check: \\n{df['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value counts for bathrooms data--having cleaned ambiguous records: \n",
      "1      2688\n",
      "2      1107\n",
      "1.5     134\n",
      "2.5      80\n",
      "3        58\n",
      "3.5       8\n",
      "4         6\n",
      "6         2\n",
      "9         1\n",
      "Name: bathrooms, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Allen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# replace any ambiguous # of bathrooms data--such as '9+' with empty strings (ie, essentially nulls) \n",
    "def replace_ambiguous_data_with_empty_str(df, col_to_transform):\n",
    "    \"\"\"Replace ambiguous rows of data (ie, any containing a plus sign) for bathrooms col with empty strings\"\"\"\n",
    "    return df[col_to_transform].str.replace(r'\\+', '')  # use str.replace() to use a regex to search for plus signs, and in effect remove these by replacing them with empty strings \n",
    "\n",
    "df['bathrooms']  = replace_ambiguous_data_with_empty_str(df, 'bathrooms')\n",
    "# sanity check\n",
    "print(f\"New value counts for bathrooms data--having cleaned ambiguous records: \\n{df['bathrooms'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining bedroom & bathroom nulls: \n",
      "bedrooms     0\n",
      "bathrooms    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# next, remove any bathroom or bedroom nulls:\n",
    "def remove_bedroom_and_br_nulls(df):\n",
    "    return df.dropna(subset=['bedrooms', 'bathrooms'])\n",
    "\n",
    "df = remove_bedroom_and_br_nulls(df)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Remaining bedroom & bathroom nulls: \\n{df[['bedrooms', 'bathrooms']].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Cleaning step-- cont'd:\n",
    "\n",
    "## If there are any rental listing duplicates between the rental table and the datetime-filtered DataFrame, then we clearly need to provide an additional filter on this DataFrame:\n",
    "\n",
    "## Namely: filter out any of these duplicate listings from the datetime-filtered pandas' DataFrame based on the above query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check on data type of bathrooms data: float32\n"
     ]
    }
   ],
   "source": [
    "# transform bathrooms data to float\n",
    "# Why float?: Because some listings specify half bathrooms--e.g., 1.5 denotes one-and-half bathrooms. Re: ids, integer data type not store the entire id value due to maximum (byte) storage constraints. \n",
    "def transform_cols_to_float(df, col_to_transform):\n",
    "    return df[col_to_transform].astype('float32')\n",
    "\n",
    "# convert bathrooms to float:\n",
    "df['bathrooms'] = transform_cols_to_float(df, 'bathrooms')    \n",
    "\n",
    "#sanity check\n",
    "print(f\"Sanity check on data type of bathrooms data: {df['bathrooms'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: The data types of ['bedrooms', 'prices', 'ids', 'sqft'] are now: \n",
      "bedrooms    int64\n",
      "prices      int64\n",
      "ids         int64\n",
      "sqft        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def transform_cols_to_int(df, list_of_cols_to_num):\n",
    "    \"\"\" Transform relevant attribute columns to numeric.\n",
    "    NB: Since the scraped 'prices' data can contain commas, we need to use str.replace(',','') to remove them before converting to numeric.\"\"\"\n",
    "    df['prices'] = df['prices'].str.replace(\",\",\"\") # remove commas from prices data (e.g.: '2500' vs '2,500')\n",
    "    # clean sqft data --remove all non-numeric data\n",
    "    df['sqft'] = df['sqft'].astype(str).str.replace(r'\\D+', '', regex=True) # remove all non-numeric data from 'sqft' col by using regex to replace any non-numeric data from col to null ('NaN') values via the str.replace() Pandas method\n",
    "    df['sqft'] = df['sqft'].replace(r'^\\s*$', np.nan, regex=True)  # replace all empty str sqft values with  null ('NaN') values \n",
    "    # clean prices data-- remove any records posted with sqft instead of price data\n",
    "    df = df[~df.prices.str.contains(\"ft2\")] # remove listings records with incorrectly posted prices data \n",
    "    # remove rows with any remaining null rows wrt list of cols (ie, sqft, prices, etc.) (so we can readily convert to int):\n",
    "    df = df.dropna(subset=list_of_cols_to_num) # remove rows with null data \n",
    "    # finally, convert all cols from list to 'int64' integer data type:\n",
    "    df[list_of_cols_to_num] = df[list_of_cols_to_num].astype('int64') # use int64 due to a) the long id values & b.) the occasional null values contained within the sqft col\n",
    "    print(f\"Sanity check: The data types of {list_of_cols_to_num} are now: \\n{df[list_of_cols_to_num].dtypes}\") # sanity check on columns' data types\n",
    "    return df\n",
    "\n",
    "\n",
    "# specify a list of cols to convert to integer\n",
    "cols_to_int = df[['bedrooms', 'prices', 'ids', 'sqft']]\n",
    "cols_to_int_lis = list(cols_to_int.columns)  # convert relevant cols to list of col names\n",
    "\n",
    "cols_to_int = [] # free space\n",
    "\n",
    "df = transform_cols_to_int(df, cols_to_int_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be no remaining duplicate listing ids (ie, 0 rows): \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_df(df):\n",
    "    \"\"\"Remove duplicate rows based on listing ids\"\"\"\n",
    "    return df.drop_duplicates(keep='first', subset = ['ids'])\n",
    "\n",
    "df = deduplicate_df(df)\n",
    "\n",
    "# sanity check -- \n",
    "clist_duplicate_ids_check = df[df.duplicated(\"ids\", keep= False)]\n",
    "print(f\"There should be no remaining duplicate listing ids (ie, 0 rows): \\n{clist_duplicate_ids_check.shape[0]}\")  # check that number of duplicate rows is false (ie, wrt duplicate listing ids)\n",
    "\n",
    "# free memory\n",
    "clist_duplicate_ids_check = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check--The remaining columns in the dataset are:\n",
      " Index(['ids', 'sqft', 'cities', 'prices', 'bedrooms', 'bathrooms', 'attr_vars',\n",
      "       'date_of_webcrawler', 'kitchen', 'date_posted', 'region', 'sub_region',\n",
      "       'cats_OK', 'dogs_OK', 'wheelchair_accessible', 'laundry_in_bldg',\n",
      "       'no_laundry', 'washer_and_dryer', 'washer_and_dryer_hookup',\n",
      "       'laundry_on_site', 'full_kitchen', 'dishwasher', 'refrigerator', 'oven',\n",
      "       'flooring_carpet', 'flooring_wood', 'flooring_tile',\n",
      "       'flooring_hardwood', 'flooring_other', 'apt', 'in_law_apt', 'condo',\n",
      "       'townhouse', 'cottage_or_cabin', 'single_fam', 'duplex', 'flat', 'land',\n",
      "       'is_furnished', 'attached_garage', 'detached_garage', 'carport',\n",
      "       'off_street_parking', 'no_parking', 'EV_charging', 'air_condition',\n",
      "       'no_smoking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def remove_col_with_given_starting_name(df, col_starting_name):\n",
    "    \"\"\"Remove each column from df that has a given starting name substring.\"\"\"\n",
    "    return df.loc[:, ~df.columns.str.startswith(col_starting_name)] \n",
    "\n",
    "# remove 'Unnamed' columns, which might be imported errouneously via pd.read_csv()\n",
    "df = remove_col_with_given_starting_name(df, 'Unnamed')\n",
    "\n",
    "# remove listing_urls column since we do not want to store these data into the SQL Server table-- why?: a.) because listing urls are not relevent to rental prices and b.) the listing urls quickly become invalid or dead links, so we have no need to refer back to them at this stage in the webscraping project.\n",
    "df = remove_col_with_given_starting_name(df, 'listing_urls')\n",
    "\n",
    "# remove listing_descrip col since we do not want to store these data in SQL table either;\n",
    "df = remove_col_with_given_starting_name(df, 'listing_descrip')\n",
    "\n",
    "# sanity check\n",
    "print(f\"Sanity check--The remaining columns in the dataset are:\\n {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data cleaning Step:\n",
    "\n",
    "## Determine if there are any rental listing duplicates between the rental table and the datetime-filtered DataFrame.\n",
    "\n",
    "## If any such duplicates exist, then we need to filter these duplicates out of the datetime-filtered DataFrame.\n",
    "\n",
    "## Implement SQL Query to determine whether there any duplicates in the datetime-filtered DataFrame relative to the SQL rental table:\n",
    "\n",
    "### I.e., use a query with an IN() operator to check for a list of all listing id values from the datetime-filtered DataFrame, and compare those values with all stored rental listings data in the rental table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7444562463',\n",
       " '7444561871',\n",
       " '7444562359',\n",
       " '7444562620',\n",
       " '7444561488',\n",
       " '7444562067',\n",
       " '7444555242',\n",
       " '7444544140',\n",
       " '7444543866',\n",
       " '7444543404',\n",
       " '7442184532',\n",
       " '7442468806',\n",
       " '7444521438',\n",
       " '7444509101',\n",
       " '7444501200',\n",
       " '7444496508',\n",
       " '7443602840',\n",
       " '7441918360',\n",
       " '7444496033',\n",
       " '7444495690',\n",
       " '7444493139',\n",
       " '7444492966',\n",
       " '7444492538',\n",
       " '7444492451',\n",
       " '7444492377',\n",
       " '7444492290',\n",
       " '7444492200',\n",
       " '7444490137',\n",
       " '7444489592',\n",
       " '7444489451',\n",
       " '7444486790',\n",
       " '7444485390',\n",
       " '7444485181',\n",
       " '7444484883',\n",
       " '7444483834',\n",
       " '7444482612',\n",
       " '7444481977',\n",
       " '7444481322',\n",
       " '7444481312',\n",
       " '7444480953',\n",
       " '7444479681',\n",
       " '7444477596',\n",
       " '7444476831',\n",
       " '7444476749',\n",
       " '7444476668',\n",
       " '7444476228',\n",
       " '7442290175',\n",
       " '7444474276',\n",
       " '7444467806',\n",
       " '7444465445',\n",
       " '7444465386',\n",
       " '7444461161',\n",
       " '7444458960',\n",
       " '7444459780',\n",
       " '7444459388',\n",
       " '7444459379',\n",
       " '7444459265',\n",
       " '7444456739',\n",
       " '7444459095',\n",
       " '7444457620',\n",
       " '7444457079',\n",
       " '7444456924',\n",
       " '7444456742',\n",
       " '7444456191',\n",
       " '7444455129',\n",
       " '7444451121',\n",
       " '7444450679',\n",
       " '7444449884',\n",
       " '7444449135',\n",
       " '7444445397',\n",
       " '7444443827',\n",
       " '7444442735',\n",
       " '7444442592',\n",
       " '7444441617',\n",
       " '7444440892',\n",
       " '7444436708',\n",
       " '7443120792',\n",
       " '7444426199',\n",
       " '7444424893',\n",
       " '7444428652',\n",
       " '7444428180',\n",
       " '7444427994',\n",
       " '7444426584',\n",
       " '7444424828',\n",
       " '7444419850',\n",
       " '7444418709',\n",
       " '7444397361',\n",
       " '7444401361',\n",
       " '7444400309',\n",
       " '7444398516',\n",
       " '7444395572',\n",
       " '7444392015',\n",
       " '7444386634',\n",
       " '7444386160',\n",
       " '7444385433',\n",
       " '7444385093',\n",
       " '7444385013',\n",
       " '7444384720',\n",
       " '7444383861',\n",
       " '7444383284',\n",
       " '7444381617',\n",
       " '7444381497',\n",
       " '7444378460',\n",
       " '7443399272',\n",
       " '7444372522',\n",
       " '7444371474',\n",
       " '7444369063',\n",
       " '7444368323',\n",
       " '7444363566',\n",
       " '7444362071',\n",
       " '7444360110',\n",
       " '7444359159',\n",
       " '7444357852',\n",
       " '7444354009',\n",
       " '7444355636',\n",
       " '7444353444',\n",
       " '7444352599',\n",
       " '7444350509',\n",
       " '7444350475',\n",
       " '7444350326',\n",
       " '7444350296',\n",
       " '7444350137',\n",
       " '7444350114',\n",
       " '7444349950',\n",
       " '7444349772',\n",
       " '7443112410',\n",
       " '7444346864',\n",
       " '7444345333',\n",
       " '7443015563',\n",
       " '7444337471',\n",
       " '7444337300',\n",
       " '7444324286',\n",
       " '7444335023',\n",
       " '7444329238',\n",
       " '7444329059',\n",
       " '7444328622',\n",
       " '7444328412',\n",
       " '7444328248',\n",
       " '7444326217',\n",
       " '7444320154',\n",
       " '7444314046',\n",
       " '7444312649',\n",
       " '7444310571',\n",
       " '7444312296',\n",
       " '7444293254',\n",
       " '7443121139',\n",
       " '7444311621',\n",
       " '7444311498',\n",
       " '7444310524',\n",
       " '7444309899',\n",
       " '7444309597',\n",
       " '7444309414',\n",
       " '7444309242',\n",
       " '7444309044',\n",
       " '7444308907',\n",
       " '7444308190',\n",
       " '7444292035',\n",
       " '7444307501',\n",
       " '7442898273',\n",
       " '7444293887',\n",
       " '7444291046',\n",
       " '7444292601',\n",
       " '7444298800',\n",
       " '7444296638',\n",
       " '7444296255',\n",
       " '7442275181',\n",
       " '7444286021',\n",
       " '7444282585',\n",
       " '7444289856',\n",
       " '7444289677',\n",
       " '7444289490',\n",
       " '7444289276',\n",
       " '7444288586',\n",
       " '7444286863',\n",
       " '7444286630',\n",
       " '7444284614',\n",
       " '7444283549',\n",
       " '7444283132',\n",
       " '7444279258',\n",
       " '7444276640',\n",
       " '7444274530',\n",
       " '7444279415',\n",
       " '7444278195',\n",
       " '7444277121',\n",
       " '7444275593',\n",
       " '7444258541',\n",
       " '7444264419',\n",
       " '7444271395',\n",
       " '7444270396',\n",
       " '7444269410',\n",
       " '7444261047',\n",
       " '7444266732',\n",
       " '7444265352',\n",
       " '7444263057',\n",
       " '7444263982',\n",
       " '7444263033',\n",
       " '7444262461',\n",
       " '7444261811',\n",
       " '7444261406',\n",
       " '7444256933',\n",
       " '7444260920',\n",
       " '7444257900',\n",
       " '7444259045',\n",
       " '7444253762',\n",
       " '7444247539',\n",
       " '7444246514',\n",
       " '7444245434',\n",
       " '7444239801',\n",
       " '7444237771',\n",
       " '7444237076',\n",
       " '7444236878',\n",
       " '7444234328',\n",
       " '7442647123',\n",
       " '7442993021',\n",
       " '7444186702',\n",
       " '7444183151',\n",
       " '7444125288',\n",
       " '7444124072',\n",
       " '7444113777',\n",
       " '7444105314',\n",
       " '7444103339',\n",
       " '7444103466',\n",
       " '7441953019',\n",
       " '7444098422',\n",
       " '7444096595',\n",
       " '7444089637',\n",
       " '7444079514',\n",
       " '7444072657',\n",
       " '7444055657',\n",
       " '7444050095',\n",
       " '7444048982',\n",
       " '7444046890',\n",
       " '7444042390',\n",
       " '7444042030',\n",
       " '7444041849',\n",
       " '7444038486',\n",
       " '7444036289',\n",
       " '7444034181',\n",
       " '7444033542',\n",
       " '7444030711',\n",
       " '7444030534',\n",
       " '7444029324',\n",
       " '7444025274',\n",
       " '7444019953',\n",
       " '7444019520',\n",
       " '7444019343',\n",
       " '7444018533',\n",
       " '7444016498',\n",
       " '7444016084',\n",
       " '7444015886',\n",
       " '7444014827',\n",
       " '7444012336',\n",
       " '7444005053',\n",
       " '7444004466',\n",
       " '7444002505',\n",
       " '7443999205',\n",
       " '7443999031',\n",
       " '7443986050',\n",
       " '7443986817',\n",
       " '7443075205',\n",
       " '7442683228',\n",
       " '7443979759',\n",
       " '7443971840',\n",
       " '7443966315',\n",
       " '7443966271',\n",
       " '7443940306',\n",
       " '7443940236',\n",
       " '7443924191',\n",
       " '7443932549',\n",
       " '7443928753',\n",
       " '7443929384',\n",
       " '7443927028',\n",
       " '7442106252',\n",
       " '7443924988',\n",
       " '7443924070',\n",
       " '7443923756',\n",
       " '7443922952',\n",
       " '7443919987',\n",
       " '7443919252',\n",
       " '7443918828',\n",
       " '7443916467',\n",
       " '7443903534',\n",
       " '7443900425',\n",
       " '7443912895',\n",
       " '7442728618',\n",
       " '7443912006',\n",
       " '7443900155',\n",
       " '7443900952',\n",
       " '7443896362',\n",
       " '7443903420',\n",
       " '7443897922',\n",
       " '7443894417',\n",
       " '7443893749',\n",
       " '7443880123',\n",
       " '7443879595',\n",
       " '7443878261',\n",
       " '7443874633',\n",
       " '7443874216',\n",
       " '7443873845',\n",
       " '7443873197',\n",
       " '7443872400',\n",
       " '7443872208',\n",
       " '7443870475',\n",
       " '7443870375',\n",
       " '7443870153',\n",
       " '7443869779',\n",
       " '7443869181',\n",
       " '7443865448',\n",
       " '7443858226',\n",
       " '7443857251',\n",
       " '7443854982',\n",
       " '7443847148',\n",
       " '7443853076',\n",
       " '7443844849',\n",
       " '7442883353',\n",
       " '7443842031',\n",
       " '7443839334',\n",
       " '7443835504',\n",
       " '7443834341',\n",
       " '7443832116',\n",
       " '7443831740',\n",
       " '7443830137',\n",
       " '7443830027',\n",
       " '7443814200',\n",
       " '7443826848',\n",
       " '7443825837',\n",
       " '7443822917',\n",
       " '7443821521',\n",
       " '7443819132',\n",
       " '7443814623',\n",
       " '7443812069',\n",
       " '7443810082',\n",
       " '7443787485',\n",
       " '7443761681',\n",
       " '7443740787',\n",
       " '7443723529',\n",
       " '7443723203',\n",
       " '7443719543',\n",
       " '7443719348',\n",
       " '7443686470',\n",
       " '7443659463',\n",
       " '7443656373',\n",
       " '7443655221',\n",
       " '7443651451',\n",
       " '7443651433',\n",
       " '7443649124',\n",
       " '7443646061',\n",
       " '7443642206',\n",
       " '7443623862',\n",
       " '7443612761',\n",
       " '7443606943',\n",
       " '7443606025',\n",
       " '7443605578',\n",
       " '7443596366',\n",
       " '7443601573',\n",
       " '7443602561',\n",
       " '7443594884',\n",
       " '7443597458',\n",
       " '7443590308',\n",
       " '7443593257',\n",
       " '7443593145',\n",
       " '7443592924',\n",
       " '7443592705',\n",
       " '7443592146',\n",
       " '7443592061',\n",
       " '7443581468',\n",
       " '7443585688',\n",
       " '7443586391',\n",
       " '7443585509',\n",
       " '7443582744',\n",
       " '7443578613',\n",
       " '7443569575',\n",
       " '7443568413',\n",
       " '7443563610',\n",
       " '7443563122',\n",
       " '7443559198',\n",
       " '7443551611',\n",
       " '7443549782',\n",
       " '7443548631',\n",
       " '7443545676',\n",
       " '7443545405',\n",
       " '7443547025',\n",
       " '7443544090',\n",
       " '7443547645',\n",
       " '7443545045',\n",
       " '7443546792',\n",
       " '7443544714',\n",
       " '7443543405',\n",
       " '7443539547',\n",
       " '7443538989',\n",
       " '7443535105',\n",
       " '7443522919',\n",
       " '7443520597',\n",
       " '7443513710',\n",
       " '7443519635',\n",
       " '7443515016',\n",
       " '7443514075',\n",
       " '7443509066',\n",
       " '7443502241',\n",
       " '7443498088',\n",
       " '7443492774',\n",
       " '7442350022',\n",
       " '7441969162',\n",
       " '7443484064',\n",
       " '7443477694',\n",
       " '7443477115',\n",
       " '7443474369',\n",
       " '7443472071',\n",
       " '7443461679',\n",
       " '7443461212',\n",
       " '7443468059',\n",
       " '7443467183',\n",
       " '7443444219',\n",
       " '7443460441',\n",
       " '7443457128',\n",
       " '7443443476',\n",
       " '7443445587',\n",
       " '7443443727',\n",
       " '7443443164',\n",
       " '7443440468',\n",
       " '7443440301',\n",
       " '7443438869',\n",
       " '7443437578',\n",
       " '7443435598',\n",
       " '7443435554',\n",
       " '7443431051',\n",
       " '7443430131',\n",
       " '7443426537',\n",
       " '7443422572',\n",
       " '7443415530',\n",
       " '7443413293',\n",
       " '7443398351',\n",
       " '7443410528',\n",
       " '7443399914',\n",
       " '7443392887',\n",
       " '7443362238',\n",
       " '7443381806',\n",
       " '7443380506',\n",
       " '7443378709',\n",
       " '7443378618',\n",
       " '7443373800',\n",
       " '7443374480',\n",
       " '7443373004',\n",
       " '7443359625',\n",
       " '7443355039',\n",
       " '7443354443',\n",
       " '7443352451',\n",
       " '7443350695',\n",
       " '7443348905',\n",
       " '7443346690',\n",
       " '7443343573',\n",
       " '7443340893',\n",
       " '7443294844',\n",
       " '7443215211',\n",
       " '7443211734',\n",
       " '7443208840',\n",
       " '7443202576',\n",
       " '7443201935',\n",
       " '7443200747',\n",
       " '7443195205',\n",
       " '7443191486',\n",
       " '7443184627',\n",
       " '7443182364',\n",
       " '7443180954',\n",
       " '7443178552',\n",
       " '7443174439',\n",
       " '7443170131',\n",
       " '7443161782',\n",
       " '7443149670',\n",
       " '7443149303',\n",
       " '7443145768',\n",
       " '7443146374',\n",
       " '7443141980',\n",
       " '7443137576',\n",
       " '7443139728',\n",
       " '7443137861',\n",
       " '7443137609',\n",
       " '7443132819',\n",
       " '7443132337',\n",
       " '7443119729',\n",
       " '7443117018',\n",
       " '7443116705',\n",
       " '7443114430',\n",
       " '7443113036',\n",
       " '7443112962',\n",
       " '7443107935',\n",
       " '7443108949',\n",
       " '7443108488',\n",
       " '7443103170',\n",
       " '7443110309',\n",
       " '7443103926',\n",
       " '7443096009',\n",
       " '7443075775',\n",
       " '7443076415',\n",
       " '7443076728',\n",
       " '7443071630',\n",
       " '7443066454',\n",
       " '7443065547',\n",
       " '7443065491',\n",
       " '7443056256',\n",
       " '7443056217',\n",
       " '7443048041',\n",
       " '7443040958',\n",
       " '7443017078',\n",
       " '7443006631',\n",
       " '7443001784',\n",
       " '7442997819',\n",
       " '7442987549',\n",
       " '7442975169',\n",
       " '7442969484',\n",
       " '7442966013',\n",
       " '7442964033',\n",
       " '7442952065',\n",
       " '7442959636',\n",
       " '7442951248',\n",
       " '7442949858',\n",
       " '7442949134',\n",
       " '7442936390',\n",
       " '7442929148',\n",
       " '7442926104',\n",
       " '7442925610',\n",
       " '7442919346',\n",
       " '7442922248',\n",
       " '7442106367',\n",
       " '7442896564',\n",
       " '7442908438',\n",
       " '7442908032',\n",
       " '7442907474',\n",
       " '7442906920',\n",
       " '7442906313',\n",
       " '7442903434',\n",
       " '7442886894',\n",
       " '7442849887',\n",
       " '7442744835',\n",
       " '7442739394',\n",
       " '7442722289',\n",
       " '7442720054',\n",
       " '7442700590',\n",
       " '7442700506',\n",
       " '7442697898',\n",
       " '7442697792',\n",
       " '7442690919',\n",
       " '7442683541',\n",
       " '7442679183',\n",
       " '7442678275',\n",
       " '7442674964',\n",
       " '7442673881',\n",
       " '7442667385',\n",
       " '7442665975',\n",
       " '7442655995',\n",
       " '7442640327',\n",
       " '7442630305',\n",
       " '7442613249',\n",
       " '7442609161',\n",
       " '7442608959',\n",
       " '7442608648',\n",
       " '7442608469',\n",
       " '7442608256',\n",
       " '7442607577',\n",
       " '7442597570',\n",
       " '7442582577',\n",
       " '7442578677',\n",
       " '7442554734',\n",
       " '7442555271',\n",
       " '7442553555',\n",
       " '7442551358',\n",
       " '7442536047',\n",
       " '7442544201',\n",
       " '7442536906',\n",
       " '7442533773',\n",
       " '7442532748',\n",
       " '7442524930',\n",
       " '7442517123',\n",
       " '7442516634',\n",
       " '7442512019',\n",
       " '7442509831',\n",
       " '7442509545',\n",
       " '7442500696',\n",
       " '7442492985',\n",
       " '7442492020',\n",
       " '7442382690',\n",
       " '7442381579',\n",
       " '7442378604',\n",
       " '7442372709',\n",
       " '7442361088',\n",
       " '7442361009',\n",
       " '7442358683',\n",
       " '7442356703',\n",
       " '7442343562',\n",
       " '7442336868',\n",
       " '7442333885',\n",
       " '7442333276',\n",
       " '7442332685',\n",
       " '7442331929',\n",
       " '7442330551',\n",
       " '7442326595',\n",
       " '7442326385',\n",
       " '7442325994',\n",
       " '7442325783',\n",
       " '7442325597',\n",
       " '7442324274',\n",
       " '7442310572',\n",
       " '7442306622',\n",
       " '7442310085',\n",
       " '7442309618',\n",
       " '7442299283',\n",
       " '7442299012',\n",
       " '7442298789',\n",
       " '7442298592',\n",
       " '7442298469',\n",
       " '7442298416',\n",
       " '7442297996',\n",
       " '7442297593',\n",
       " '7442294866',\n",
       " '7442290531',\n",
       " '7442283889',\n",
       " '7442283510',\n",
       " '7442290419',\n",
       " '7442284808',\n",
       " '7442285215',\n",
       " '7442283703',\n",
       " '7442284106',\n",
       " '7442285082',\n",
       " '7442283178',\n",
       " '7442276560',\n",
       " '7442280728',\n",
       " '7442271734',\n",
       " '7442275618',\n",
       " '7442275867',\n",
       " '7442271203',\n",
       " '7442269726',\n",
       " '7442251299',\n",
       " '7442252452',\n",
       " '7442262289',\n",
       " '7442250388',\n",
       " '7442230469',\n",
       " '7442230214',\n",
       " '7442238678',\n",
       " '7442237701',\n",
       " '7442229989',\n",
       " '7442227473',\n",
       " '7442226138',\n",
       " '7442225991',\n",
       " '7442220484',\n",
       " '7442212347',\n",
       " '7442205394',\n",
       " '7442204286',\n",
       " '7442203922',\n",
       " '7442200221',\n",
       " '7442198461',\n",
       " '7442197570',\n",
       " '7442189539',\n",
       " '7442185337',\n",
       " '7442165707',\n",
       " '7442180891',\n",
       " '7442177893',\n",
       " '7442170550',\n",
       " '7442161485',\n",
       " '7442156918',\n",
       " '7442151842',\n",
       " '7442151622',\n",
       " '7442150779',\n",
       " '7442150528',\n",
       " '7442143892',\n",
       " '7442114945',\n",
       " '7442122994',\n",
       " '7442121143',\n",
       " '7442120896',\n",
       " '7442115860',\n",
       " '7442114379',\n",
       " '7442116268',\n",
       " '7442110252',\n",
       " '7442112641',\n",
       " '7442101800',\n",
       " '7442081807',\n",
       " '7442081389',\n",
       " '7442014135',\n",
       " '7441973625',\n",
       " '7441961118',\n",
       " '7441957736',\n",
       " '7441957597',\n",
       " '7441950769',\n",
       " '7441949960',\n",
       " '7441947115',\n",
       " '7441944991',\n",
       " '7441944691',\n",
       " '7441934530',\n",
       " '7441922873',\n",
       " '7441913501',\n",
       " '7441913177',\n",
       " '7441909635',\n",
       " '7441907098',\n",
       " '7441908391',\n",
       " '7441905855',\n",
       " '7441903708',\n",
       " '7441902839',\n",
       " '7447549099',\n",
       " '7447548552',\n",
       " '7447543129',\n",
       " '7447540580',\n",
       " '7445555948',\n",
       " '7447521833',\n",
       " '7447514608',\n",
       " '7447510523',\n",
       " '7447504699',\n",
       " '7447500993',\n",
       " '7445377365',\n",
       " '7447491495',\n",
       " '7447488700',\n",
       " '7447482817',\n",
       " '7447482027',\n",
       " '7447479819',\n",
       " '7447475147',\n",
       " '7447475937',\n",
       " '7447475304',\n",
       " '7447474743',\n",
       " '7447474684',\n",
       " '7447474318',\n",
       " '7447471235',\n",
       " '7447470844',\n",
       " '7447469195',\n",
       " '7447469021',\n",
       " '7447468837',\n",
       " '7447467796',\n",
       " '7447467253',\n",
       " '7447465755',\n",
       " '7447460642',\n",
       " '7447424874',\n",
       " '7447459645',\n",
       " '7447462044',\n",
       " '7447457157',\n",
       " '7447455224',\n",
       " '7447455140',\n",
       " '7447453555',\n",
       " '7446438681',\n",
       " '7447440223',\n",
       " '7447436269',\n",
       " '7447437861',\n",
       " '7447437389',\n",
       " '7447437053',\n",
       " '7447433321',\n",
       " '7447435968',\n",
       " '7447435702',\n",
       " '7447433641',\n",
       " '7447428742',\n",
       " '7447428344',\n",
       " '7444983544',\n",
       " '7447420399',\n",
       " '7447418714',\n",
       " '7447417604',\n",
       " '7447414758',\n",
       " '7447414485',\n",
       " '7447414161',\n",
       " '7447409579',\n",
       " '7447407277',\n",
       " '7447407061',\n",
       " '7447407000',\n",
       " '7447400455',\n",
       " '7447405245',\n",
       " '7447399351',\n",
       " '7447393484',\n",
       " '7447398351',\n",
       " '7447393249',\n",
       " '7447393042',\n",
       " '7447395299',\n",
       " '7447392545',\n",
       " '7447394404',\n",
       " '7447393185',\n",
       " '7447392826',\n",
       " '7447390960',\n",
       " '7447389422',\n",
       " '7447375871',\n",
       " '7447376001',\n",
       " '7447388239',\n",
       " '7447376233',\n",
       " '7447387397',\n",
       " '7444645884',\n",
       " '7447383936',\n",
       " '7447383377',\n",
       " '7447375465',\n",
       " '7447379657',\n",
       " '7447377621',\n",
       " '7447375396',\n",
       " '7447374853',\n",
       " '7447371555',\n",
       " '7447370749',\n",
       " '7447366917',\n",
       " '7447364771',\n",
       " '7447364762',\n",
       " '7447360895',\n",
       " '7447361675',\n",
       " '7447358072',\n",
       " '7447355786',\n",
       " '7447355244',\n",
       " '7447355229',\n",
       " '7447354660',\n",
       " '7447351230',\n",
       " '7447346010',\n",
       " '7447343626',\n",
       " '7447333311',\n",
       " '7445970220',\n",
       " '7447333496',\n",
       " '7447333463',\n",
       " '7447332784',\n",
       " '7447331328',\n",
       " '7447305882',\n",
       " '7447297245',\n",
       " '7445320858',\n",
       " '7447319695',\n",
       " '7444841724',\n",
       " '7447315049',\n",
       " '7447314934',\n",
       " '7446043397',\n",
       " '7447312686',\n",
       " '7447312189',\n",
       " '7447311850',\n",
       " '7447311466',\n",
       " '7447305111',\n",
       " '7446350218',\n",
       " '7447304990',\n",
       " '7447304032',\n",
       " '7447302536',\n",
       " '7447301310',\n",
       " '7447300435',\n",
       " '7447300375',\n",
       " '7447299607',\n",
       " '7447299490',\n",
       " '7447298200',\n",
       " '7447297820',\n",
       " '7447297435',\n",
       " '7447295531',\n",
       " '7447291501',\n",
       " '7447289755',\n",
       " '7447289433',\n",
       " '7447287885',\n",
       " '7447284458',\n",
       " '7447277184',\n",
       " '7447276863',\n",
       " '7447275998',\n",
       " '7447275078',\n",
       " '7447274994',\n",
       " '7445912235',\n",
       " '7443848212',\n",
       " '7447269276',\n",
       " '7447265698',\n",
       " '7447265631',\n",
       " '7447265059',\n",
       " '7447264742',\n",
       " '7447264060',\n",
       " '7447263825',\n",
       " '7447263697',\n",
       " '7447263661',\n",
       " '7447263625',\n",
       " '7447263063',\n",
       " '7447258202',\n",
       " '7447258071',\n",
       " '7447256855',\n",
       " '7447256101',\n",
       " '7447255894',\n",
       " '7447255326',\n",
       " '7447255011',\n",
       " '7447254702',\n",
       " '7447254189',\n",
       " '7447245706',\n",
       " '7447253042',\n",
       " '7447251862',\n",
       " '7447251377',\n",
       " '7447251073',\n",
       " '7447250687',\n",
       " '7447250260',\n",
       " '7447242236',\n",
       " '7447248696',\n",
       " '7447247639',\n",
       " '7447246724',\n",
       " '7447245620',\n",
       " '7447243512',\n",
       " '7447231912',\n",
       " '7447229997',\n",
       " '7444927353',\n",
       " '7447200426',\n",
       " '7447198303',\n",
       " '7447161194',\n",
       " '7447158792',\n",
       " '7447114943',\n",
       " '7447114898',\n",
       " '7447112267',\n",
       " '7447108305',\n",
       " '7447101524',\n",
       " '7447095389',\n",
       " '7447093679',\n",
       " '7447092915',\n",
       " '7447084126',\n",
       " '7447073082',\n",
       " '7447072846',\n",
       " '7445662116',\n",
       " '7447059924',\n",
       " '7447053743',\n",
       " '7447052917',\n",
       " '7447051578',\n",
       " '7447048698',\n",
       " '7447044784',\n",
       " '7447038694',\n",
       " '7447037764',\n",
       " '7447035415',\n",
       " '7447034846',\n",
       " '7447033260',\n",
       " '7447032610',\n",
       " '7447032303',\n",
       " '7447031742',\n",
       " '7447031716',\n",
       " '7447031307',\n",
       " '7447030961',\n",
       " '7447029850',\n",
       " '7447029002',\n",
       " '7447028032',\n",
       " '7447027368',\n",
       " '7447026929',\n",
       " '7447026899',\n",
       " '7447025649',\n",
       " '7447025645',\n",
       " '7447019911',\n",
       " '7447018565',\n",
       " '7447017432',\n",
       " '7447015374',\n",
       " '7447013504',\n",
       " '7447012740',\n",
       " '7447012638',\n",
       " '7445142876',\n",
       " '7445142781',\n",
       " '7447007272',\n",
       " '7447006243',\n",
       " '7447005798',\n",
       " '7446997394',\n",
       " '7447003349',\n",
       " '7447002019',\n",
       " '7446999350',\n",
       " '7446997254',\n",
       " '7446995854',\n",
       " '7446994939',\n",
       " '7446993857',\n",
       " '7446990147',\n",
       " '7446981266',\n",
       " '7446987667',\n",
       " '7446984400',\n",
       " '7446984261',\n",
       " '7446980121',\n",
       " '7446979194',\n",
       " '7446971410',\n",
       " '7446970919',\n",
       " '7446970815',\n",
       " '7446970586',\n",
       " '7446969757',\n",
       " '7445154553',\n",
       " '7444881517',\n",
       " '7446964197',\n",
       " '7446958642',\n",
       " '7446959023',\n",
       " '7446957231',\n",
       " '7446953941',\n",
       " '7446942623',\n",
       " '7446946370',\n",
       " '7446930576',\n",
       " '7446929782',\n",
       " '7446925332',\n",
       " '7446924736',\n",
       " '7446911229',\n",
       " '7446921239',\n",
       " '7446917387',\n",
       " '7446916345',\n",
       " '7446908771',\n",
       " '7445681247',\n",
       " '7446891530',\n",
       " '7446891337',\n",
       " '7446888495',\n",
       " '7446885495',\n",
       " '7446884549',\n",
       " '7446884258',\n",
       " '7446881409',\n",
       " '7446880530',\n",
       " '7446875412',\n",
       " '7446877910',\n",
       " '7446874057',\n",
       " '7445747058',\n",
       " '7446868525',\n",
       " '7444838865',\n",
       " '7444843152',\n",
       " '7446865210',\n",
       " '7446864251',\n",
       " '7446863387',\n",
       " '7446862459',\n",
       " '7446862030',\n",
       " '7446861317',\n",
       " '7446858132',\n",
       " '7446849708',\n",
       " '7446849189',\n",
       " '7446848668',\n",
       " '7446848405',\n",
       " '7446846660',\n",
       " '7446843513',\n",
       " '7446842988',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.astype(str)  # convert dataframe to string, so we can enable pandas' df data to be more compatible with pyodbc library \n",
    "df_filtered_ids = df['ids'].to_list()  # obtain a list of all ids from the filtered df\n",
    "\n",
    "# get various '?' SQL placeholders (ie, to prevent SQL injections) and a comma for each elements of the list of ids values--NB: reference the len() of the df_filtered_ids list to get the proper number of placeholders\n",
    "q_placeholders = ','.join(\"?\" * len(df_filtered_ids))\n",
    "\n",
    "df_filtered_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "('42000', '[42000] [Microsoft][SQL Server Native Client 11.0]Syntax error, permission violation, or other nonspecific error (0) (SQLPrepare)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-115-c6f163399f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     99\u001b[0m \u001b[0mSQL_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQL_Database\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql_config_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    100\u001b[0m \u001b[1;31m# perform query to check for any listings that are duplicate between the data in the rental table and the datetime-filtered dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 101\u001b[1;33m \u001b[0mid_query_duplicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQL_db\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_for_listing_ids_via_SQL_in_operator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatest_date_str\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# only apply query if latest_date_str is *not* \"None\", to ensure at least some data has been inserted into SQL table before trying to perform the query on it!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-c6f163399f3f>\u001b[0m in \u001b[0;36mcheck_for_listing_ids_via_SQL_in_operator\u001b[1;34m(self, df, target_date)\u001b[0m\n",
      "\u001b[0;32m     62\u001b[0m             \u001b[0msql_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"SELECT * FROM rental WHERE listing_id IN ({q_placeholders});\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mid_query_duplicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_filtered_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42000', '[42000] [Microsoft][SQL Server Native Client 11.0]Syntax error, permission violation, or other nonspecific error (0) (SQLPrepare)')"
     ]
    }
   ],
   "source": [
    "# verify whether any of the listing id's in the filtered DataFrame dataset are already stored within the rental listing dataset\n",
    "\n",
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def check_for_listing_ids_via_SQL_in_operator(self, df, target_date):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        try:  # try to establish connection to SQL Server table via pyodbc connector\n",
    "            conn = pyodbc.connect(\n",
    "            f'DRIVER={self.driver};'\n",
    "            f'SERVER={self.server};'\n",
    "            f'DATABASE={self.database};'\n",
    "            f'UID={self.username};'\n",
    "            f'PWD={self.password};'\n",
    "            'Trusted_Connection=yes;'\n",
    "            )\n",
    "        \n",
    "        except pyodbc.Error as err:  # account for possible pyodbc SQL Server connection error\n",
    "            print(\"Python was not able to connect to SQL server database and. Please try again.\") \n",
    "\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        ## Perform SQL query on rental table to determine whether there are any rental listing duplicates--ie, listing ids from the filtered DataFrame that have already been inserted into the rental table--via SQL IN operator:\n",
    "        \n",
    "        # NB: *only* perform SQL query if at least some data has been inserted into SQL table--we can check whether any data exists based on whether the target_date has a value other than \"None\":\n",
    "        if target_date != \"None\":   # account for scenario in which *no data* has yet been inserted into the SQL table, by ensuring the target_date (ie, latest_date_str) is not equal to \"None\"\n",
    "\n",
    "            ## get a list of all listing ids from the datetime-filtered dataframe:\n",
    "            # df = df.astype(str)  # convert dataframe to string, so we can enable pandas' df data to be more compatible with pyodbc library \n",
    "            df_filtered_ids = df['ids'].to_list()  # obtain a list of all ids from the filtered df\n",
    "\n",
    "            # get various '?' SQL placeholders (ie, to prevent SQL injections) and a comma for each elements of the list of ids values--NB: reference the len() of the df_filtered_ids list to get the proper number of placeholders\n",
    "            \n",
    "            # q_placeholders = \", \".join([\"?\"] * len(df_filtered_ids))   # get '?' placeholders and comma for each id element from the df_filtered_ids list\n",
    "\n",
    "            # q_placeholders = \",\".join(\"?\" * len(df_filtered_ids))  # get '?' placeholders and comma for each id element from the df_filtered_ids list\n",
    "\n",
    "\n",
    "            ## specify query, and add placeholders in between each value being checked via the IN operator\n",
    "            # sql_query = \"\"\"SELECT * FROM rental WHERE listing_id IN (\"\"\"+ q_placeholders + \") \"\n",
    "\n",
    "            # sql_query = \"SELECT * FROM rental WHERE listing_id IN (%s)\" % df_filtered_ids\n",
    "\n",
    "            q_placeholders = ','.join(['?']*len(df_filtered_ids))\n",
    "\n",
    "            sql_query = \"\"\"SELECT * FROM rental WHERE listing_id IN ({q_placeholders});\"\"\"\n",
    "\n",
    "            id_query_duplicates = cursor.execute(sql_query, df_filtered_ids).fetchall()\n",
    "\n",
    "\n",
    "\n",
    "            # sql_query = 'SELECT * FROM rental WHERE listing_id IN ({})'.format(', '.join(['?' for _ in df_filtered_ids]))\n",
    "\n",
    "\n",
    "            # sql_query = \"SELECT * FROM rental WHERE listing_id IN (%s)\" % q_placeholders\n",
    "\n",
    "\n",
    "            # sql_executable = cursor.execute(sql_query, df_filtered_ids)\n",
    "\n",
    "            # id_query_duplicates = cursor.fetchall()\n",
    "\n",
    "            # sql_query = cursor.execute(\"\"\"SELECT * FROM rental WHERE listing_id IN {}\"\"\".format(q_placeholders), df_filtered_ids)\n",
    "\n",
    "\n",
    "            ## use Pandas' read_sql() method to parse the query, and use the id values of the df--ie, from the df_filtered_ids list--as the argument of the IN operator: \n",
    "            \n",
    "            # id_query_duplicates = pd.read_sql(sql_query, con=conn, params=[df_filtered_ids])  # query for any listings that have duplicate id's from the rental table relative to the \n",
    "            \n",
    "            # sanity check-- Are there any duplicate listings in the SQL table vs the datetime-filtered DataFrame?\n",
    "            print(f'Duplicate listings from the datetime-filtered dataframe relative to the SQL rental table:\\n{id_query_duplicates}')        \n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "            return id_query_duplicates\n",
    "        \n",
    "        else:  # ie: if *no data* has yet been inserted into SQL table, then do nothing w/ this method\n",
    "            pass\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# perform query to check for any listings that are duplicate between the data in the rental table and the datetime-filtered dataframe \n",
    "id_query_duplicates = SQL_db.check_for_listing_ids_via_SQL_in_operator(df, latest_date_str)   # only apply query if latest_date_str is *not* \"None\", to ensure at least some data has been inserted into SQL table before trying to perform the query on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_query_duplicates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-48-d63c6f0db938>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[1;31m# apply the filter to remove duplicate ids, based on the query results (ie, id_query_duplicates)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_duplicate_ids_relative_to_SQL_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_query_duplicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sanity check\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'id_query_duplicates' is not defined"
     ]
    }
   ],
   "source": [
    "# filter dataframe to remove any duplicate ids\n",
    "def remove_duplicate_ids_relative_to_SQL_table(df, id_query_duplicates):\n",
    "    \"\"\"Remove any records from df whose ids are already inserted into (ie, duplicates of) the rental SQL table, as given by the id_query_duplicates query results\"\"\"\n",
    "    if id_query_duplicates is not None:\n",
    "        id_query_duplicates_list = id_query_duplicates['listing_id'].astype(float).apply(int).to_list() # derive a list of duplicate ids from the query results' listing_id column\n",
    "        filtered_df = df[~df['ids'].isin(id_query_duplicates_list)]  # remove (ie, filter out) all duplicate ids using the negation of isin() \n",
    "        return filtered_df\n",
    "    else:   # do *not* apply filter if no data has yet been inserted into SQL table (ie, if id_query_duplicates is None)\n",
    "        return df\n",
    "    \n",
    "\n",
    "# apply the filter to remove duplicate ids, based on the query results (ie, id_query_duplicates)\n",
    "df = remove_duplicate_ids_relative_to_SQL_table(df, id_query_duplicates)\n",
    "\n",
    "df.info() # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step-- Data pipeline and data ingestion: \n",
    "\n",
    "## Insert the cleaned/filtered/deduplicated pandas' DataFrame into SQL rental table:\n",
    "\n",
    "#### NB: When using pyodbc to insert data from a dataframe into SQL Server table, we *may* need to transform all of the data from the dataframe to string, if SQL Server returns an error during an insertion attempt. This is because pyodbc will typically transform string values of numeric to SQL numeric data types such as int or float. \n",
    "\n",
    "#### Regardless, when using pyodbc to insert data into a SQL Server table, *always* use the  execute_many = True option so that the INSERT INTO statements will not be called upon as frequently. Ie, execute_many helps ensure that there will not be separate inserts for each row, as this is entirely unneccessary and extremely inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, insert all scraped data into the rental SQL Server table:\n",
    "\n",
    "\n",
    "\n",
    "### Insert scraped data from Pandas' dataframe to SQL Server rental table via pyodbc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist\n",
      "The number of records stored in the SQL table is: (15877, )\n",
      "\n",
      "A few of the inserted records are: [(7376737280, 10500, 'Daly City', 2295, 1, Decimal('1.0'), 'apartment\\nlaundry in bldg\\noff-street parking\\nrent period: monthly', datetime.datetime(2021, 10, 12, 0, 0), 1, datetime.datetime(2021, 9, 6, 21, 54), 'sfbay', 'pen', 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0), (7377009152, 23360, 'Burlingame', 1650, 0, Decimal('1.0'), 'cats are OK - purrr\\nflooring: wood\\napartment\\nlaundry in bldg\\nno smoking\\noff-street parking\\nrent period: monthly', datetime.datetime(2021, 10, 12, 0, 0), 1, datetime.datetime(2021, 9, 7, 12, 5), 'sfbay', 'pen', 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1), (7377930752, 6500, 'Redwood City', 3550, 1, Decimal('1.0'), 'EV charging\\nair conditioning\\napplication fee details: $38 credit/background check\\ncats are OK - purrr\\ndogs are OK - wooof\\napartment\\nw/d in unit\\nno smoking\\nattached garage\\nrent period: monthly\\nwheelchair accessible', datetime.datetime(2021, 10, 12, 0, 0), 1, datetime.datetime(2021, 9, 9, 10, 25), 'sfbay', 'pen', 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1)]\n"
     ]
    }
   ],
   "source": [
    "class SQL_Database:\n",
    "    def __init__(self, path_for_SQL_config):\n",
    "\n",
    "        with open(path_for_SQL_config,'r') as fh:\n",
    "            config = json.load(fh)\n",
    "\n",
    "        self.driver = config['driver']\n",
    "        self.server = config['server']\n",
    "        self.database = config['database']\n",
    "        self.username = config['username']\n",
    "        self.password = config['password']\n",
    "\n",
    "        print(self.database)\n",
    "\n",
    "    def insert_df_to_SQL_ETL(self, df):\n",
    "        \"\"\"Insert scraped Craigslist rental listings data (ie, the Pandas' dataframe)\n",
    "        to SQL Server database 'rental' table\"\"\"\n",
    "\n",
    "        # establish connection to SQL Server database-specify login credentials:\n",
    "        try:  # try to establish connection to SQL Server table via pyodbc connector\n",
    "            conn = pyodbc.connect(\n",
    "            f'DRIVER={self.driver};'\n",
    "            f'SERVER={self.server};'\n",
    "            f'DATABASE={self.database};'\n",
    "            f'UID={self.username};'\n",
    "            f'PWD={self.password};'\n",
    "            'Trusted_Connection=yes;'\n",
    "            )\n",
    "        \n",
    "        except pyodbc.Error as err:  # account for possible pyodbc SQL Server connection error\n",
    "            print(\"Python was not able to connect to SQL server database and. Please try again.\") \n",
    "\n",
    "\n",
    "        # initialize cursor so we can execute SQL code\n",
    "        cursor = conn.cursor() \n",
    "\n",
    "        cursor.fast_executemany = True  # speed up data ingesting by reducing the numbers of calls to server for inserts\n",
    "\n",
    "        # convert all variables from dataframe to str to avoid following SQL Server pyodbc error: 'ProgrammingError: ('Invalid parameter type.  param-index=2 param-type=function', 'HY105')'\n",
    "        # df = df.astype(str) # convert all df variables to str for ease of loading data into SQl Server table\n",
    "        \n",
    "        # insert scraped data from df to SQL table-- iterate over each row of each df col via .itertuples() method\n",
    "        \n",
    "        # Get the number of needed '?' placeholders by looking up the # of cols (ie, len()) of the dataframe), and use .join() to have each question mark separated by commas (NB: these placeholders are used to mitigate--and ideally avoid altogether--SQL injections)\n",
    "        q_mark_str = ','.join('?'*len(df.columns))  \n",
    "         \n",
    "\n",
    "        # specify INSERT INTO SQL statement--iterate over each row in df, and insert into SQL database:\n",
    "        for row in df.itertuples():  # iterate over each row from df\n",
    "            cursor.execute(f\"\"\"INSERT INTO rental (listing_id, sqft, city, price, bedrooms, bathrooms, attr_vars,\n",
    "            date_of_webcrawler, kitchen, date_posted, region, sub_region, cats_OK, dogs_OK, wheelchair_accessible,laundry_in_bldg, no_laundry, \n",
    "            washer_and_dryer, washer_and_dryer_hookup, laundry_on_site, full_kitchen, dishwasher, refrigerator,\n",
    "            oven,flooring_carpet, flooring_wood, flooring_tile, flooring_hardwood, flooring_other,apt, in_law_apt, condo, townhouse, cottage_or_cabin, single_fam, duplex, flat, land, is_furnished, attached_garage,\n",
    "            detached_garage, carport, off_street_parking, no_parking, EV_charging, air_condition, no_smoking) \n",
    "            VALUES ({q_mark_str})\"\"\",\n",
    "            (row.ids,\n",
    "            row.sqft,\n",
    "            row.cities,\n",
    "            row.prices, \n",
    "            row.bedrooms,\n",
    "            row.bathrooms,\n",
    "            row.attr_vars, \n",
    "            row.date_of_webcrawler,\n",
    "            row.kitchen,\n",
    "            row.date_posted,\n",
    "            row.region,\n",
    "            row.sub_region,\n",
    "            row.cats_OK,\n",
    "            row.dogs_OK,\n",
    "            row.wheelchair_accessible,\n",
    "            row.laundry_in_bldg, \n",
    "            row.no_laundry,\n",
    "            row.washer_and_dryer,\n",
    "            row.washer_and_dryer_hookup,\n",
    "            row.laundry_on_site,\n",
    "            row.full_kitchen,\n",
    "            row.dishwasher,\n",
    "            row.refrigerator,\n",
    "            row.oven,\n",
    "            row.flooring_carpet,\n",
    "            row.flooring_wood,\n",
    "            row.flooring_tile,\n",
    "            row.flooring_hardwood,\n",
    "            row.flooring_other,\n",
    "            row.apt, \n",
    "            row.in_law_apt,\n",
    "            row.condo,\n",
    "            row.townhouse,\n",
    "            row.cottage_or_cabin,\n",
    "            row.single_fam, \n",
    "            row.duplex,\n",
    "            row.flat,\n",
    "            row.land,\n",
    "            row.is_furnished,\n",
    "            row.attached_garage,\n",
    "            row.detached_garage,\n",
    "            row.carport,\n",
    "            row.off_street_parking,\n",
    "            row.no_parking,\n",
    "            row.EV_charging,\n",
    "            row.air_condition,\n",
    "            row.no_smoking)\n",
    "            )\n",
    "            \n",
    "        # save and commit changes to database\n",
    "        conn.commit()\n",
    "\n",
    "        # # sanity check-- ensure some data has been inserted into new SQL table\n",
    "        sql_table_count_records = conn.execute(\"\"\"SELECT COUNT(*) FROM rental;\"\"\").fetchall()\n",
    "        print(f\"The number of records stored in the SQL table is: {sql_table_count_records[0]}\")     \n",
    "        \n",
    "        sql_query_for_record_samples = conn.execute(\"\"\"SELECT TOP 3 * FROM rental;\"\"\").fetchall() # check out several of the records\n",
    "        print(f\"\\nA few of the inserted records are: {sql_query_for_record_samples}\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# specify path to json file containing SQL configuration/username data\n",
    "sql_config_path = \"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\SQL_config\\\\config.json\" \n",
    "\n",
    "SQL_db = SQL_Database(sql_config_path)  # NB: be sure to pass in path to the json SQL configuration file so we can load in the needed username, password, and configuration data to be able to access the SQL database\n",
    "# Ingest data from pandas' dataframe to SQL server--data pipeline: \n",
    "SQL_db.insert_df_to_SQL_ETL(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craigslist_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
