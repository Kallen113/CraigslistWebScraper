{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing city names data:\n",
    "\n",
    "### In this notebook, we want to clean data for several weeks' worth of *sfbay* rental listings in which the webcrawler had not been updated properly to scrape city names. In short, several weeks of rental listings from Jan-Feb 2023 have missing city names data (ie, for the 'cities' column).\n",
    "\n",
    "### To do this, we need to:\n",
    "\n",
    "### 1) Import all *sfbay* rental listings as a *single* conatenated DataFrame. \n",
    "\n",
    "### 1b) Then, clean and remove null city names.\n",
    "\n",
    "### 1c) Finally, parse out and clean the city names. We want to ultimately obtain a list of all unique city names for sfbay rental listings.\n",
    "\n",
    "### 1d) As said in 1c, derive a list of unique city names for sfbay rental listings.\n",
    "\n",
    "### 2) Second, we need to import each week of sfbay rental listings that have missing city names data--by subregion--as *separate* dataframes, ie, with each df corresponding to a given week and a given subregion. To do this, we need to filter the data to only data from mid January 2023 to \n",
    "\n",
    "### 2b) Then, **replace** the missing city names with matching city names--ie, based on the list of unique city names (see step 1)--vis-a-vis a regex str.findall() search of the city names as parsed from the rental listing URLs (ie, the listing_url col).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Import all sfbay rental listings data, so we can derive all unique sfbay city names: \n",
    "\n",
    "# imports-- file processing & datetime libraries\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "# SQL ODBC for API connection between Python & SQL Server\n",
    "import pyodbc\n",
    "# use json library to open a json file, which contains SQL credentials & configuration--ie, username, password, etc.\n",
    "import json \n",
    "\n",
    "## Data pipeline of Pandas' df to SQL Server -- import scraped craigslist rental listings data from CSV files to single Pandas' df: \n",
    "\n",
    "# recursively search parent direc to look up CSV files within subdirectories\n",
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files to a single Pandas' df using pd.concat()\"\"\"\n",
    "    # specify parent path of the relevant (sfbay) scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes\n",
    "    path =  parent_direc \n",
    "    # import each CSV file from directories, and then concatenate each CSV file into a single Pandas' DataFrame \n",
    "    df_concat = pd.concat((pd.read_csv(file,\n",
    "                                        sep = ',', encoding='utf-8'\n",
    "                                        ) for file in glob.iglob( # iterate over each CSV file in path\n",
    "                                            os.path.join(path, '**', fn_regex), # have glob.iglob() search for *only* CSV files-- ie, '*.csv'  \n",
    "                                            recursive=True)), ignore_index=True)  # set recursive to True to recursively search through all relevant child directories (ie, all subregions within given parent region path) \n",
    "    # ))\n",
    "    # df_concat = pd.concat((pd.read_csv(file, # import each CSV file from directory\n",
    "    #                                     sep=',',encoding = 'utf-8'  # assume standard CSV (ie, comma separated ) formt and use utf-8 encoding\n",
    "    #                                     ) for file in glob.iglob( # iterate over each CSV file in path\n",
    "    #                                         os.path.join(path, '**', fn_regex), \n",
    "    #                                         recursive=True)), ignore_index=True)  # recursively iterate over each CSV file in path, and use os.path.join to help ensure this concatenation is OS independent\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 317751 entries, 0 to 317750\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   listing_urls             317751 non-null  object \n",
      " 1   ids                      317394 non-null  float64\n",
      " 2   sqft                     248865 non-null  object \n",
      " 3   cities                   317634 non-null  object \n",
      " 4   prices                   317324 non-null  object \n",
      " 5   bedrooms                 312888 non-null  object \n",
      " 6   bathrooms                317386 non-null  object \n",
      " 7   attr_vars                317384 non-null  object \n",
      " 8   listing_descrip          317380 non-null  object \n",
      " 9   date_of_webcrawler       317636 non-null  object \n",
      " 10  kitchen                  317634 non-null  float64\n",
      " 11  date_posted              317366 non-null  object \n",
      " 12  region                   317751 non-null  object \n",
      " 13  sub_region               317751 non-null  object \n",
      " 14  cats_OK                  317751 non-null  object \n",
      " 15  dogs_OK                  317751 non-null  object \n",
      " 16  wheelchair_accessible    317751 non-null  object \n",
      " 17  laundry_in_bldg          317751 non-null  object \n",
      " 18  no_laundry               317751 non-null  object \n",
      " 19  washer_and_dryer         317751 non-null  object \n",
      " 20  washer_and_dryer_hookup  317751 non-null  object \n",
      " 21  laundry_on_site          317751 non-null  object \n",
      " 22  full_kitchen             317751 non-null  object \n",
      " 23  dishwasher               317751 non-null  object \n",
      " 24  refrigerator             317751 non-null  object \n",
      " 25  oven                     317751 non-null  object \n",
      " 26  flooring_carpet          317751 non-null  object \n",
      " 27  flooring_wood            317751 non-null  object \n",
      " 28  flooring_tile            317751 non-null  object \n",
      " 29  flooring_hardwood        317751 non-null  object \n",
      " 30  flooring_other           317751 non-null  object \n",
      " 31  apt                      317751 non-null  object \n",
      " 32  in_law_apt               317751 non-null  object \n",
      " 33  condo                    317751 non-null  object \n",
      " 34  townhouse                317751 non-null  object \n",
      " 35  cottage_or_cabin         317751 non-null  object \n",
      " 36  single_fam               317751 non-null  object \n",
      " 37  duplex                   317751 non-null  object \n",
      " 38  flat                     312342 non-null  float64\n",
      " 39  land                     312342 non-null  float64\n",
      " 40  is_furnished             317751 non-null  object \n",
      " 41  attached_garage          317751 non-null  object \n",
      " 42  detached_garage          317751 non-null  object \n",
      " 43  carport                  317751 non-null  object \n",
      " 44  off_street_parking       317751 non-null  object \n",
      " 45  no_parking               317751 non-null  object \n",
      " 46  EV_charging              317751 non-null  object \n",
      " 47  air_condition            317751 non-null  object \n",
      " 48  no_smoking               317751 non-null  object \n",
      "dtypes: float64(4), object(45)\n",
      "memory usage: 64.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# 1)-- cont'd: Import *all* sfbay rental listings data as a single df:\n",
    "# specify path of sfbay scraped data\n",
    "scraped_data_sfbay = r'D:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\scraped_data\\sfbay'\n",
    "\n",
    "# import all available scraped sfbay data:\n",
    "df = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_sfbay)\n",
    "\n",
    "# sanity check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next, let's subset the data to 2 separate dfs:\n",
    "\n",
    "\n",
    "\n",
    "# 1) b) Clean city names by removing anyb null city names:\n",
    "#  No 'cities' (ie, city names) nulls: subset data to all scraped data that actually contains city names data--ie: subset to no missing city names\n",
    "def filter_out_null_vals_for_col(df, col):\n",
    "    return df.loc[df[col].notnull()]\n",
    "\n",
    "# filter out missing city names:\n",
    "df_no_city_nulls = filter_out_null_vals_for_col(df, 'cities')\n",
    "\n",
    "# sanity check\n",
    "df_no_city_nulls['cities'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime64[ns, UTC]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Import Data from mid-January 2023 to Feb 14, 2023: subset data for Jan 15, 2023 to February 14, 2023\n",
    "\n",
    "# change col to datetime \n",
    "def transform_col_to_datetime(df, datetime_like_col):\n",
    "    \"\"\" Transform to datetime. \n",
    "    NB: use utc=True since pd.datetime() will otherwise throw a ValueError: Cannot mix tz-aware with tz-naive values.\n",
    "    utc=True will tell Pandas to create a timezone-aware datetime conversion.\"\"\"\n",
    "    return pd.to_datetime(df[datetime_like_col], utc=True)\n",
    "\n",
    "\n",
    "df['date_posted'] = transform_col_to_datetime(df, 'date_posted')\n",
    "\n",
    "# sanity check-- ensure datetime data type\n",
    "df['date_posted'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2021-12-27 17:45:00+00:00\n",
       "1        2022-01-03 00:49:00+00:00\n",
       "2        2022-01-03 00:20:00+00:00\n",
       "3        2021-12-10 13:16:00+00:00\n",
       "4        2021-12-19 02:14:00+00:00\n",
       "                    ...           \n",
       "317746   2022-11-04 11:07:00+00:00\n",
       "317747   2022-11-04 11:05:00+00:00\n",
       "317748   2022-11-04 10:57:00+00:00\n",
       "317749   2022-11-04 11:04:00+00:00\n",
       "317750   2022-11-04 11:04:00+00:00\n",
       "Name: date_posted, Length: 317751, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new datetime format\n",
    "df['date_posted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_urls</th>\n",
       "      <th>ids</th>\n",
       "      <th>sqft</th>\n",
       "      <th>cities</th>\n",
       "      <th>prices</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>attr_vars</th>\n",
       "      <th>listing_descrip</th>\n",
       "      <th>date_of_webcrawler</th>\n",
       "      <th>...</th>\n",
       "      <th>land</th>\n",
       "      <th>is_furnished</th>\n",
       "      <th>attached_garage</th>\n",
       "      <th>detached_garage</th>\n",
       "      <th>carport</th>\n",
       "      <th>off_street_parking</th>\n",
       "      <th>no_parking</th>\n",
       "      <th>EV_charging</th>\n",
       "      <th>air_condition</th>\n",
       "      <th>no_smoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266209</th>\n",
       "      <td>https://sfbay.craigslist.org/sfc/apa/d/san-fra...</td>\n",
       "      <td>7.578609e+09</td>\n",
       "      <td>725.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>3,295</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>application fee details: $35 per applicant for...</td>\n",
       "      <td>centrally located 1 bedroom 1 bath sunny garde...</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266328</th>\n",
       "      <td>https://sfbay.craigslist.org/sfc/apa/d/san-fra...</td>\n",
       "      <td>7.578635e+09</td>\n",
       "      <td>860.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>2,850</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>apartment\\nlaundry in bldg\\nno smoking\\nattach...</td>\n",
       "      <td>address:\\n155, 20 avenue, apt 4, sf, ca 94121,...</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266288</th>\n",
       "      <td>https://sfbay.craigslist.org/sfc/apa/d/san-fra...</td>\n",
       "      <td>7.578652e+09</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>3,850</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>flat\\nlaundry in bldg\\nno smoking\\nattached ga...</td>\n",
       "      <td>#top floor /large &amp; spacious 3br+1 extra room/...</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266107</th>\n",
       "      <td>https://sfbay.craigslist.org/sfc/apa/d/san-fra...</td>\n",
       "      <td>7.578653e+09</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>3,850</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>flat\\nlaundry in bldg\\nno smoking\\nattached ga...</td>\n",
       "      <td>## reduced to $3850 ##\\n\\n#top floor /large &amp; ...</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266225</th>\n",
       "      <td>https://sfbay.craigslist.org/sfc/apa/d/ready-f...</td>\n",
       "      <td>7.578670e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>3,675</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>apartment\\nno laundry on site\\nno smoking\\natt...</td>\n",
       "      <td>open house:\\n\\naddress: corner building... 191...</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255701</th>\n",
       "      <td>https://sfbay.craigslist.org/scz/apa/d/santa-c...</td>\n",
       "      <td>7.589953e+09</td>\n",
       "      <td>800.0</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>2,000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>air conditioning\\ncottage/cabin\\nw/d in unit\\n...</td>\n",
       "      <td>- lovely 1 bedroom, 1 bath stand alone cottage...</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255697</th>\n",
       "      <td>https://sfbay.craigslist.org/scz/apa/d/santa-c...</td>\n",
       "      <td>7.589960e+09</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>4,850</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>cats are OK - purrr\\ndogs are OK - wooof\\nhous...</td>\n",
       "      <td>available april 1, 2023\\n\\nlong term tenant de...</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255698</th>\n",
       "      <td>https://sfbay.craigslist.org/scz/apa/d/santa-c...</td>\n",
       "      <td>7.589960e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>1,250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>apartment\\nw/d in unit\\noff-street parking\\nre...</td>\n",
       "      <td>bright and charming, lots of natural light and...</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255699</th>\n",
       "      <td>https://sfbay.craigslist.org/scz/apa/d/santa-c...</td>\n",
       "      <td>7.589962e+09</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>2,750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>application fee details: $45\\ncats are OK - pu...</td>\n",
       "      <td>photos are similar but not actual. more photos...</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255696</th>\n",
       "      <td>https://sfbay.craigslist.org/scz/apa/d/santa-c...</td>\n",
       "      <td>7.589967e+09</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>2,995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>application fee details: $45\\ncats are OK - pu...</td>\n",
       "      <td>available now!\\n\\ntext matt for an application...</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5725 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             listing_urls           ids  \\\n",
       "266209  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7.578609e+09   \n",
       "266328  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7.578635e+09   \n",
       "266288  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7.578652e+09   \n",
       "266107  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7.578653e+09   \n",
       "266225  https://sfbay.craigslist.org/sfc/apa/d/ready-f...  7.578670e+09   \n",
       "...                                                   ...           ...   \n",
       "255701  https://sfbay.craigslist.org/scz/apa/d/santa-c...  7.589953e+09   \n",
       "255697  https://sfbay.craigslist.org/scz/apa/d/santa-c...  7.589960e+09   \n",
       "255698  https://sfbay.craigslist.org/scz/apa/d/santa-c...  7.589960e+09   \n",
       "255699  https://sfbay.craigslist.org/scz/apa/d/santa-c...  7.589962e+09   \n",
       "255696  https://sfbay.craigslist.org/scz/apa/d/santa-c...  7.589967e+09   \n",
       "\n",
       "          sqft         cities prices bedrooms bathrooms  \\\n",
       "266209   725.0  San Francisco  3,295        1         1   \n",
       "266328   860.0  San Francisco  2,850        2         1   \n",
       "266288  1685.0  San Francisco  3,850        3         2   \n",
       "266107  1685.0  San Francisco  3,850        3         2   \n",
       "266225     NaN  San Francisco  3,675        3         1   \n",
       "...        ...            ...    ...      ...       ...   \n",
       "255701   800.0     Santa Cruz  2,000        1         1   \n",
       "255697  1300.0     Santa Cruz  4,850        3         2   \n",
       "255698     NaN     Santa Cruz  1,250        1         1   \n",
       "255699   400.0     Santa Cruz  2,750        1         1   \n",
       "255696   250.0     Santa Cruz  2,995        0         1   \n",
       "\n",
       "                                                attr_vars  \\\n",
       "266209  application fee details: $35 per applicant for...   \n",
       "266328  apartment\\nlaundry in bldg\\nno smoking\\nattach...   \n",
       "266288  flat\\nlaundry in bldg\\nno smoking\\nattached ga...   \n",
       "266107  flat\\nlaundry in bldg\\nno smoking\\nattached ga...   \n",
       "266225  apartment\\nno laundry on site\\nno smoking\\natt...   \n",
       "...                                                   ...   \n",
       "255701  air conditioning\\ncottage/cabin\\nw/d in unit\\n...   \n",
       "255697  cats are OK - purrr\\ndogs are OK - wooof\\nhous...   \n",
       "255698  apartment\\nw/d in unit\\noff-street parking\\nre...   \n",
       "255699  application fee details: $45\\ncats are OK - pu...   \n",
       "255696  application fee details: $45\\ncats are OK - pu...   \n",
       "\n",
       "                                          listing_descrip date_of_webcrawler  \\\n",
       "266209  centrally located 1 bedroom 1 bath sunny garde...         2023-02-15   \n",
       "266328  address:\\n155, 20 avenue, apt 4, sf, ca 94121,...         2023-02-15   \n",
       "266288  #top floor /large & spacious 3br+1 extra room/...         2023-02-15   \n",
       "266107  ## reduced to $3850 ##\\n\\n#top floor /large & ...         2023-02-15   \n",
       "266225  open house:\\n\\naddress: corner building... 191...         2023-02-15   \n",
       "...                                                   ...                ...   \n",
       "255701  - lovely 1 bedroom, 1 bath stand alone cottage...         2023-02-16   \n",
       "255697  available april 1, 2023\\n\\nlong term tenant de...         2023-02-16   \n",
       "255698  bright and charming, lots of natural light and...         2023-02-16   \n",
       "255699  photos are similar but not actual. more photos...         2023-02-16   \n",
       "255696  available now!\\n\\ntext matt for an application...         2023-02-16   \n",
       "\n",
       "        ...  land is_furnished attached_garage detached_garage carport  \\\n",
       "266209  ...   0.0            0               0               0       0   \n",
       "266328  ...   0.0            0               1               0       0   \n",
       "266288  ...   0.0            0               1               0       0   \n",
       "266107  ...   0.0            0               1               0       0   \n",
       "266225  ...   0.0            0               1               0       0   \n",
       "...     ...   ...          ...             ...             ...     ...   \n",
       "255701  ...   0.0            0               0               0       0   \n",
       "255697  ...   0.0            0               1               0       0   \n",
       "255698  ...   0.0            0               0               0       0   \n",
       "255699  ...   0.0            0               0               0       0   \n",
       "255696  ...   0.0            0               0               0       0   \n",
       "\n",
       "       off_street_parking no_parking EV_charging air_condition no_smoking  \n",
       "266209                  0          0           0             0          1  \n",
       "266328                  0          0           0             0          1  \n",
       "266288                  0          0           0             0          1  \n",
       "266107                  0          0           0             0          1  \n",
       "266225                  0          0           0             0          1  \n",
       "...                   ...        ...         ...           ...        ...  \n",
       "255701                  1          0           0             1          1  \n",
       "255697                  0          0           0             0          1  \n",
       "255698                  1          0           0             0          0  \n",
       "255699                  0          0           0             0          1  \n",
       "255696                  0          0           0             0          1  \n",
       "\n",
       "[5725 rows x 49 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Import Data from mid-January 2023 to Feb 14, 2023: subset data for Jan 15, 2023 to February 14, 2023\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# specify subregion code\n",
    "subregion_code = 'pen'\n",
    "\n",
    "# specify parent path of all sfbay data-- NB: use an f-string combined with a raw (ie, r) string--ie, fr to modify the string so we can input the subregion code as an argument to add to the path\n",
    "path = fr'D:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\scraped_data\\sfbay\\{subregion_code}'\n",
    "\n",
    "\n",
    "# get all CSV files from path, and grab the file stems for each given CSV file \n",
    "df = pd.DataFrame({'files' : [file for file in Path(path).glob('*.csv')],\n",
    "                  'file_stem' : [file.stem for file in Path(path).glob('*.csv')]}) # get file stem using .stem method\n",
    "\n",
    "# # Parse the dates from each CSV file, and keep the same 'MM_DD_YYY' format (**including the underscore delimiters!!)\n",
    "# df['date_of_file'] = pd.to_datetime(df['file_stem'].str.extract('\\d{2}_\\d{2}_\\d{4}')[0]) \n",
    "\n",
    "# df['date_of_file'] = df['file_stem'].str.extract(r'\\d{2}_\\d{2}_\\d{4}')\n",
    "\n",
    "\n",
    "## \"craigslist_rental_sfbay_subregion_MM_DD_YYYY.csv\"\n",
    "# sanity check\n",
    "print(df)\n",
    "\n",
    "# df.to_csv('CSV_files_for_peninsula_test.csv', index=False)\n",
    "\n",
    "\n",
    "# ## ask user for the desired start & end dates, in format of 'MM_DD_YYYY'\n",
    "# start_date_month = str(input('Enter desired Start Date month: '))\n",
    "# start_date_day = str(input('Enter desired Start Date day: '))\n",
    "\n",
    "# start_date_year = str(input('Enter desired Start Date year: '))\n",
    "\n",
    "# end_date_month = str(input('Enter desired End Date month: '))\n",
    "# end_date_day = str(input('Enter desired End Date day: '))\n",
    "\n",
    "# end_date_year = str(input('Enter desired End Date year: '))\n",
    "\n",
    "\n",
    "## create a list of the start & end dates for filtering the files; then concatenate all matching files into a DataFrame\n",
    "\n",
    "# file_date_slice = df.set_index('date').loc[start_date:end_date]['files'].tolist()\n",
    "\n",
    "\n",
    "# concat_df = pd.concat([pd.read_csv(file).compute() for file in file_date_slice])\n",
    "\n",
    "\n",
    "## NB: My webcrawler program's CSV files are of this format:\n",
    "## \"craigslist_rental_sfbay_subregion_MM_DD_YYYY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove original df from memory since we no longer need it\n",
    "df = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Clean the city names data (ie, non null city names) by removing extraneous address & street data, non-sfbay cities, etc.\"\"\"\n",
    "\n",
    "def clean_split_city_names(df, address_critera: list, neighborhood_criteria:list, split_city_delimiters: list, incorrect_city_names:dict, cities_not_in_region:dict, cities_that_need_extra_cleaning:dict):\n",
    "    \"\"\"Clean city names data in several ways:\n",
    "    a.) Remove extraneous address & neighborhood data placed in the city names HTML object, such as 'Rd', 'Blvd', or 'Downtown'.\n",
    "    b.) Unsplit city names data that are split via ',' & '/' delimiters.\n",
    "    c.) Replace abbreviated or mispelled city names.\n",
    "    ci) Set all city names to lowercase by using .lower(), for sake of consistent data cleaning (casing will be parsed later).\n",
    "    d.) Remove city names that do not exist within the SF Bay Area (e.g., 'Redding')--ie, by using .replace() and replacing with whitespace (ie, ' '). \n",
    "    e.)Remove any digits/integers within the city names data--ie, by using a '\\d+' regex as the argument of str.replace() and replace it with empty strings.\n",
    "    f.) Remove any city names records that are left with merely empty strings (ie, the other steps removed all data for that given cities record).\n",
    "    g.) Remove any whitespace to avoid the same city names from being treated as different entities by Pandas, Python, or SQL. \n",
    "    h.) Use str.capwords() to capitalize words (ie, excluding apostrophes).\n",
    "    i.) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena'. \n",
    "    j) Remove any remaining empty strings, null records, or rows with literal 'nan' values (ie, resulting from previous data cleaning steps)\"\"\"\n",
    "    # specify extraneous street & address data (e.g., 'Rd') that we want to remove from the city names column:\n",
    "    addr_criteria = '|'.join(address_critera) # Join pipe ('|') symbols to address list so we can str.split() on any one of these criteria (ie, 'or' condition splitting on each element separated by pipes):\n",
    "    # specify extraneous neighborhood criteria that we should also remove from the column\n",
    "    nbhood_criteria = '|'.join(neighborhood_criteria) # specify neighborhood names as well as state abbreviation (shown on website as ' Ca') that is shown without the usual comma delimiter, which we should remove from the rows of cities col\n",
    "    # b.) specify delimiters we need to refer to un-split city names:\n",
    "    split_city_delimiters = '|'.join(split_city_delimiters) # join pipes to delimiters so we can use str.split() based on multiple 'or' criteria simultaneously\n",
    "    # clean city names data by removing extraneous address & neighborhood data, and unsplitting city names based on ',' & '\\' delimiters\n",
    "    df['cities'] =  df['cities'].str.split(addr_criteria).str[-1].str.replace(nbhood_criteria, '', case=True).str.lstrip()\n",
    "    df['cities'] = df['cities'].str.split(split_city_delimiters).str[0] #unsplit city names based on comma or forward-slash delimiters\n",
    "    # c.) replace specific abbreviated or mispelled city names\n",
    "    df = df.replace({'cities':incorrect_city_names}, regex=True) # replace mispelled & abbreviated city names\n",
    "    # ci) Set all city names data to lower-case temporarily, to ease the data cleaning & wrangling:\n",
    "    df['cities'] = df['cities'].str.lower()\n",
    "    \n",
    "    # d) remove data in which the cities are not actually located in the sfbay region:\n",
    "    df['cities'] = df['cities'].replace(cities_not_in_region, '', regex=True )  # remove (via empty string) cities that are not actually located in the sfbay region\n",
    "    # e.) Remove digits & integer-like data from cities column:\n",
    "    df['cities'] = df['cities'].str.replace('\\d+', '')  # remove any digits by using '/d+' regex to look up digits, and then replace with empty string\n",
    "    # f.) Remove any rows that have empty strings or null values for cities col (having performed the various data filtering and cleaning above)\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    # g.) Remove whitespace\n",
    "    df['cities'] = df['cities'].str.strip() \n",
    "    # h.) capitalize the city names using str.capwords() \n",
    "    df['cities'] = df['cities'].str.split().apply(lambda x: [val.capitalize() for val in x]).str.join(' ')\n",
    "    # i) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena' & 'San los' should be 'San Carlos'. Also, remove any non-Bay Area cities such as Redding:\n",
    "    df = df.replace({'cities':cities_that_need_extra_cleaning})\n",
    "    # j) Remove any remaining empty strings, null records, or rows with literal 'nan' values (ie, resulting from previous data cleaning steps)\n",
    "    # remove rows with literal 'nan' values\n",
    "    df['cities'] = df['cities'].replace('nan', '', regex=True)\n",
    "\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "     \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## clean split city names and clean abbreviated or incorrect city names:\n",
    "# specify various address and street name that we need to remove from the city names\n",
    "address_criteria = ['Boulevard', 'Blvd', 'Road', 'Rd', 'Avenue', 'Ave', 'Street', 'St', 'Drive', 'Dr', 'Real', 'E Hillsdale Blvd'] \n",
    "\n",
    "# specify various extraneous neighborhood names such as 'Downtown' \n",
    "neighborhood_criteria = ['Downtown', 'Central/Downtown', 'North', 'California', 'Ca.', 'Bay Area', 'St. Helena', 'St', 'nyon', \n",
    "'Jack London Square', 'Walking Distance To', 'El Camino', 'Mendocino County', 'San Mateo County', 'Alameda County', 'Rio Nido Nr', 'Mission Elementary', \n",
    "'Napa County', 'Golden Gate', 'Jennings', 'South Lake Tahoe', 'Tahoe Paradise', 'Kingswood Estates', 'South Bay', 'Skyline', \n",
    "'East Bay', 'Morton Dr', 'Cour De Jeune', \n",
    "'Area', 'Rotary Way', ' Ca', 'Near ', 'galen pl'] \n",
    "\n",
    "# specify what delimiters we want to search for to unsplit the split city names data:\n",
    "split_city_delimiters =  [',', '/', ' - ', '_____', '#']\n",
    "\n",
    "# specify dictionary of abbreviated & mispelled cities:\n",
    "incorrect_city_names = {'Rohnert Pk':'Rohnert Park', 'Hillsborough Ca': 'Hillsborough','Fremont Ca':'Fremont', 'South Sf': 'South San Francisco', 'Ca':'', 'East San Jose':'San Jose', 'Vallejo Ca':'Vallejo', 'Westgate On Saratoga .':'San Jose', 'Bodega':'Bodega Bay', 'Briarwood At Central Park':'Fremont', 'Campbell Ca':'Campbell', 'Almaden':'San Jose', '.':'', 'East Foothills':'San Jose', 'Lake County':'', 'West End':'Alameda', 'Redwood Shores':'Redwood City', 'Park Pacifica Neighborhood':'Pacifica'}\n",
    "\n",
    "# specify dictionary of cities that are not located in sfbay (ie, not located in the region):\n",
    "cities_not_in_region = ['Ketchum', 'Baypoinr', 'Quito' 'Redding', 'Bend', 'Pla Vada Woodland', 'San Antonio Tx', 'Mountain House Ca', 'Lakeside']\n",
    "\n",
    "# specify dictionary of city names that are mispelled after having removed various street and neighborhood substrings:\n",
    "cities_that_need_extra_cleaning = {'. Helena': 'St. Helena', '. Helena Deer Park': 'St. Helena', 'San Los':'San Carlos', 'Tro Valley':'Castro Valley', 'Rohnert Pk':'Rohnert Park',\n",
    "'Pbell':'Campbell', 'Pbell Ca':'Campbell', 'American Yon':'American Canyon', 'Millbrae On The Burlingame Border':'Millbrae', 'Ockton Ca': 'Stockton', '. Rohnert Park': 'Rohnert Park', 'Udio Behind Main House':'', '***---rohnert Park':'Rohnert Park',\n",
    "'Discovery Bay Ca':'Discovery Bay'}\n",
    "\n",
    "# specify list of city names that should be used explicitly instead of having multiple cities (e.g.: ''santa cruz columbia beach' or 'Felton area', instead of simply 'Santa Cruz' or 'Felton')\n",
    "# ie, use this list of values, look up substr via str.contains, and then use .replace() chained to .map() to replace all matching substr values with the values in the list\n",
    "city_names_for_str_contains = ['Santa Cruz', 'Felton', 'Pleasure Point', 'Lodi', 'Berkeley' ]\n",
    "\n",
    "\n",
    "# clean city names data:\n",
    "df_no_city_nulls = clean_split_city_names(df_no_city_nulls, address_criteria, neighborhood_criteria, split_city_delimiters, incorrect_city_names, cities_not_in_region, cities_that_need_extra_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Next, we need to determine each unique city name for each sfbay subregion.\n",
    "First, remove all city values listed as 'nan', which resulted from the previous data wrangling step--ie,  .\n",
    "Then, return this set of unique city names as a Series (new col), and convert to a flattened array (ie, Python list)\"\"\"\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "# determine all unique (non-null) city names--ie, using df_no_city_nulls\n",
    "def determine_unique_col_vals(df, col):\n",
    "    return df[col].unique()\n",
    "\n",
    "# return unique city names\n",
    "determine_unique_col_vals(df_no_city_nulls, 'cities') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brentwood',\n",
       " 'vallejo',\n",
       " 'hayward',\n",
       " 'concord',\n",
       " 'westbrae',\n",
       " 'elmwood',\n",
       " 'hercules',\n",
       " 'oakland',\n",
       " 'berkeley',\n",
       " 'fremont',\n",
       " 'danville',\n",
       " 'alameda',\n",
       " 'walnut creek',\n",
       " 'san leandro',\n",
       " 'dublin',\n",
       " 'san lorenzo',\n",
       " 'pittsburg',\n",
       " 'richmond',\n",
       " 'livermore',\n",
       " 'emeryville',\n",
       " 'albany',\n",
       " 'lafayette',\n",
       " 'fairfield',\n",
       " 'san jose',\n",
       " 'nan',\n",
       " 'san ramon',\n",
       " 'benicia',\n",
       " 'irvington high area',\n",
       " 'crockett',\n",
       " 'el sobrante',\n",
       " 'pleasanton',\n",
       " 'tracy',\n",
       " 'stockton',\n",
       " 'castro valley',\n",
       " 'san pablo',\n",
       " 'pleasant hill',\n",
       " 'nevada city',\n",
       " 'midtown sacramento',\n",
       " 'san francisco',\n",
       " 'neighborhood',\n",
       " 'east palo alto',\n",
       " 'oakley',\n",
       " 'bay point',\n",
       " 's francisco way',\n",
       " 'san jose ca',\n",
       " 'san mateo',\n",
       " 'laurence ranch neighborhood',\n",
       " 'dimond district',\n",
       " 'mcarthur',\n",
       " 'santa clara',\n",
       " 'metrosix',\n",
       " 'antioch',\n",
       " 'rockridge',\n",
       " 'pacifica',\n",
       " 'rotary way vallejo',\n",
       " 'discovery bay ca',\n",
       " 'sunnyvale',\n",
       " 'union city',\n",
       " 'santa rosa',\n",
       " 'moraga',\n",
       " 'montclair',\n",
       " 'niles',\n",
       " 'santa fe',\n",
       " 'old city vallejo',\n",
       " 'bushrod',\n",
       " 'el cerrito',\n",
       " 'american canyon',\n",
       " 'newark',\n",
       " 'bethel island',\n",
       " 'cleveland heights',\n",
       " 'sausalito',\n",
       " 'upper dimond',\n",
       " 'martinez',\n",
       " 'pinole',\n",
       " 'tracy hills',\n",
       " 'temescal',\n",
       " 'mt. hamilton',\n",
       " 'down town san jose',\n",
       " 'los gatos',\n",
       " 'piedmont',\n",
       " 'laurel district',\n",
       " 'dixon',\n",
       " 'alamo',\n",
       " 'not berkeley',\n",
       " 'side',\n",
       " 'lakeside',\n",
       " 'rodeo',\n",
       " 'mountain house ca',\n",
       " 'patton',\n",
       " 'berkeley- temscal',\n",
       " \". vincent's hill\",\n",
       " 'near lodi or near walnut grove',\n",
       " 'temescal bushrod',\n",
       " 'santa cruz area',\n",
       " 'port costa',\n",
       " 'glenview',\n",
       " 'galen pl fremont',\n",
       " 'hiddenbrooke',\n",
       " 'watergate by the bay in emeryville',\n",
       " '. fremont',\n",
       " 'pla vada woodland',\n",
       " 'clayton valley',\n",
       " 'fort washington & millbrook',\n",
       " 'suisun',\n",
       " 'west sacramento',\n",
       " 'near walnut grove or lodi',\n",
       " 'roseville',\n",
       " 'mountain house',\n",
       " 'el cerrito ca',\n",
       " 'sierra foothills',\n",
       " 'ambar pl fremont',\n",
       " 'ashby',\n",
       " '. tomas',\n",
       " 'garden creek pl danville',\n",
       " 'colville pl fremont',\n",
       " '------ambar pl fremont',\n",
       " 'tracy glencreek neighborhood',\n",
       " 'keats cir pleasant hill',\n",
       " 'hayes valley',\n",
       " 'ockton',\n",
       " 'southwest berkeley',\n",
       " 'shasta ct pleasanton',\n",
       " 'walnut grove',\n",
       " 'suisun city',\n",
       " 'discovery bay',\n",
       " 'south berkeley',\n",
       " 'highland hospital',\n",
       " 'garden creek circle',\n",
       " 'hilltop',\n",
       " 'sacramento',\n",
       " 'tahoma',\n",
       " 'hiller highlands',\n",
       " 'west okland',\n",
       " 'ivy hill',\n",
       " 'rio vista',\n",
       " 'dobson way',\n",
       " 'spring town livermore',\n",
       " 'lathrop',\n",
       " '--brookside',\n",
       " '--city of lathrop',\n",
       " 'modesto',\n",
       " 'glen ellen',\n",
       " 'manteca',\n",
       " 'hunter ranch',\n",
       " 'tiburon',\n",
       " 'orinda',\n",
       " 'hogan ct apt',\n",
       " 'oaklan d',\n",
       " 'milpitas',\n",
       " 'napa',\n",
       " 'highland park',\n",
       " 'mill valley',\n",
       " 'sonoma',\n",
       " 'fairfax',\n",
       " 'kentfield',\n",
       " 'san rafael',\n",
       " 'novato',\n",
       " 'sebastopol',\n",
       " 'healdsburg',\n",
       " 'petaluma',\n",
       " 'greenbrae',\n",
       " 'rohnert park',\n",
       " 'san anselmo',\n",
       " 'russian river',\n",
       " 'windsor',\n",
       " 'middletown',\n",
       " 'guerneville',\n",
       " 'corte madera',\n",
       " 'west marin',\n",
       " 'rio nido',\n",
       " 'cloverdale',\n",
       " 'sonoma valley',\n",
       " 'monte rio',\n",
       " 'larkspur',\n",
       " 'penngrove',\n",
       " 'occidental',\n",
       " 'morgan hill',\n",
       " 'point reyes',\n",
       " 'bodega bay',\n",
       " 'forest knolls',\n",
       " 'forestville',\n",
       " 'cupertino',\n",
       " 'near mount lassen',\n",
       " 'campbell',\n",
       " 'west petaluma',\n",
       " 'colonial ln',\n",
       " 'helena',\n",
       " 'west end san rafael',\n",
       " 'lincoln hill- san rafael',\n",
       " 'redwood valley',\n",
       " 'saint helena',\n",
       " 'woodacre',\n",
       " 'y creek',\n",
       " 'cotati',\n",
       " 'santa rosa wikiup',\n",
       " 'belvedere island',\n",
       " 'marin county',\n",
       " 'shadow lane santa rosa',\n",
       " '. san rafael',\n",
       " 'vallejo mare island',\n",
       " 'west county',\n",
       " 'belvedere tiburon',\n",
       " 'kenwood',\n",
       " 'santa rosa east',\n",
       " 'all',\n",
       " 'gregory ln',\n",
       " 'nantucket pl fairfield',\n",
       " 'kelseyville',\n",
       " 'vallejo midtown',\n",
       " 'marina bay',\n",
       " 'r san mateo',\n",
       " 'alamo square',\n",
       " 'geyserville',\n",
       " 'golf course',\n",
       " 'felton',\n",
       " 'redwood city',\n",
       " 'p meeker',\n",
       " 'r fairfield',\n",
       " 'san bruno',\n",
       " 'iftwood cir fairfield ca',\n",
       " 'fairfield rancho solano golf couse',\n",
       " 'fairfield ca',\n",
       " 'soma',\n",
       " 'channing way san rafael',\n",
       " 'clay bank',\n",
       " 'yountville',\n",
       " 'udio in west santa rosa',\n",
       " 'kahului',\n",
       " 'villa cir',\n",
       " 'fairfield west side',\n",
       " 'puerto vallarta',\n",
       " 'lakehead',\n",
       " 'palo alto',\n",
       " 'foretville',\n",
       " 'barlow ct fairfield',\n",
       " 'rincon valley',\n",
       " 'nw petaluma',\n",
       " 'mendocino',\n",
       " 'el verano',\n",
       " 'alameda near gold coast',\n",
       " 'pt richmond',\n",
       " 'lindberg ln petaluma',\n",
       " 'seaside',\n",
       " 'vallejo heritage district',\n",
       " 'travion court fairfield',\n",
       " 'santa rosa jc neighborhood',\n",
       " 'boyd park',\n",
       " 'cobb',\n",
       " 'historic neighborhood',\n",
       " 'bolinas',\n",
       " 'lakeport',\n",
       " 'istoga',\n",
       " 'forestvlle',\n",
       " 'graton',\n",
       " 'auburn',\n",
       " 'sebastopol`',\n",
       " 'bicentennial way santa rosa',\n",
       " '******* santa rosa******',\n",
       " 'gerstle park',\n",
       " 'inson beach',\n",
       " 'old redwood highway santa rosa',\n",
       " 'hollister',\n",
       " 'between guerneville & forestville',\n",
       " 'ukiah',\n",
       " 'west sonoma county',\n",
       " 'nice ca',\n",
       " 'lantana pi fairfield',\n",
       " 'chesley ct',\n",
       " 'rio robles',\n",
       " 'hidden valley lake',\n",
       " 'gualala',\n",
       " 'kelsey knolls santa rosa',\n",
       " 'gershwin cir fairfield',\n",
       " 'clear lake oaks',\n",
       " 'iftwood cir',\n",
       " 'foster city',\n",
       " 'santa ynez valley buellton solvang los olivos',\n",
       " 'circle oaks',\n",
       " 'bay village circle santa rosa',\n",
       " 'tiara community',\n",
       " 'eastsound',\n",
       " 'san anselmo border',\n",
       " 'grass valley',\n",
       " 'davis ca',\n",
       " 'bennett valley',\n",
       " 'old courthouse square',\n",
       " \"santa rosa's sonoma garden apartments\",\n",
       " 'wood hollow crk & laurel creek park',\n",
       " 'cool',\n",
       " 'summerville',\n",
       " 'st. helena',\n",
       " 'burlingame',\n",
       " 'oceanview',\n",
       " 'woodside',\n",
       " 'mountain view',\n",
       " 'millbrae',\n",
       " 'daly city',\n",
       " 'brisbane',\n",
       " 'menlo park',\n",
       " 'south san francisco',\n",
       " 'belmont',\n",
       " 'los altos',\n",
       " 'san carlos',\n",
       " 'portola valley',\n",
       " 'inner sunset',\n",
       " 'half moon bay',\n",
       " 'atherton',\n",
       " 'hillsborough',\n",
       " \"mariner's island\",\n",
       " 'coastside',\n",
       " 'la honda',\n",
       " 'gilroy',\n",
       " 'saratoga',\n",
       " 'santa cruz',\n",
       " 'watsonville',\n",
       " 'aptos',\n",
       " 'corralitos',\n",
       " 'soquel',\n",
       " 'boulder creek',\n",
       " 'pleasure point',\n",
       " 'montara',\n",
       " 'bahama ln',\n",
       " 'moss beach',\n",
       " 'dillon beach',\n",
       " 'meadow ridge cir',\n",
       " 'central san mateo',\n",
       " 'onestown',\n",
       " 'francis',\n",
       " 'daly city near serramonte shopping center',\n",
       " '-',\n",
       " 'hilbar ln',\n",
       " 'arnold',\n",
       " 'hillborough',\n",
       " 's. delaware san mateo',\n",
       " 'in-law unit with office space',\n",
       " 'truckee',\n",
       " 'barron park',\n",
       " 'sf',\n",
       " 'emerald hills',\n",
       " '. foster city',\n",
       " 'la honda hills',\n",
       " 'palto alto',\n",
       " 'peninsula',\n",
       " 'burlingame___ca',\n",
       " 'westborough',\n",
       " 'san mateo hayward park',\n",
       " 'best area of daly city',\n",
       " 'sonoma healdsburg wine country',\n",
       " 'city of',\n",
       " 'hillsdale',\n",
       " 'glenloch way',\n",
       " 'pescadero',\n",
       " 'kings beach',\n",
       " 'larkspur ca',\n",
       " 'millbrae ca',\n",
       " 'armstrong tract',\n",
       " 'los banos',\n",
       " 'pioneer',\n",
       " 'la honda-san gregorio',\n",
       " 'portola valley schools',\n",
       " 'ladera portola valley',\n",
       " 'off mission in hayward',\n",
       " 'lexington way burlingame',\n",
       " 'el granada',\n",
       " 'portola valley ladera',\n",
       " 'salinas',\n",
       " 'brian',\n",
       " 'sebastian',\n",
       " 'apt',\n",
       " 'alpine meadows',\n",
       " 'marina',\n",
       " 'redwood city ca',\n",
       " 'oakland ca',\n",
       " 'montclair hill ln',\n",
       " 'ceres',\n",
       " 'sonora',\n",
       " 'gustine',\n",
       " 'prunedale',\n",
       " 'south fremont',\n",
       " 'fremont mission district',\n",
       " 'saratoga-brookview',\n",
       " 'blossom valley',\n",
       " 'greenwich cir. fremont',\n",
       " 'japantown',\n",
       " 'lake merritt',\n",
       " 'e remington sunnyvale',\n",
       " 'almaden valley',\n",
       " 'south beach',\n",
       " 'san los ca',\n",
       " 'graeagle',\n",
       " 'santa monica',\n",
       " 'rose garden',\n",
       " 'burbank',\n",
       " 'shasta-hanchett park',\n",
       " 'soledad',\n",
       " 'eron park',\n",
       " 'east sanjose',\n",
       " 'santa teresa',\n",
       " 'los altos hills',\n",
       " 'tully',\n",
       " 'tully and mclaughlin',\n",
       " 'san jos',\n",
       " 'rose garden area',\n",
       " 'essex junction',\n",
       " 'berryessa near milpitas',\n",
       " 'tro valley-',\n",
       " 'compton cir unit c',\n",
       " 'quail hollow',\n",
       " 'turlock',\n",
       " 'berryessa',\n",
       " 'burlingame ca',\n",
       " 'san mateo ca',\n",
       " 'gomes ct',\n",
       " 'valley to the beach',\n",
       " 'tenderloin',\n",
       " 'brian park',\n",
       " 'civic center',\n",
       " \"it's silicon valley lowest price\",\n",
       " \"{it's silicon valley lowest price}\",\n",
       " \"cupertino {it's silicon valley lowest price}\",\n",
       " \"cupertino[it's silicon valley lowest price]\",\n",
       " \"cupertino [it's silicon valley lowest price]\",\n",
       " 'pbell area',\n",
       " 'silver creek',\n",
       " 'santana row',\n",
       " 'los gatos mountains',\n",
       " 'pacific heights',\n",
       " 'west pbell',\n",
       " '___ morgan hill',\n",
       " 'los banos ca',\n",
       " 'oakland area',\n",
       " 'bonny doon',\n",
       " 'la selva beach',\n",
       " 'brookdale',\n",
       " 'ben lomond',\n",
       " 'santa cruz ca',\n",
       " 'santa cruz mountains',\n",
       " 'boulder creek near',\n",
       " 'interlaken-watsonville',\n",
       " 'pleasure point neighborhood',\n",
       " 'highway',\n",
       " 'south felton',\n",
       " 'area in the santa cruz mountains',\n",
       " 'pleasure point area',\n",
       " 'wilkes cir',\n",
       " 'aromas',\n",
       " 'pleasure point adjacent',\n",
       " 'felton ca',\n",
       " 'near pleasure point',\n",
       " 'santa cruz columbia beach',\n",
       " 'safe and peaceful setting in felton',\n",
       " 'felton-lompico',\n",
       " 'santa cruz -live oak']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, convert the set of city names data to a list:\n",
    "\n",
    "\n",
    "# convert numpy array to list\n",
    "def transform_np_array_to_list(np_array):\n",
    "    return np_array.tolist() \n",
    "\n",
    "unique_city_names = determine_unique_col_vals(df_no_city_nulls, 'cities')\n",
    "\n",
    "unique_city_names_lis = transform_np_array_to_list(unique_city_names)\n",
    "\n",
    "# apply lower-case to each character of each city name for sake of consistency\n",
    "def each_list_el_to_lowercase(list_arg):\n",
    "    return [char.lower() for char in list_arg]\n",
    "\n",
    "unique_city_names_lis = each_list_el_to_lowercase(unique_city_names_lis)\n",
    "\n",
    "# sanity check\n",
    "unique_city_names_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brentwood-',\n",
       " 'vallejo-',\n",
       " 'hayward',\n",
       " 'concord-',\n",
       " 'westbrae',\n",
       " 'elmwood',\n",
       " 'hercules',\n",
       " 'oakland',\n",
       " 'berkeley--',\n",
       " 'fremont-',\n",
       " 'danville-',\n",
       " 'alameda',\n",
       " 'berkeley',\n",
       " 'walnut-creek',\n",
       " 'san-leandro',\n",
       " 'dublin-',\n",
       " 'san-lorenzo',\n",
       " 'pittsburg-',\n",
       " 'richmond',\n",
       " 'livermore',\n",
       " 'emeryville',\n",
       " 'albany-',\n",
       " 'lafayette-',\n",
       " 'fairfield-',\n",
       " 'east-san-jose',\n",
       " 'west-end',\n",
       " 'nan',\n",
       " 'san-ramon',\n",
       " 'dublin',\n",
       " 'lafayette',\n",
       " 'briarwood-at-central-park',\n",
       " 'benicia',\n",
       " 'fremont',\n",
       " 'concord',\n",
       " 'irvington-high-area',\n",
       " 'crockett',\n",
       " '',\n",
       " 'vallejo',\n",
       " 'el-sobrante',\n",
       " 'pleasanton',\n",
       " 'tracy',\n",
       " 'ockton-ca',\n",
       " '54',\n",
       " 'tro-valley',\n",
       " 'san-pablo',\n",
       " 'pleasant-hill',\n",
       " 'pittsburg',\n",
       " 'brentwood',\n",
       " 'nevada-city',\n",
       " 'midtown-sacramento',\n",
       " 'san-francisco',\n",
       " 'neighborhood',\n",
       " 'east-palo-alto',\n",
       " 'san-jose',\n",
       " 'oakley',\n",
       " 'bay-point',\n",
       " '3569--san-jose',\n",
       " 's-francisco-way',\n",
       " 'san-jose-ca',\n",
       " 'san-mateo',\n",
       " 'danville',\n",
       " 'laurence-ranch-neighborhood',\n",
       " 'dimond-district',\n",
       " 'mcarthur',\n",
       " 'santa-clara',\n",
       " 'albany',\n",
       " 'metrosix55',\n",
       " 'antioch',\n",
       " 'rockridge',\n",
       " 'pacifica',\n",
       " '60-rotary-way-vallejo',\n",
       " 'discovery-bay-ca',\n",
       " 'sunnyvale',\n",
       " 'union-city',\n",
       " 'santa-rosa',\n",
       " 'moraga',\n",
       " 'montclair',\n",
       " 'niles',\n",
       " 'santa-fe',\n",
       " 'old-city-vallejo',\n",
       " 'bushrod',\n",
       " 'el-cerrito',\n",
       " 'american-yon',\n",
       " 'newark',\n",
       " 'bethel-island',\n",
       " 'cleveland-heights',\n",
       " 'sausalito',\n",
       " 'upper-dimond',\n",
       " 'martinez',\n",
       " 'pinole',\n",
       " 'tracy-hills',\n",
       " 'temescal',\n",
       " 'mt.-hamilton',\n",
       " 'down-town-san-jose',\n",
       " 'los-gatos',\n",
       " 'piedmont',\n",
       " 'laurel-district',\n",
       " 'dixon',\n",
       " 'fairfield',\n",
       " 'alamo',\n",
       " 'not-berkeley',\n",
       " 'side',\n",
       " 'lakeside',\n",
       " 'rodeo',\n",
       " 'mountain-house-ca',\n",
       " 'patton',\n",
       " 'temescal-',\n",
       " 'berkeley--temscal',\n",
       " \".-vincent's-hill\",\n",
       " 'near-lodi-or-near-walnut-grove',\n",
       " 'temescal-bushrod',\n",
       " 'santa-cruz-area',\n",
       " 'port-costa',\n",
       " 'glenview',\n",
       " 'galen-pl-fremont',\n",
       " 'hiddenbrooke',\n",
       " 'watergate-by-the-bay-in-emeryville',\n",
       " '.-fremont',\n",
       " 'pla-vada-woodland',\n",
       " 'clayton-valley',\n",
       " 'fort-washington-&-millbrook',\n",
       " 'suisun',\n",
       " 'west-sacramento',\n",
       " 'near-walnut-grove-or-lodi',\n",
       " 'roseville',\n",
       " 'mountain-house',\n",
       " 'el-cerrito-ca',\n",
       " 'sierra-foothills',\n",
       " 'ambar-pl-fremont',\n",
       " 'berkeley-',\n",
       " 'ashby',\n",
       " '.-tomas',\n",
       " 'garden-creek-pl-danville',\n",
       " 'colville-pl-fremont',\n",
       " '------ambar-pl-fremont',\n",
       " 'tracy-glencreek-neighborhood',\n",
       " 'keats-cir-pleasant-hill',\n",
       " 'hayes-valley',\n",
       " 'ockton',\n",
       " 'southwest-berkeley',\n",
       " 'shasta-ct-pleasanton',\n",
       " 'walnut-grove',\n",
       " 'suisun-city',\n",
       " 'discovery-bay',\n",
       " 'south-berkeley',\n",
       " 'highland-hospital',\n",
       " '2854-garden-creek-circle',\n",
       " 'hilltop',\n",
       " 'sacramento',\n",
       " 'tahoma',\n",
       " 'hiller-highlands',\n",
       " 'west-okland',\n",
       " 'ivy-hill-',\n",
       " 'rio-vista',\n",
       " 'dobson-way',\n",
       " 'spring-town-livermore',\n",
       " 'lathrop',\n",
       " '--brookside-',\n",
       " '--city-of-lathrop',\n",
       " 'modesto',\n",
       " 'glen-ellen',\n",
       " 'manteca',\n",
       " 'hunter-ranch',\n",
       " 'tiburon',\n",
       " 'orinda',\n",
       " '101-hogan-ct-apt',\n",
       " 'oaklan-d',\n",
       " 'milpitas',\n",
       " 'napa',\n",
       " 'highland-park',\n",
       " 'mill-valley',\n",
       " 'sonoma',\n",
       " 'fairfax',\n",
       " 'kentfield-',\n",
       " 'san-rafael',\n",
       " 'novato',\n",
       " 'sebastopol',\n",
       " 'healdsburg-',\n",
       " 'petaluma',\n",
       " 'greenbrae',\n",
       " 'rohnert-pk-',\n",
       " 'san-anselmo',\n",
       " 'russian-river',\n",
       " 'windsor',\n",
       " 'middletown',\n",
       " 'guerneville',\n",
       " 'tiburon-',\n",
       " 'corte-madera',\n",
       " 'west-marin',\n",
       " 'lake-county',\n",
       " 'rio-nido',\n",
       " 'cloverdale',\n",
       " 'sonoma-valley',\n",
       " 'monte-rio',\n",
       " 'larkspur',\n",
       " 'healdsburg',\n",
       " 'penngrove',\n",
       " 'occidental',\n",
       " 'morgan-hill',\n",
       " '.-rohnert-park',\n",
       " 'point-reyes',\n",
       " 'rohnert-park',\n",
       " 'bodega',\n",
       " 'forest-knolls',\n",
       " 'forestville',\n",
       " 'cupertino',\n",
       " 'udio-behind-main-house',\n",
       " 'near-mount-lassen',\n",
       " 'pbell',\n",
       " 'west-petaluma',\n",
       " 'colonial-ln',\n",
       " 'helena',\n",
       " 'west-end-san-rafael',\n",
       " 'lincoln-hill--san-rafael',\n",
       " 'redwood-valley',\n",
       " 'saint-helena',\n",
       " 'woodacre',\n",
       " 'y-creek',\n",
       " 'cotati',\n",
       " 'santa-rosa-wikiup',\n",
       " 'belvedere-island',\n",
       " 'marin-county',\n",
       " '4145-shadow-lane-santa-rosa',\n",
       " 'kentfield',\n",
       " '.-san-rafael',\n",
       " 'vallejo-mare-island',\n",
       " 'west-county',\n",
       " 'belvedere-tiburon',\n",
       " 'bodega-bay',\n",
       " 'kenwood',\n",
       " 'santa-rosa-east',\n",
       " 'all-',\n",
       " '423-gregory-ln-',\n",
       " 'nantucket-pl-fairfield',\n",
       " 'kelseyville',\n",
       " 'vallejo-midtown',\n",
       " 'marina-bay',\n",
       " 'r-san-mateo',\n",
       " 'alamo-square',\n",
       " 'geyserville',\n",
       " 'golf-course',\n",
       " 'felton',\n",
       " 'redwood-city',\n",
       " 'p-meeker',\n",
       " 'r-fairfield',\n",
       " 'san-bruno',\n",
       " 'iftwood-cir-fairfield-ca',\n",
       " 'fairfield-rancho-solano-golf-couse',\n",
       " 'fairfield-ca',\n",
       " 'soma',\n",
       " '300-channing-way-san-rafael',\n",
       " 'clay-bank',\n",
       " 'yountville',\n",
       " 'udio-in-west-santa-rosa',\n",
       " 'kahului',\n",
       " 'villa-cir-',\n",
       " 'fairfield-west-side',\n",
       " 'puerto-vallarta',\n",
       " 'lakehead',\n",
       " 'palo-alto',\n",
       " 'foretville',\n",
       " '73-barlow-ct-fairfield',\n",
       " 'rincon-valley',\n",
       " 'nw-petaluma',\n",
       " 'mendocino',\n",
       " 'el-verano',\n",
       " 'alameda-near-gold-coast',\n",
       " 'pt-richmond',\n",
       " 'lindberg-ln-petaluma',\n",
       " 'seaside',\n",
       " 'vallejo-heritage-district',\n",
       " 'vallejo-ca',\n",
       " '1600-travion-court-fairfield',\n",
       " 'santa-rosa-jc-neighborhood',\n",
       " 'boyd-park',\n",
       " 'cobb',\n",
       " 'historic-neighborhood',\n",
       " 'bolinas',\n",
       " 'lakeport',\n",
       " 'istoga',\n",
       " 'forestvlle',\n",
       " 'graton',\n",
       " 'auburn',\n",
       " 'sebastopol`',\n",
       " '200-bicentennial-way-santa-rosa',\n",
       " '*******-santa-rosa******',\n",
       " 'gerstle-park',\n",
       " 'inson-beach',\n",
       " '4450-old-redwood-highway-santa-rosa',\n",
       " 'hollister',\n",
       " 'between-guerneville-&-forestville',\n",
       " 'ukiah',\n",
       " 'west-sonoma-county',\n",
       " 'nice-ca',\n",
       " '719-lantana-pi-fairfield',\n",
       " '5-chesley-ct',\n",
       " 'rio-robles-',\n",
       " 'hidden-valley-lake',\n",
       " 'gualala',\n",
       " '3680-kelsey-knolls-santa-rosa',\n",
       " '1717-gershwin-cir-fairfield',\n",
       " 'clear-lake-oaks',\n",
       " 'iftwood-cir',\n",
       " 'foster-city',\n",
       " 'santa-ynez-valley-buellton-solvang-los-olivos',\n",
       " 'circle-oaks',\n",
       " '2980-bay-village-circle-santa-rosa',\n",
       " 'tiara-community',\n",
       " 'eastsound',\n",
       " 'san-anselmo-border',\n",
       " 'grass-valley',\n",
       " 'davis-ca',\n",
       " 'bennett-valley',\n",
       " 'old-courthouse-square',\n",
       " \"santa-rosa's-sonoma-garden-apartments\",\n",
       " 'wood-hollow-crk-&-laurel-creek-park',\n",
       " 'cool',\n",
       " 'hayward-',\n",
       " 'bend',\n",
       " 'summerville',\n",
       " 'ketchum',\n",
       " '.-helena',\n",
       " '.-helena-deer-park',\n",
       " 'burlingame',\n",
       " 'oceanview94132',\n",
       " 'woodside',\n",
       " 'mountain-view',\n",
       " 'millbrae',\n",
       " 'daly-city',\n",
       " 'brisbane',\n",
       " 'menlo-park',\n",
       " 'south-san-francisco',\n",
       " 'belmont',\n",
       " 'los-altos',\n",
       " 'san-los',\n",
       " 'redwood-shores',\n",
       " 'millbrae-on-the-burlingame-border',\n",
       " 'portola-valley',\n",
       " 'inner-sunset',\n",
       " 'half-moon-bay',\n",
       " 'atherton',\n",
       " 'hillsborough-ca',\n",
       " 'park-pacifica-neighborhood',\n",
       " 'south-sf',\n",
       " \"mariner's-island\",\n",
       " 'coastside',\n",
       " 'la-honda',\n",
       " 'gilroy',\n",
       " '.',\n",
       " 'saratoga',\n",
       " 'pbell-ca',\n",
       " 'santa-cruz',\n",
       " 'watsonville',\n",
       " 'aptos',\n",
       " 'corralitos',\n",
       " 'soquel',\n",
       " 'boulder-creek',\n",
       " 'pleasure-point',\n",
       " 'montara',\n",
       " 'bahama-ln',\n",
       " 'moss-beach',\n",
       " 'dillon-beach',\n",
       " 'meadow-ridge-cir',\n",
       " 'central-san-mateo',\n",
       " 'onestown',\n",
       " 'francis',\n",
       " 'daly-city-near-serramonte-shopping-center',\n",
       " '1333-1287-82',\n",
       " 'hilbar-ln',\n",
       " 'hillsborough',\n",
       " 'daly-city-',\n",
       " 'arnold',\n",
       " 'hillborough',\n",
       " '1650-s.-delaware-san-mateo',\n",
       " 'in-law-unit-with-office-space',\n",
       " 'truckee',\n",
       " 'millbrae-',\n",
       " 'barron-park',\n",
       " 'sf-',\n",
       " 'emerald-hills',\n",
       " '94301',\n",
       " '.-foster-city',\n",
       " 'la-honda-hills',\n",
       " 'palto-alto',\n",
       " 'peninsula',\n",
       " 'burlingame___ca',\n",
       " 'westborough',\n",
       " 'san-mateo-hayward-park',\n",
       " 'best-area-of-daly-city',\n",
       " 'sonoma-healdsburg-wine-country',\n",
       " 'city-of-',\n",
       " 'hillsdale-',\n",
       " '621-glenloch-way',\n",
       " 'pescadero',\n",
       " 'kings-beach',\n",
       " 'larkspur-ca',\n",
       " 'millbrae-ca',\n",
       " 'armstrong-tract',\n",
       " 'los-banos',\n",
       " '204-san-francisco',\n",
       " 'foster-city-',\n",
       " 'pioneer',\n",
       " 'la-honda-san-gregorio',\n",
       " 'portola-valley-schools-',\n",
       " 'ladera-portola-valley',\n",
       " 'off-mission-in-hayward',\n",
       " 'lexington-way-burlingame',\n",
       " 'el-granada',\n",
       " 'portola-valley-ladera',\n",
       " 'salinas',\n",
       " 'brian',\n",
       " 'fremont-ca',\n",
       " 'sebastian',\n",
       " '***---rohnert-park',\n",
       " 'apt-93',\n",
       " 'oakland-ca',\n",
       " '4722-montclair-hill-ln',\n",
       " 'ceres',\n",
       " 'sonora',\n",
       " 'gustine',\n",
       " 'prunedale',\n",
       " 'south-fremont',\n",
       " 'fremont-mission-district',\n",
       " 'saratoga-brookview',\n",
       " 'blossom-valley',\n",
       " '38747-greenwich-cir.-fremont',\n",
       " 'japantown',\n",
       " 'lake-merritt-',\n",
       " 'east-foothills',\n",
       " '150-e-remington-sunnyvale',\n",
       " 'almaden-valley',\n",
       " 'south-beach',\n",
       " 'san-los-ca',\n",
       " 'redwood-city-ca',\n",
       " 'graeagle',\n",
       " 'santa-monica',\n",
       " 'rose-garden',\n",
       " 'burbank',\n",
       " 'shasta-hanchett-park',\n",
       " 'soledad',\n",
       " 'eron-park',\n",
       " 'east-sanjose',\n",
       " 'santa-teresa-',\n",
       " 'los-altos-hills',\n",
       " 'tully',\n",
       " 'tully-and-mclaughlin',\n",
       " 'san-jos',\n",
       " 'rose-garden-area',\n",
       " 'essex-junction',\n",
       " 'berryessa-near-milpitas',\n",
       " 'tro-valley-',\n",
       " 'compton-cir-unit-c',\n",
       " 'quail-hollow',\n",
       " 'turlock',\n",
       " 'berryessa',\n",
       " 'burlingame-ca',\n",
       " 'san-mateo-ca',\n",
       " 'gomes-ct-',\n",
       " 'valley-to-the-beach',\n",
       " 'lake-merritt',\n",
       " 'tenderloin',\n",
       " 'brian-park',\n",
       " 'civic-center',\n",
       " \"it's-silicon-valley-lowest-price\",\n",
       " \"{it's-silicon-valley-lowest-price}\",\n",
       " \"cupertino-{it's-silicon-valley-lowest-price}\",\n",
       " \"cupertino[it's-silicon-valley-lowest-price]\",\n",
       " \"cupertino-[it's-silicon-valley-lowest-price]\",\n",
       " 'pbell-area',\n",
       " 'silver-creek',\n",
       " 'santana-row',\n",
       " 'los-gatos-mountains',\n",
       " 'pacific-heights',\n",
       " '1305-west-pbell',\n",
       " '___-morgan-hill',\n",
       " 'marina',\n",
       " 'los-banos-ca',\n",
       " 'sunnyvale-',\n",
       " 'oakland-area',\n",
       " 'richmond-',\n",
       " 'bonny-doon',\n",
       " 'la-selva-beach',\n",
       " 'brookdale',\n",
       " 'ben-lomond',\n",
       " 'boulder-creek-near-',\n",
       " 'interlaken-watsonville',\n",
       " 'corralitos-',\n",
       " 'pleasure-point-neighborhood',\n",
       " 'santa-cruz-mountains',\n",
       " '6440-highway-9',\n",
       " 'south-felton',\n",
       " 'area-in-the-santa-cruz-mountains',\n",
       " 'pleasure-point-area',\n",
       " '311-wilkes-cir',\n",
       " 'aromas',\n",
       " 'pleasure-point-adjacent',\n",
       " 'felton-ca',\n",
       " 'near-pleasure-point',\n",
       " 'santa-cruz-columbia-beach',\n",
       " 'safe-and-peaceful-setting-in-felton',\n",
       " 'felton-lompico',\n",
       " 'santa-cruz--live-oak']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Next, add dash delimiters *in between* each word (ie, in place of whitespace in between each word of each city name) for each element (read: city name) \n",
    "from the unique_city_names_lis list.  \n",
    "\n",
    "Why add a dash delimiter in b/w each word of each city name?:\n",
    "Because the rental listings' listing_urls URLs each contain--(as of craigslist's server's changes in Jan 2023)\n",
    "--the listing's city name in the URL. \n",
    "***But!: The city names in the URL are ***always*** listed with a dash delimiter in between each word!\"\"\"\n",
    "\n",
    "def add_dash_delimiter_in_bw_each_word_of_city_names(city_names:list):\n",
    "    return [word.replace(' ', '-') for word in city_names]  # use str.replace() method to replace whitespaces with dashes\n",
    "\n",
    "unique_city_names_lis_dash_delim = add_dash_delimiter_in_bw_each_word_of_city_names(unique_city_names_lis)\n",
    "\n",
    "# sanity check\n",
    "unique_city_names_lis_dash_delim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcrawler to get wikipedia table data on **all** SF Bay Area city names & SC county cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Identify a list of all unique SF Bay Area & SC county city names, and output to a list\n",
    "\n",
    "# 1a) Create a few simple webcrawlers to grab the city names data from 2 wikipedia tables\n",
    "\n",
    "#web crawling, web scraping & webdriver libraries and modules\n",
    "from selenium import webdriver  # NB: this is the main module we will use to implement the webcrawler and webscraping. A webdriver is an automated browser.\n",
    "from webdriver_manager.chrome import ChromeDriverManager # import webdriver_manager package to automatically take care of any needed updates to Chrome webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options  # Options enables us to tell Selenium to open WebDriver browsers using maximized mode, and we can also disable any extensions or infobars\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "### SF bay area city names data\n",
    "\n",
    "\n",
    "# sf bay area city names wiki page:\n",
    "sfbay_cities_wiki_url = 'https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_the_San_Francisco_Bay_Area'\n",
    "\n",
    "\n",
    "# access page, and grab city names, append to list\n",
    "\n",
    "def obtain_cities_from_wiki_sfbay(webpage_url,list_of_cities):\n",
    "    # initialize web driver\n",
    "            \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "    \n",
    "    # access webpage\n",
    "    driver.get(webpage_url)\n",
    "\n",
    "    xpaths_table = '//table[@class=\"wikitable plainrowheaders sortable jquery-tablesorter\"]'\n",
    "\n",
    "    # search for wiki data tables:\n",
    "    table = driver.find_element(By.XPATH, xpaths_table)\n",
    "\n",
    "\n",
    "    # print(f'Full table:\\n\\n{table.text}\\n\\n\\n\\n\\n')\n",
    "\n",
    "    # iterate over each table row and then row_val within each row to get data from the given table, pertaining to the city names\n",
    "    for row in table.find_elements(By.CSS_SELECTOR, 'tr'): # iterate over each row in the table\n",
    "        \n",
    "        \n",
    "        city_names =  row.find_elements(By.TAG_NAME, 'th')  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "        # city_names =  row.find_elements(By.TAG_NAME, 'td')[0]  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "\n",
    "        # extract text, but *skip* the first 2 rows of the table  rows' values since these are only the column names!\n",
    "        for city_name in city_names[:2]: # skip first 2 rows \n",
    "\n",
    "            # append the remaining data to list\n",
    "            list_of_cities.append(city_name.text)\n",
    "\n",
    "\n",
    "    # exit webpage \n",
    "    driver.close()\n",
    "\n",
    "    # # sanity check\n",
    "    # print(f'List of city names:\\n{list_of_cities}')\n",
    "\n",
    "    return list_of_cities\n",
    "\n",
    "\n",
    "\n",
    "# initialize lists:\n",
    "sfbay_city_names = []\n",
    "\n",
    "\n",
    "#sfbay data\n",
    "obtain_cities_from_wiki_sfbay(sfbay_cities_wiki_url, sfbay_city_names)\n",
    "\n",
    "# remove remaining col names:\n",
    "sfbay_city_names = sfbay_city_names[4:]\n",
    "\n",
    "# sanity check\n",
    "print(f'sfbay city names:{sfbay_city_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sc county city names data:\n",
    "\n",
    "#web crawling, web scraping & webdriver libraries and modules\n",
    "from selenium import webdriver  # NB: this is the main module we will use to implement the webcrawler and webscraping. A webdriver is an automated browser.\n",
    "from webdriver_manager.chrome import ChromeDriverManager # import webdriver_manager package to automatically take care of any needed updates to Chrome webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options  # Options enables us to tell Selenium to open WebDriver browsers using maximized mode, and we can also disable any extensions or infobars\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "# sc county wiki page url\n",
    "sc_county_cities_wiki_url = 'https://en.wikipedia.org/wiki/Santa_Cruz_County,_California#Population_ranking'\n",
    "\n",
    "\n",
    "sc_county_city_names = []\n",
    "\n",
    "\n",
    "def obtain_cities_from_wiki_sc(webpage_url,list_of_cities):\n",
    "    # initialize web driver\n",
    "            \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())  # install or update latest Chrome webdriver using using ChromeDriverManager() library\n",
    "    \n",
    "    # access webpage\n",
    "    driver.get(webpage_url)\n",
    "\n",
    "    # # NB!: there are 2 tables with the same class name; only select data from the 2nd one\n",
    "    # xpaths_table = '//table[@class=\"wikitable sortable jquery-tablesorter\"][2]'  # 2nd table on webpage with this class name\n",
    "\n",
    "\n",
    "    # # search for given wiki data tables:\n",
    "    # table = driver.find_element(By.XPATH, xpaths_table)\n",
    "\n",
    "    # NB!: there are 2 tables with the same class name; only select data from the 2nd one\n",
    "    xpaths_table = '//table[@class=\"wikitable sortable jquery-tablesorter\"][2]//tr//td[2]'  # 2nd table on webpage with this class name\n",
    "\n",
    "\n",
    "    # search for given wiki data tables:\n",
    "    table = driver.find_elements(By.XPATH, xpaths_table)\n",
    "\n",
    "\n",
    "    print(f'Full table:\\n\\n{table}\\n\\n\\n\\n\\n')\n",
    "\n",
    "    for row in table:\n",
    "        print(f'City names:{row.text}')\n",
    "        list_of_cities.append(row.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(f'Exclude 1st element of table:{table.text[1:]}')\n",
    "\n",
    "\n",
    "    # # grab data from the table body:\n",
    "    # table_body = table.find_elements(By.XPATH, '//*[@id=\"mw-content-text\"]/div[1]/table[13]/tbody//b[2]/a')\n",
    "\n",
    "    # # grab text from table body\n",
    "    # table_body_2nd_col = table_body\n",
    "\n",
    "    # print(f'Table body text\\n:{table_body_2nd_col}')\n",
    "\n",
    "    # iterate over each table row for *only* the 2nd column of the table body\n",
    "\n",
    "\n",
    "#     for row in table.find_elements(By.XPATH, 'tr'): # iterate over each row from the table body (ie, tbody)\n",
    "\n",
    "# # //*[@id=\"mw-content-text\"]/div[1]/table[13]/tbody//b[2]/a\n",
    "\n",
    "#         city_names =  row.text  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "\n",
    "#         print(city_names)\n",
    "        \n",
    "        # city_names =  row.find_elements(By.TAG_NAME, 'td')  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "        # city_names =  row.find_elements(By.TAG_NAME, 'td')[0]  # iterate over value of each row, *but* ONLY for the 1st column--ie, the 0th index\n",
    "\n",
    "        \n",
    "\n",
    "        # # extract text, but *skip* the first 2 rows of the table  rows' values since these are only the column names!\n",
    "        # # for city_name in city_names[1]: # skip first column\n",
    "\n",
    "        # #     append the remaining data to list\n",
    "        # #     list_of_cities.append(city_name.text)\n",
    "\n",
    "    \n",
    "        # list_of_cities = city_names\n",
    "\n",
    "    # exit webpage \n",
    "    driver.close()\n",
    "\n",
    "    # # sanity check\n",
    "    # print(f'List of city names:\\n{list_of_cities}')\n",
    "\n",
    "    return list_of_cities\n",
    "\n",
    "obtain_cities_from_wiki_sc(sc_county_cities_wiki_url, sc_county_city_names)\n",
    "\n",
    "\n",
    "#  # clean data by removing extraneous 'â€ ' char from city names list\n",
    "sc_county_city_names = list(map(lambda x: x.replace('â€ ',''), sc_county_city_names))\n",
    "\n",
    "## finally, remove any whitespace from list-- use list comprehension\n",
    "sc_county_city_names = [s for s in sc_county_city_names if s.strip()]\n",
    "\n",
    "# sanity check\n",
    "print(f'sc county city names:{sc_county_city_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Import data for January 1-Feb 14: ie, the data in which the city names are missing!\n",
    "\n",
    "### NB: We will need to use separate lists of dataframes for **Each** subregion, given the path structure of the scraped data derived from the webcrawler: \n",
    "\n",
    "## 2) Next, import data from Jan 15 to Feb 14, 2023\n",
    "\n",
    "### NB: check following stackoverflow for useful info on how to do this, but I will need to *add* and apply a separate *datetime filter*,  *or* use the glob library to import CSV files based on a sort of regex, to the files such that I **only**  import the right dates of data:\n",
    "\n",
    "### See following article on how to import CSV files only from specified date-range:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Import Data from mid-January 2023 to Feb 14, 2023: subset data for Jan 15, 2023 to February 14, 2023\n",
    "\n",
    "def recursively_import_CSV_files_since_date_range(parent_path, subregion:str, month_date_range:str, day_date_range:str):\n",
    "    \"\"\" Recusrively import all CSV files found in parent directory, but \n",
    "    *only* import files that match the specified date-range.\n",
    "\n",
    "    NB: month_date_range: specify a str comprising the range of months--ie, '01-02' for January to February\n",
    "    day_date_range: a str comprising the range of days: ie, '01-15' to mean the 1st through 15th \n",
    "    subregion: \n",
    "    \n",
    "    Return the CSV files as separate dataframes\n",
    "    within a list of dfs.\"\"\"\n",
    "    # specify parent path of the relevant (sfbay) scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes\n",
    "    path = parent_path\n",
    "\n",
    "    # import *only* CSV files that match specified *subregion* & *date-range*\n",
    "    csv_files_date_range = glob.glob(f'./craigslist_rental_sfbay_{[subregion]}_02_17_2023_{[month_date_range]}_{[day_date_range]}-*.csv')\n",
    "\n",
    "    # # NB: taken from stackexchange example: \n",
    "    # BANK\\BANK_NIFTY_5MINs_2020-03-01.csv'\n",
    "\n",
    "    # all_files = glob.glob('./BANK_NIFTY_5MINs_2020-[2-3]-*.csv')\n",
    "    # # see URL for more details: \n",
    "    # # https://stackoverflow.com/questions/74386583/how-to-select-specific-csv-files-for-specified-date-range-from-a-folder-in-pytho\n",
    "\n",
    "\n",
    "\n",
    "    # # specify an empty list to contain a list of dataframes\n",
    "    # list_of_dfs = []\n",
    "\n",
    "    # for files in csv_files_date_range:\n",
    "    #     # Iterate recursively over each CSV file from within specified date-range, and import as separate DataFrames:\n",
    "    #     dfs = pd.read_csv(files, \n",
    "    #                     sep=',',encoding = 'utf-8'  # use utf-8 encoding since it is OS-agnostic\n",
    "    #                     )\n",
    "        #  # append each imported df into the list of dfs\n",
    "\n",
    "        # list_of_dfs.append(dfs)\n",
    "\n",
    "\n",
    "    # use list comprehension to import all relevant dfs as a list of dfs:\n",
    "    list_of_dfs = [pd.read_csv(file, sep=',',encoding = 'utf-8') for file in csv_files_date_range]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # # # NB: for reference on how to import multiple CSVs as separate dfs within a list of dfs, see this example below:\n",
    "    # # Read multiple CSV files into separate DataFrames in Python\n",
    "    # # <https://www.geeksforgeeks.org/read-multiple-csv-files-into-separate-dataframes-in-python/>\n",
    "\n",
    "    # # append datasets into the list\n",
    "    # for i in range(len(list_of_names)):\n",
    "    #     temp_df = pd.read_csv(\"./csv/\"+list_of_names[i]+\".csv\")\n",
    "    #     dataframes_list.append(temp_df)\n",
    "\n",
    "    # # Nb: also see example below on another possible way of importing multiple dfs recursively:\n",
    "    # list_of_dfs = pd.read_csv(csv_files_date_range, # import each CSV file from directory\n",
    "    #                                     sep=',',encoding = 'utf-8'  # use utf-8 encoding since it is OS-agnostic\n",
    "    #                                     ) for csv_files_date_range in glob.iglob(  # iterate over each CSV file recursively, given specified date-range\n",
    "    #                                         os.path.join(path, '**', fn_regex), # have glob.iglob() search for *only* CSV files-- ie, '*.csv', & os.path.join helps ensure this concatenation is OS-independent\n",
    "    #                                         recursive=True), ignore_index=True)  \n",
    "\n",
    "\n",
    "    return list_of_dfs\n",
    "\n",
    "\n",
    "\n",
    "# import all Jan 1-Feb 14th data, by subregion\n",
    "\n",
    "# specify parent path of sfbay data\n",
    "parent_path_sfbay  = r'D:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\scraped_data\\sfbay'\n",
    "\n",
    "\n",
    "# specify the craigslist subregion codes for as a dictionary \n",
    "dict_of_subregion_codes = {'Peninsula':'pen', 'SF':'sfc', 'East Bay':'eby', 'South Bay':'sby', \n",
    "'Santa Cruz':'scz', 'North Bay':'nby'}\n",
    "\n",
    "# Peninsula_subregion = 'pen'\n",
    "# SF_subregion ='sfc'\n",
    "# ebay\n",
    "\n",
    "\n",
    "# specify months & dates (strings) over which we want to apply the datetime \"filter\"\n",
    "month_date_range_str = '01-02'\n",
    "\n",
    "day_date_range_str = '01-31'\n",
    "\n",
    "# Import Peninsula data:\n",
    "Peninsula_dfs_since_jan_2023 = recursively_import_CSV_files_since_date_range(parent_path_sfbay, dict_of_subregion_codes.get(\"Peninsula\"),\n",
    "month_date_range_str, day_date_range_str)\n",
    "\n",
    "# sanity check\n",
    "print(f'Pensinsula data list of dfs from Jan-Feb 14, 2023--1st week (df) from this date-range: {Peninsula_dfs_since_jan_2023[0].head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Import Data from mid-January 2023 to Feb 14, 2023: subset data for Jan 15, 2023 to February 14, 2023\n",
    "\n",
    "# imports-- file processing & datetime libraries\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "\n",
    "# specify subregion code\n",
    "subregion_code = 'sfc'\n",
    "\n",
    "# specify parent path of all sfbay data-- NB: use an f-string combined with a raw (ie, r) string--ie, fr to modify the string so we can input the subregion code as an argument to add to the path\n",
    "path = fr'D:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\scraped_data\\sfbay\\{subregion_code}'\n",
    "\n",
    "\n",
    "# get all CSV files from path, and grab the file stems for each given CSV file \n",
    "df = pd.DataFrame({'files' : [file for file in Path(path).glob('*.csv')],\n",
    "                  'file_stem' : [file.stem for file in Path(path).glob('*.csv')]}) # get file stem using .stem method\n",
    "\n",
    "# # Parse the dates from each CSV file, and keep the same 'MM_DD_YYY' format (**including the underscore delimiters!!), as the webcrawler CSV file naming convention:\n",
    "\n",
    "df['date_of_file'] = df['file_stem'].str.extract(r'(\\d{2}_\\d{2}_\\d{4})')\n",
    "\n",
    "\n",
    "## \"craigslist_rental_sfbay_subregion_MM_DD_YYYY.csv\"\n",
    "# sanity check\n",
    "print(f'File dates:\\n{df[[\"date_of_file\", \"file_stem\"]].sort_values(by=\"date_of_file\").tail()}')\n",
    "\n",
    "\n",
    "\n",
    "## ask user for the desired start & end dates, in format of 'MM_DD_YYYY'\n",
    "\n",
    "# start date inputs\n",
    "\n",
    "start_date_month = str(input('Enter desired Start Date month: '))\n",
    "start_date_day = str(input('Enter desired Start Date day: '))\n",
    "\n",
    "start_date_year = str(input('Enter desired Start Date year: '))\n",
    "\n",
    "# concat to single string, with **underscore** delimiters in between each component\n",
    "underscore_delimiter = '_'\n",
    "\n",
    "start_date = start_date_month + underscore_delimiter + start_date_day + underscore_delimiter + start_date_year\n",
    "\n",
    "# sanity check on  resulting str\n",
    "print(f'Start date for file filter:\\n{start_date}')\n",
    "\n",
    "# end date inputs\n",
    "end_date_month = str(input('Enter desired End Date month: '))\n",
    "end_date_day = str(input('Enter desired End Date day: '))\n",
    "\n",
    "end_date_year = str(input('Enter desired End Date year: '))\n",
    "\n",
    "# concat to single string, with **underscore** delimiters in between each component\n",
    "end_date = end_date_month + underscore_delimiter + end_date_day + underscore_delimiter + end_date_year\n",
    "\n",
    "\n",
    "# ## create a list of the start & end dates for filtering the files; then concatenate all matching files into a DataFrame\n",
    "# index the list of files, convert to a list of date values, to be used for the filter\n",
    "file_date_slice = df.set_index('date_of_file').loc[start_date:end_date]['files'].tolist()\n",
    "\n",
    "\n",
    "# # sanity check\n",
    "print(f'File dates we will use to filter the CSV files:\\n{file_date_slice}')\n",
    "\n",
    "\n",
    "# use list comprehension to import all relevant dfs as a list of dfs:\n",
    "list_of_dfs = [pd.read_csv(file, sep=',',encoding = 'utf-8') for file in file_date_slice]\n",
    "\n",
    "# # concat_df = pd.concat([pd.read_csv(file).compute() for file in file_date_slice])\n",
    "\n",
    "\n",
    "# print the first element in the list:\n",
    "print(f'First imported rental listings df from the period of {start_date} to {end_date} for {subregion_code} subregion, from the list of dfs:\\n{list_of_dfs[0]}')\n",
    "\n",
    "\n",
    "## NB: My webcrawler program's CSV files are of this format:\n",
    "## \"craigslist_rental_sfbay_subregion_MM_DD_YYYY.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish importing the list of dfs\n",
    "\n",
    "\n",
    "# SF data\n",
    "SF_dfs_since_jan_2023 = recursively_import_CSV_files_since_date_range(parent_path_sfbay, dict_of_subregion_codes.get(\"SF\"),\n",
    "month_date_range_str, day_date_range_str)\n",
    "\n",
    "# East Bay\n",
    "East_bay_dfs_since_jan_2023 = recursively_import_CSV_files_since_date_range(parent_path_sfbay, dict_of_subregion_codes.get(\"East Bay\"),\n",
    "month_date_range_str, day_date_range_str)\n",
    "\n",
    "# North Bay\n",
    "North_bay_dfs_since_jan_2023 = recursively_import_CSV_files_since_date_range(parent_path_sfbay, dict_of_subregion_codes.get(\"North Bay\"),\n",
    "month_date_range_str, day_date_range_str)\n",
    "\n",
    "# South Bay\n",
    "South_bay_dfs_since_jan_2023 = recursively_import_CSV_files_since_date_range(parent_path_sfbay, dict_of_subregion_codes.get(\"South Bay\"),\n",
    "month_date_range_str, day_date_range_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Import Data from mid-January 2023 to Feb 14, 2023: subset data for Jan 15, 2023 to February 14, 2023\n",
    "\n",
    "\n",
    "# # NB: the following is a possible path I could take whereby you use a regex to parse out certain files based on their dates\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# path = '\\tmp\\s3\\bucket\\files'\n",
    "\n",
    "# df = pd.DataFrame({'files' : [f for f in Path(path).glob('*.csv')],\n",
    "#                   'stem' : [f.stem for f in Path(path).glob('*.csv')]})\n",
    "\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['stem'].str.extract('(\\d{4}-\\d{2}-\\d{2})')[0])\n",
    "\n",
    "# print(df)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = r'D:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\scraped_data'\n",
    "\n",
    "df = pd.DataFrame({'files' : [f for f in Path(path).glob('*.csv')],\n",
    "                  'stem' : [f.stem for f in Path(path).glob('*.csv')]})\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['stem'].str.extract('(\\d{4}-\\d{2}-\\d{2})')[0])\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "## ask user for the desired start & end dates, in format of 'MM_DD_YYYY'\n",
    "# start_date = str(input('Enter desired Start Date: '))\n",
    "# end_date = str(input('Enter desired End Date: '))\n",
    "\n",
    "\n",
    "## create a list of the start & end dates for filtering the files; then concatenate all matching files into a DataFrame\n",
    "\n",
    "# file_date_slice = df.set_index('date').loc[start_date:end_date]['files'].tolist()\n",
    "\n",
    "\n",
    "# concat_df = pd.concat([pd.read_csv(file).compute() for file in file_date_slice])\n",
    "\n",
    "\n",
    "## NB: My webcrawler program's CSV files are of this format:\n",
    "## \"craigslist_rental_sfbay_subregion_MM_DD_YYYY.csv\"\n",
    "\n",
    "\n",
    "\n",
    "def recursively_import_CSV_files_since_date_range(parent_path, region: str, subregion:str, month_date_range:str, \n",
    "day_date_range:str):\n",
    "\n",
    "    \"\"\" Recusrively import all CSV files found in parent directory, but \n",
    "    *only* import files that match the specified date-range.\n",
    "\n",
    "    NB: month_date_range: specify a str comprising the range of months--ie, '01-02' for January to February\n",
    "    day_date_range: a str comprising the range of days: ie, '01-15' to mean the 1st through 15th \n",
    "    subregion: \n",
    "    \n",
    "    Return the CSV files as separate dataframes\n",
    "    within a list of dfs.\"\"\"\n",
    "\n",
    "    # specify parent path of the relevant (sfbay) scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes\n",
    "    path = parent_path\n",
    "\n",
    "    # get files from only the specified subregion\n",
    "    subregion_full_path = glob.glob(path + subregion)\n",
    "\n",
    "    # # import *only* CSV files that match specified *subregion* & *date-range*\n",
    "    # # csv_files_date_range = glob.glob(f'./craigslist_rental_sfbay_{[subregion]}_[1-2]_*.csv')\n",
    "    # csv_files_date_range = glob.glob(parent_path +  \"*.csv\")\n",
    "    \n",
    "\n",
    "    # sanity check on CSV files:\n",
    "    print(f'List of all files from given subregion:\\n{subregion_full_path}')\n",
    "\n",
    "\n",
    "    # # NB: taken from stackexchange example: \n",
    "    # BANK\\BANK_NIFTY_5MINs_2020-03-01.csv'\n",
    "\n",
    "    # all_files = glob.glob('./BANK_NIFTY_5MINs_2020-[2-3]-*.csv')\n",
    "    # # see URL for more details: \n",
    "    # # https://stackoverflow.com/questions/74386583/how-to-select-specific-csv-files-for-specified-date-range-from-a-folder-in-pytho\n",
    "\n",
    "\n",
    "\n",
    "    # # specify an empty list to contain a list of dataframes\n",
    "    # list_of_dfs = []\n",
    "\n",
    "    # for files in csv_files_date_range:\n",
    "    #     # Iterate recursively over each CSV file from within specified date-range, and import as separate DataFrames:\n",
    "    #     dfs = pd.read_csv(files, \n",
    "    #                     sep=',',encoding = 'utf-8'  # use utf-8 encoding since it is OS-agnostic\n",
    "    #                     )\n",
    "        #  # append each imported df into the list of dfs\n",
    "\n",
    "        # list_of_dfs.append(dfs)\n",
    "\n",
    "\n",
    "    # use list comprehension to import all relevant dfs as a list of dfs:\n",
    "    list_of_dfs = [pd.read_csv(file, sep=',',encoding = 'utf-8') for file in csv_files_date_range]\n",
    "\n",
    "    # print the first element in the list:\n",
    "    print(f'First imported rental listings df from {month_date_range} for {subregion} subregion, from the list of dfs:\\n{list_of_dfs}')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # # # NB: for reference on how to import multiple CSVs as separate dfs within a list of dfs, see this example below:\n",
    "    # # Read multiple CSV files into separate DataFrames in Python\n",
    "    # # <https://www.geeksforgeeks.org/read-multiple-csv-files-into-separate-dataframes-in-python/>\n",
    "\n",
    "    # # append datasets into the list\n",
    "    # for i in range(len(list_of_names)):\n",
    "    #     temp_df = pd.read_csv(\"./csv/\"+list_of_names[i]+\".csv\")\n",
    "    #     dataframes_list.append(temp_df)\n",
    "\n",
    "    # # Nb: also see example below on another possible way of importing multiple dfs recursively:\n",
    "    # list_of_dfs = pd.read_csv(csv_files_date_range, # import each CSV file from directory\n",
    "    #                                     sep=',',encoding = 'utf-8'  # use utf-8 encoding since it is OS-agnostic\n",
    "    #                                     ) for csv_files_date_range in glob.iglob(  # iterate over each CSV file recursively, given specified date-range\n",
    "    #                                         os.path.join(path, '**', fn_regex), # have glob.iglob() search for *only* CSV files-- ie, '*.csv', & os.path.join helps ensure this concatenation is OS-independent\n",
    "    #                                         recursive=True), ignore_index=True)  \n",
    "\n",
    "\n",
    "    return list_of_dfs\n",
    "\n",
    "\n",
    "\n",
    "# import all Jan 1-Feb 14th data, by subregion\n",
    "\n",
    "# specify parent path of sfbay data\n",
    "parent_path_sfbay  = r'D:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\scraped_data'\n",
    "\n",
    "\n",
    "# specify the craigslist subregion codes for as a dictionary \n",
    "dict_of_subregion_codes = {'Peninsula':'pen', \n",
    "                        'SF':'sfc', 'East Bay':'eby', 'South Bay':'sby', \n",
    "                        'Santa Cruz':'scz', 'North Bay':'nby'\n",
    "                        }\n",
    "\n",
    "# Peninsula_subregion = 'pen'\n",
    "# SF_subregion ='sfc'\n",
    "# ebay\n",
    "\n",
    "\n",
    "# specify months & dates (strings) over which we want to apply the datetime \"filter\"\n",
    "month_date_range_str = '01'\n",
    "\n",
    "day_date_range_str = '01-31'\n",
    "\n",
    "# specify (parent) region\n",
    "region = 'sfbay' \n",
    "\n",
    "# Import Peninsula data:\n",
    "Peninsula_dfs_since_jan_2023 = recursively_import_CSV_files_since_date_range(parent_path_sfbay, region, dict_of_subregion_codes.get(\"Peninsula\"),\n",
    "month_date_range_str, day_date_range_str)\n",
    "\n",
    "# sanity check\n",
    "print(f'Pensinsula data list of dfs from Jan-Feb 14, 2023--1st week (df) from this date-range: {Peninsula_dfs_since_jan_2023[0].head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_df_since_given_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KEVINA~1\\AppData\\Local\\Temp/ipykernel_22068/4133169455.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# apply datetime filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf_since_jan_2023\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_df_since_given_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date_posted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_range_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_range_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# sanity check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filter_df_since_given_date' is not defined"
     ]
    }
   ],
   "source": [
    "# filter since date function \n",
    "def filter_df_to_date_range(df, datetime_col_for_filter, date_range_start, date_range_end):\n",
    "    \"\"\" Filter data to correspond to be within a specified range of dates.\"\"\"\n",
    "    return df[(df[datetime_col_for_filter] >= date_range_start)&(df[datetime_col_for_filter]<= date_range_end)]\n",
    "\n",
    "# specify target start date for date-range filter\n",
    "date_range_start = '2023-01-15' # start date for date range\n",
    "\n",
    "# end date\n",
    "date_range_end = '2023-02-14'\n",
    "\n",
    "# apply datetime filter\n",
    "df_since_jan_2023 = filter_df_since_given_date(df, 'date_posted', date_range_start, date_range_end)\n",
    "\n",
    "# sanity check\n",
    "df_since_jan_2023.sort_values(by=['date_posted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "nothing to repeat at position 3616",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KEVINA~1\\AppData\\Local\\Temp/ipykernel_348/4191139066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mdf_since_jan_2023_cpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_since_jan_2023\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdf_since_jan_2023_cpy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cities'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_city_names_from_listing_URL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_since_jan_2023_cpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_city_names_lis_dash_delim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mdf_since_jan_2023_cpy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KEVINA~1\\AppData\\Local\\Temp/ipykernel_348/4191139066.py\u001b[0m in \u001b[0;36mparse_city_names_from_listing_URL\u001b[1;34m(df, unique_city_names_dash_delim)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstr_search_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_city_names_dash_delim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# now, use str.contains() to search for matching city names from rental listing URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdF_filter\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'listing_urls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_search_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# match city names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdF_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 )\n\u001b[0;32m    115\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36mcontains\u001b[1;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \"\"\"\n\u001b[1;32m-> 1153\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m             warnings.warn(\n\u001b[0;32m   1155\u001b[0m                 \u001b[1;34m\"This pattern has match groups. To actually get the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kevin allen\\appdata\\local\\programs\\python\\python37-32\\lib\\re.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kevin allen\\appdata\\local\\programs\\python\\python37-32\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kevin allen\\appdata\\local\\programs\\python\\python37-32\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kevin allen\\appdata\\local\\programs\\python\\python37-32\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, pattern)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kevin allen\\appdata\\local\\programs\\python\\python37-32\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[1;32m--> 426\u001b[1;33m                            not nested and not items))\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kevin allen\\appdata\\local\\programs\\python\\python37-32\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mAT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                 raise source.error(\"nothing to repeat\",\n\u001b[1;32m--> 651\u001b[1;33m                                    source.tell() - here + len(this))\n\u001b[0m\u001b[0;32m    652\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_REPEATCODES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m                 raise source.error(\"multiple repeat\",\n",
      "\u001b[1;31merror\u001b[0m: nothing to repeat at position 3616"
     ]
    }
   ],
   "source": [
    "\"\"\" Finally, parse the city names data for all of the data since mid-Jan 2023,\n",
    "which are currently missing city names data\"\"\"\n",
    "\n",
    "def parse_city_names_from_listing_URL(df, unique_city_names_dash_delim:list):\n",
    "    \"\"\" 1) Use str.contains() method chained to a .join() method in which we perform an 'OR' boolean via\n",
    "    the pipe (ie, '|' operator--ie, so we can search for multiple substrings (ie, each element \n",
    "    from the list arg) to look up any matching instances of city names\n",
    "    from the unique_city_names... list \n",
    "    relative to the rental listing URLs (ie, listing_urls).\n",
    "\n",
    "    2) Then, parse each such first city name by taking the first matched city name only,\n",
    "    \n",
    "    3) Use these parsed city name values to **replace** the values for the 'cities' column!\"\"\"\n",
    "    #1) identify any matching city names based on listing URLs\n",
    "    # use .join() method to add boolean \"OR\" pipe operators to each element from unique_city_names... list\n",
    "    str_search_pattern = '|'.join(unique_city_names_dash_delim) \n",
    "    # # now, use str.contains() to search for matching city names from rental listing URLs\n",
    "    # dF_filter =  df[df['listing_urls'].str.contains(str_search_pattern)] # match city names\n",
    "    # return dF_filter\n",
    "\n",
    "    ### NB!!: see useful stackoverlow discussion re: how to updaate one column based on whether *another* col contains a substring: \n",
    "    # \"Pandas: Updating Column B value if A contains string\": <https://stackoverflow.com/questions/69639237/pandas-updating-column-b-value-if-a-contains-string>\n",
    "\n",
    "    ## Also see\n",
    "\n",
    "\n",
    "    # # 2) parse the first city name only\n",
    "\n",
    "    # df['cities'] = dF_filter[0]  # parse first city name only\n",
    "\n",
    "    ## 3) Replace values of the 'cities' column **with** the parsed city names data (ie, by using a str.findall() method (ie, similar to regex findall()) to search over any matchin city names from the listing_urls col)\n",
    "    df['cities'] = df[df['listing_urls'].str.findall(str_search_pattern).str[0]]  # replace city names data by finding any matching city names as based on the rental listing URLs \n",
    "    # # NB: alternatively, use str.extract in tandem with fillna():\n",
    "    # df['cities'] = df['listing_urls'].str.extract(str_search_pattern, expand=False).fillna(df['cities'])\n",
    "\n",
    "    # return the updated city names data and assign back to the original 'cities' col:\n",
    "    return df['cities']\n",
    "\n",
    "# get copy of df\n",
    "df_since_jan_2023_cpy = df_since_jan_2023.copy()\n",
    "\n",
    "df_since_jan_2023_cpy['cities'] = parse_city_names_from_listing_URL(df_since_jan_2023, unique_city_names_dash_delim)\n",
    "\n",
    "# sanity check\n",
    "print(f'Filtered data: {df_since_jan_2023_cpy}')\n",
    "\n",
    "# df_since_jan_2023_cpy['cities'] = parse_city_names_from_listing_URL(df_since_jan_2023_cpy, unique_city_names_lis_dash_delim)\n",
    "# df_since_jan_2023_cpy['cities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Import Data from mid-January 2023 to Feb 14, 2023: subset data for Jan 15, 2023 to February 14, 2023\n",
    "\n",
    "\n",
    "# # NB: the following is a possible path I could take whereby you use a regex to parse out certain files based on their dates\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# path = '\\tmp\\s3\\bucket\\files'\n",
    "\n",
    "# df = pd.DataFrame({'files' : [f for f in Path(path).glob('*.csv')],\n",
    "#                   'stem' : [f.stem for f in Path(path).glob('*.csv')]})\n",
    "\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['stem'].str.extract('(\\d{4}-\\d{2}-\\d{2})')[0])\n",
    "\n",
    "# print(df)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Finally, parse the city names data for all of the data since mid-Jan 2023,\n",
    "which are currently missing city names data\"\"\"\n",
    "\n",
    "def parse_city_names_from_listing_URL(dictionary_of_dfs: dict, unique_city_names_dash_delim:list):\n",
    "\n",
    "  \"\"\" 1) Use str.contains() method chained to a .join() method in which we perform an 'OR' boolean via\n",
    "  the pipe (ie, '|' operator--ie, so we can search for multiple substrings (ie, each element \n",
    "  from the list arg) to look up any matching instances of city names\n",
    "  from the unique_city_names... list \n",
    "  relative to the rental listing URLs (ie, listing_urls).\n",
    "\n",
    "  2) Then, parse each such first city name by taking the first matched city name only,\n",
    "  \n",
    "  3) Use these parsed city name values to **replace** the values for the 'cities' column!\"\"\"\n",
    "\n",
    "  for df in dictionary_of_dfs.items():   \n",
    "\n",
    "    # step 1: use str.split() on '/apa/d' and get the 2nd element after performing the split:\n",
    "    df['listing_urls_for_str_match'] = df['listing_urls'].str.split('/apa/d/').str[1]  # obtain the 2nd resulting element\n",
    "\n",
    "    ## 2a) First!!: convert all string elements in col to lower-case for sake of consistency\n",
    "    df['listing_urls_for_str_match'] = df['listing_urls_for_str_match'].str.lower()  # apply lowercase to all characters of each row's string vals \n",
    "\n",
    "    ## Next, do same for the list of SF Bay + SC county names:\n",
    "    unique_city_names_dash_delim  = [el.lower() for el in unique_city_names_dash_delim]\n",
    "\n",
    "\n",
    "    # step 3: match a substring from this newly-parsed column-- ie, 'listing_urls_for_str_match'\n",
    "    # -- to matching substrings from the  sfbay_city_names list:\n",
    "    # How?: use str.contains() and join pipe operators to each element of the list to perform an essentially  boolean \"OR\" str.contains() search for any matching city names\n",
    "\n",
    "    # pipe operator\n",
    "    pipe_operator = '|'\n",
    "\n",
    "    df['cities'] = np.where(\n",
    "        df['listing_urls_for_str_match'].str.contains(\n",
    "        pipe_operator.join(\n",
    "                \n",
    "                unique_city_names_dash_delim   # look for any matching city names from sfbay_city_names list\n",
    "                )  # parse city names by matching the data to the list of possible SF Bay Area city names\n",
    "\n",
    "                # flags=re.I\n",
    "        ),\n",
    "        df['listing_urls_for_str_match'],   # ie, if there is a matching city name, then given row value for cities column  \n",
    "        df['cities']  # if no match, then keep original row value \n",
    "      )\n",
    "    \n",
    "    # return df['cities]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # step 1: use str.split() on '/apa/d' and get the 2nd element after performing the split:\n",
    "# df['listing_urls_for_str_match'] = df['listing_urls'].str.split('/apa/d')[1]  # obtain the 2nd resulting element\n",
    "\n",
    "# # step 2: use str.split() on the forward slashes (ie, essentially a delimiter), and obtain the first such element\n",
    "# df['listing_urls_for_str_match'] = df['listing_urls_for_str_match'].str.split('/')[0]  # obtain the 1st resulting element\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # step 3: match a substring from this newly-parsed column-- ie, 'listing_urls_for_str_match'\n",
    "# -- to matching substrings from the  sfbay_city_names list:\n",
    "# How?: use str.contains() and join pipe operators to each element of the list to perform an essentially  boolean \"OR\" str.contains() search for any matching city names\n",
    "\n",
    "# # pipe operator\n",
    "# pipe_operator = '|'\n",
    "\n",
    "# df['cities'] = np.where(\n",
    "#     df['listing_urls_for_str_match'].str.contains(\n",
    "#     pipe_operator.join(\n",
    "            \n",
    "#             sfbay_city_names   # look for any matching city names from sfbay_city_names list\n",
    "#             )  # parse city names by matching the data to the list of possible SF Bay Area city names\n",
    "\n",
    "#             # flags=re.I\n",
    "#     ),\n",
    "#     df['listing_urls_for_str_match'],   # ie, if there is a matching city name, then given row value for cities column  \n",
    "#     df['cities']  # if no match, then keep original row value \n",
    "#   ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# apply function to each dictionary of dfs\n",
    "# NB: use dicitonary comprehensions to apply the function to *each* df within each given dictionary:\n",
    "\n",
    "# Peninsula\n",
    "dict_of_dfs_pen2 = {key: val.pipe(parse_city_names_from_listing_URL(dict_of_dfs_pen, sfbay_city_names)) for key, val in dict_of_dfs_pen.items()}\n",
    "dict_of_dfs_pen2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, export all dfs containing  the cleaned city names--ie, since Jan 2023. \n",
    "\n",
    "### Loop over each df from each list of dfs, and return .to_csv() to re-export them replace the old CSV files!!\n",
    "\n",
    "\n",
    "### \"Exporting Pandas output for multiple CSV files\": https://stackoverflow.com/questions/67959271/exporting-pandas-output-for-multiple-csv-files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Warning!!!: This is pseudo-code. I need to revise with the new dfs, and test this on a smaller scale first!!!\"\"\"\n",
    "\n",
    "\"\"\" NB: see following for soverflow article on how to do this--*assuming I've managed to import each df as separate subregion & week data:\n",
    "\"How to export data frame back to the original csv file that I imported it from?\" \n",
    "<https://stackoverflow.com/questions/69159238/how-to-export-data-frame-back-to-the-original-csv-file-that-i-imported-it-from>\"\"\"\n",
    "\n",
    "# loop over all dfs containing data since Jan 2023 \n",
    "for df in dfs:\n",
    "    # replace all old CSV files with the cleaned city names\n",
    "    return pd.to_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craigslist_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7904fe1ebcaccf0a2b09d4fcad0ad53ef9109e8984f3cdcc60e975041080fad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
