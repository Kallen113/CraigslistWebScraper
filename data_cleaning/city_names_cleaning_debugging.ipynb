{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports-- file processing & datetime libraries\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "# SQL ODBC for API connection between Python & SQL Server\n",
    "import pyodbc\n",
    "# use json library to open a json file, which contains SQL credentials & configuration--ie, username, password, etc.\n",
    "import json \n",
    "\n",
    "## Data pipeline of Pandas' df to SQL Server -- import scraped craigslist rental listings data from CSV files to single Pandas' df: \n",
    "\n",
    "# recursively search parent direc to look up CSV files within subdirectories\n",
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files to a single Pandas' df using pd.concat()\"\"\"\n",
    "    path =  parent_direc # specify parent path of directories containing the scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes??\n",
    "    df_concat = pd.concat((pd.read_csv(file, # import each CSV file from directory\n",
    "                                        sep=',',encoding = 'utf-8'  # assume standard CSV (ie, comma separated ) formt and use utf-8 encoding\n",
    "                                        ) for file in glob.iglob( # iterate over each CSV file in path\n",
    "                                            os.path.join(path, '**', fn_regex), \n",
    "                                            recursive=True)), ignore_index=True)  # recursively iterate over each CSV file in path, and use os.path.join to help ensure this concatenation is OS independent\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Import all scraped rental listings data -- NB: in our case, we have various SF Bay Area listings data\n",
    "scraped_data_path = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\scraped_data\\\\sfbay\"\n",
    "df = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "d:\\Coding and Code projects\\Python\\craigslist_data_proj\\CraigslistWebScraper\\craigslist_venv\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "def clean_split_city_names(df, address_critera: list, neighborhood_criteria:list, split_city_delimiters: list, incorrect_city_names:dict, cities_not_in_region:dict, cities_that_need_extra_cleaning:dict):\n",
    "    \"\"\"Clean city names data in several ways:\n",
    "    a.) Remove extraneous address & neighborhood data placed in the city names HTML object, such as 'Rd', 'Blvd', or 'Downtown'.\n",
    "    b.) Unsplit city names data that are split via ',' & '/' delimiters.\n",
    "    c.) Replace abbreviated or mispelled city names, and remove city names that do not exist within the SF Bay Area (e.g., 'Redding').\n",
    "    d.) Remove any digits/integers within the city names data--ie, by using a '\\d+' regex as the argument of str.replace() and replace it with empty strings.\n",
    "    e.) Remove any city names records that are left with merely empty strings (ie, the other steps removed all data for that given cities record).\n",
    "    f.) Remove any whitespace to avoid the same city names from being treated as different entities by Pandas, Python, or SQL. \n",
    "    g.) Use str.capwords() to capitalize words (ie, excluding apostrophes).\n",
    "    h.) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena'. \"\"\"\n",
    "    # specify extraneous street & address data (e.g., 'Rd') that we want to remove from the city names column:\n",
    "    addr_criteria = '|'.join(address_critera) # Join pipe ('|') symbols to address list so we can str.split() on any one of these criteria (ie, 'or' condition splitting on each element separated by pipes):\n",
    "    # specify extraneous neighborhood criteria we should also remove from col\n",
    "    nbhood_criteria = '|'.join(neighborhood_criteria) # remove neighborhood names as well as state abbreviation (shown on website as 'Ca') that is shown without the usual comma delimiter!\n",
    "    # b.) specify delimiters we need to refer to un-split city names:\n",
    "    split_city_delimiters = '|'.join(split_city_delimiters) # join pipes to delimiters so we can use str.split() based on multiple 'or' criteria simultaneously\n",
    "    # clean city names data by removing extraneous address & neighborhood data, and unsplitting city names based on ',' & '\\' delimiters\n",
    "    df['cities'] =  df['cities'].str.split(addr_criteria).str[-1].str.replace(nbhood_criteria, '', case=True).str.lstrip()\n",
    "    df['cities'] = df['cities'].str.split(split_city_delimiters).str[0] #unsplit city names based on comma or forward-slash delimiters\n",
    "    # c.) replace specific abbreviated or mispelled city names, and remove cities that are not actually located in the sfbay region:\n",
    "    df = df.replace({'cities':incorrect_city_names}) # replace mispelled & abbreviated city names\n",
    "    df = df.replace({'cities':cities_not_in_region})  # remove (via empty string) cities that are not actually located in the sfbay region\n",
    "    # d.) Remove digits/integer-like data from cities column:\n",
    "    df['cities'] = df['cities'].str.replace('\\d+', '')  # remove any digits by using '/d+' regex to look up digits, and then replace with empty string\n",
    "    # e.) Remove any rows that have empty strings or null values for cities col (having performed the various data filtering and cleaning above)\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    # f.) Remove whitespace\n",
    "    df['cities'] = df['cities'].str.strip() \n",
    "    # g.) capitalize the city names using str.capwords() \n",
    "    df['cities'] = df['cities'].str.split().apply(lambda x: [val.capitalize() for val in x]).str.join(' ')\n",
    "    # h) Replace city names that are mispelled after having removed various street and neighborhood substrings such as 'St' or 'Ca'--e.g., '. Helena' should be 'St. Helena' & 'San los' should be 'San Carlos'. Also, remove any non-Bay Area cities such as Redding:\n",
    "    df = df.replace({'cities':cities_that_need_extra_cleaning})\n",
    "    # i) Remove any remaining empty strings or null records\n",
    "    df = df[df['cities'].str.strip().astype(bool)] # remove rows with empty strings (ie, '') for cities col \n",
    "    df = df.dropna(subset=['cities']) # remove any remaining 'cities' null records\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## clean split city names and clean abbreviated or incorrect city names:\n",
    "# specify various address and street name that we need to remove from the city names\n",
    "address_criteria = ['Boulevard', 'Blvd', 'Road', 'Rd', 'Avenue', 'Ave', 'Street', 'St', 'Drive', 'Dr', 'Real', 'E Hillsdale Blvd'] \n",
    "\n",
    "# specify various extraneous neighborhood names such as 'Downtown' \n",
    "neighborhood_criteria = ['Downtown', 'Central/Downtown', 'North', 'California', 'Ca.', 'Bay Area', 'St. Helena', 'St', 'nyon', \n",
    "'Jack London Square', 'Walking Distance To', 'El Camino', 'Mendocino County', 'San Mateo County', 'Alameda County', 'Rio Nido Nr', 'Mission Elementary', \n",
    "'Napa County', 'Golden Gate', 'Jennings', 'South Lake Tahoe', 'Tahoe Paradise', 'Kingswood Estates', 'South Bay', 'Skyline', 'San Antonio Tx', \n",
    "'East Bay', 'Morton Dr', 'Cour De Jeune'] \n",
    "\n",
    "# specify what delimiters we want to search for to unsplit the split city names data:\n",
    "split_city_delimiters =  [',', '/', ' - ', '_____', '#']\n",
    "\n",
    "# specify dictionary of abbreviated & mispelled cities:\n",
    "incorrect_city_names = {'Rohnert Pk':'Rohnert Park', 'Hillsborough Ca': 'Hillsborough','Fremont Ca':'Fremont', 'South Sf': 'South San Francisco', 'Ca':'', 'East San Jose':'San Jose', 'Vallejo Ca':'Vallejo', 'Westgate On Saratoga .':'San Jose', 'Bodega':'Bodega Bay', 'Briarwood At Central Park':'Fremont', 'Campbell Ca':'Campbell', 'Almaden':'San Jose', '.':'', 'East Foothills':'San Jose', 'Lake County':'', 'West End':'Alameda', 'Redwood Shores':'Redwood City', 'Park Pacifica Neighborhood':'Pacifica'}\n",
    "\n",
    "# specify dictionary of cities that are not located in sfbay (ie, not located in the region):\n",
    "cities_not_in_region = {'Ketchum':'', 'Baypoinr':'', 'Quito': '', 'Redding':'', 'Bend' :''}\n",
    "\n",
    "# specify dictionary of city names that are mispelled after having removed various street and neighborhood substrings:\n",
    "cities_that_need_extra_cleaning = {'. Helena': 'St. Helena', '. Helena Deer Park': 'St. Helena', 'San Los':'San Carlos', 'Tro Valley':'Castro Valley', 'Rohnert Pk':'Rohnert Park',\n",
    "'Pbell':'Campbell', 'Pbell Ca':'Campbell', 'American Yon':'American Canyon', 'Millbrae On The Burlingame Border':'Millbrae', 'Ockton Ca': 'Stockton', '. Rohnert Park': 'Rohnert Park', 'Udio Behind Main House':'', '***---rohnert Park':'Rohnert Park'}\n",
    "\n",
    "# clean city names data:\n",
    "df = clean_split_city_names(df, address_criteria, neighborhood_criteria, split_city_delimiters, incorrect_city_names, cities_not_in_region, cities_that_need_extra_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "San Francisco     4440\n",
       "San Jose          2904\n",
       "Oakland           2138\n",
       "Sunnyvale         1232\n",
       "Mountain View     1208\n",
       "                  ... \n",
       "Sonoma Valley        1\n",
       "Inner Sunset         1\n",
       "Monte Rio            1\n",
       "Penngrove            1\n",
       "La Selva Beach       1\n",
       "Name: cities, Length: 143, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cities.value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9245c747e8241d920f220897f12edbe786b61c4594b5fa55595ea3a9c3131f03"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('craigslist_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
